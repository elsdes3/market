[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning for Propensity Modeling",
    "section": "",
    "text": "E-Commerce is important since it allows a business to reach customers across a larger footprint than a group of physical (brick-and-mortar) stores. Visitors to an e-commerce store can make a purchase at an time, in any location and in their choice of currency. The ability to attract such a diverse customer base is the main value of e-commerce to a business. While website traffic is a highly-tracked metric by e-commerce businesses, their hard work and efforts to attract visitors to their site should not go to waste. It is customers that every e-commerce site owner needs to sustain their business."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Machine Learning for Propensity Modeling",
    "section": "",
    "text": "E-Commerce is important since it allows a business to reach customers across a larger footprint than a group of physical (brick-and-mortar) stores. Visitors to an e-commerce store can make a purchase at an time, in any location and in their choice of currency. The ability to attract such a diverse customer base is the main value of e-commerce to a business. While website traffic is a highly-tracked metric by e-commerce businesses, their hard work and efforts to attract visitors to their site should not go to waste. It is customers that every e-commerce site owner needs to sustain their business."
  },
  {
    "objectID": "index.html#problem-with-visitors-to-an-e-commerce-store",
    "href": "index.html#problem-with-visitors-to-an-e-commerce-store",
    "title": "Machine Learning for Propensity Modeling",
    "section": "Problem with Visitors to an E-Commerce Store",
    "text": "Problem with Visitors to an E-Commerce Store\nThe majority of visitors to an e-commerce site leave without performing a transaction (making a purchase) on their first visit. In the case of the the Google merchandise store’s site, the fraction of such visitors leaving is greater than 95%. Industry research shows that the majority of purchases by a visitor to such a site don’t occur on the visitor’s first visit. If they do purchase, then more often than not they will return later and make a purchase."
  },
  {
    "objectID": "index.html#why-market-to-return-users",
    "href": "index.html#why-market-to-return-users",
    "title": "Machine Learning for Propensity Modeling",
    "section": "Why Market to Return Users?",
    "text": "Why Market to Return Users?\nBeing able to identify such high-value visitors ahead of time can be of tremendous help to a marketing team to develop campaigns to grow the number of first-time visitors who make a purchase (converters) or a return purchase (repeat customers).\nThe marketing team can design and deploy a campaign after visitors’ first visit to improve their chances of making a purchase on a future visit."
  },
  {
    "objectID": "index.html#why-machine-learning",
    "href": "index.html#why-machine-learning",
    "title": "Machine Learning for Propensity Modeling",
    "section": "Why Machine Learning?",
    "text": "Why Machine Learning?\nThrough the use of machine learning (ML), we can scale this approach to capture all first-time visitors to the store and also improve their likelihood (or propensity) to make a future purchase while they search on a competitor’s site for the same or a similar product. Doing so is one way to help grow the base of converters and repeat customers."
  },
  {
    "objectID": "index.html#who-is-the-business-client",
    "href": "index.html#who-is-the-business-client",
    "title": "Machine Learning for Propensity Modeling",
    "section": "Who is the Business Client?",
    "text": "Who is the Business Client?\nThis project would be directly useful to Robertson Marketing, who is responsible for management of the Google merchandise store."
  },
  {
    "objectID": "index.html#what-is-this-project-about",
    "href": "index.html#what-is-this-project-about",
    "title": "Machine Learning for Propensity Modeling",
    "section": "What is This Project About?",
    "text": "What is This Project About?\nIn this project, ML predictions are used to select a marketing audience with a low, medium and high propensity to make a purchase on a return visit. Within each group, we also develop and briefly profile test (or treatment) and control cohorts in order to help facilitate deployment of a marketing campaign."
  },
  {
    "objectID": "references/Scope.html",
    "href": "references/Scope.html",
    "title": "Project Scope",
    "section": "",
    "text": "The Google Merchandise store is an e-commerce store that sells Google-branded products. Less than 5% of visitors make a purchase from the Google merchandise store on the Google Marketplace. So, a large number of visitors are not making a purchase. They are either just visiting the store once (their first and only visit) and leaving, or visiting multiple times and but not making a purchase on any of those visits. It goes without saying that customers, and not visitors alone, ensure the sustainability of an e-commerce business.\nConsiderable effort has been made by the web design team to build the store’s website to attract site traffic and make a good impression on first-time visitors to the store. This effort should not go to waste. However, approximately 90% of purchases do not happen during an initial visit to an e-commerce website. Furthermore, repeat customers spend 33% more with a brand than new customers do. Only approximately 20% of existing customers account for approximately 80% of future profits. Getting visitors to return to a site is important, but is possibly of equal or greater importance to an e-commerce business to have these visitors make a purchase on a return visit. This underscores the importance of getting visitors to make a purchase on a subsequent visit to the store.\nBut, it is not just sufficient to ensure return visits occur. This is of no use to a business since they can’t grow customer revenue by relying visitors to return and make purchaes on their own volition. The visitor should be motivated between visits to return and make a purchase on one or more future visits, rather than just returning and browsing through the store as they did during a previous visit. Between August 1, 2016 and August 1, 2017, a little less than 2% of visitors made a purchase on a return visit to the Google merchandise store. One of the main reasons for visitors browsing an e-commerce store, rather than making a purchase, is because they are comparson-shopping across multiple such websites looking for the lowest price for the same or a very similar product.\nIn summary, it will help the Google Merchandise store grow their customer base and increase revenue if some of these first-time visitors who have\n\nnot made a purchase during their first visit\nmade a purchase during their first visit\n\nto the store will make a purchase during a subsequent visit. In other words, it is desirable that first-time visitors to the store become customers or repeat customers.\n\n\n\nIf the business can find ways to reach out to (interact with) visitors to the store after their first visit, and provide promotions, shipping offers, etc., then this could be one way to motivate these visitors to make a purchase on a return visit to the store. If the Google merchandise store management company can reach out to these first-time visitors based on the characteristics of their first visit to the store and get them to make a purchase on a return visit then this can not only grow the business’ customer base but also reduce the loss of customer revenue to a competitor.\nAlphabet is the parent company of Google, but Robertson Marketing is the company that manages the Google Merchandise store. The management company (Robertson Marketing) is impacted by the problem of low conversions or repeat customers among the pool of first-time visitors to the merchandise store and they would be interested in ways to turn such first-time visitors into one of the following\n\nfuture customers (converting visitors into customers, or conversions)\nrepeat-customers (customer retention)\n\n\n\n\nThe business (store management company, Robertson Marketing) has tasked its marketing team with growing\n\nnew customers (conversion)\nrepeat customers (getting customers to become repeat customers)\n\nfrom the pool of first-time visitors to the store.\nFor obvious reasons, not all first-time visitors to the merchandise store are alike and reaching out to visitors is a costly process. With this mind, the marketing team would like to design an appropriate marketing campaign to help achive the business’ objectives. With a strong preference to spend marketing funds (budget) wisely, the marketing team wants to interact with such first-time visitors through focused and relevant recommendations, reminders and other types of marketing promotions after their first visit to the store.\nA logical approach to developing targeted promotions to grow customers or repeat customers is to offer a promotion based on a visitor’s likelihood of making a purchase during a future visit to the merchandise store. Knowing this likelihood is useful in knowing which visitors the marketing team should be focused on and, subsequently, how much funding can be allocated to communicating with those visitors. Accordingly, the business can determine the type of promotion that should be offered. For example, a minimal discount could be offered to visitors with a high likelihood of making a future purchase. Similarly, more loyalty points, coupon giveaways or free shipping could be offered to visitors who are deemed less likely to make a future purchase from the store. These promotions can be offered after a visitor’s first visit to the store with the aim of persuading them to make a purchase during a future visit.\n\n\n\nWithout knowing visitors’ likelihood of making a purchase on a return visit, it is not possible to segment visitors into audience groups (eg. visitors with a high, medium or low likelihood of making a purchase on a return visit) after their first visit. If these groups and cohorts were known, the marketing team could test how they respond to marketing strategies. Naive random guesses at visitor likelihood groupings are costly and unlikely to get buy-in for funding requests from business management. Furthermore, a test and control cohort is needed within each group in order to quantitatively determine how each group responds to the chosen marketing campaign (i.e. these cohorts are needed to evaluate the performance of the campaign).\nWe will assume that the marketing team has not yet designed any approaches to address this problem. With this in mind, both the size of the audience groups and the size of these cohorts are currently completely unknown."
  },
  {
    "objectID": "references/Scope.html#understanding-the-problem",
    "href": "references/Scope.html#understanding-the-problem",
    "title": "Project Scope",
    "section": "",
    "text": "The Google Merchandise store is an e-commerce store that sells Google-branded products. Less than 5% of visitors make a purchase from the Google merchandise store on the Google Marketplace. So, a large number of visitors are not making a purchase. They are either just visiting the store once (their first and only visit) and leaving, or visiting multiple times and but not making a purchase on any of those visits. It goes without saying that customers, and not visitors alone, ensure the sustainability of an e-commerce business.\nConsiderable effort has been made by the web design team to build the store’s website to attract site traffic and make a good impression on first-time visitors to the store. This effort should not go to waste. However, approximately 90% of purchases do not happen during an initial visit to an e-commerce website. Furthermore, repeat customers spend 33% more with a brand than new customers do. Only approximately 20% of existing customers account for approximately 80% of future profits. Getting visitors to return to a site is important, but is possibly of equal or greater importance to an e-commerce business to have these visitors make a purchase on a return visit. This underscores the importance of getting visitors to make a purchase on a subsequent visit to the store.\nBut, it is not just sufficient to ensure return visits occur. This is of no use to a business since they can’t grow customer revenue by relying visitors to return and make purchaes on their own volition. The visitor should be motivated between visits to return and make a purchase on one or more future visits, rather than just returning and browsing through the store as they did during a previous visit. Between August 1, 2016 and August 1, 2017, a little less than 2% of visitors made a purchase on a return visit to the Google merchandise store. One of the main reasons for visitors browsing an e-commerce store, rather than making a purchase, is because they are comparson-shopping across multiple such websites looking for the lowest price for the same or a very similar product.\nIn summary, it will help the Google Merchandise store grow their customer base and increase revenue if some of these first-time visitors who have\n\nnot made a purchase during their first visit\nmade a purchase during their first visit\n\nto the store will make a purchase during a subsequent visit. In other words, it is desirable that first-time visitors to the store become customers or repeat customers.\n\n\n\nIf the business can find ways to reach out to (interact with) visitors to the store after their first visit, and provide promotions, shipping offers, etc., then this could be one way to motivate these visitors to make a purchase on a return visit to the store. If the Google merchandise store management company can reach out to these first-time visitors based on the characteristics of their first visit to the store and get them to make a purchase on a return visit then this can not only grow the business’ customer base but also reduce the loss of customer revenue to a competitor.\nAlphabet is the parent company of Google, but Robertson Marketing is the company that manages the Google Merchandise store. The management company (Robertson Marketing) is impacted by the problem of low conversions or repeat customers among the pool of first-time visitors to the merchandise store and they would be interested in ways to turn such first-time visitors into one of the following\n\nfuture customers (converting visitors into customers, or conversions)\nrepeat-customers (customer retention)\n\n\n\n\nThe business (store management company, Robertson Marketing) has tasked its marketing team with growing\n\nnew customers (conversion)\nrepeat customers (getting customers to become repeat customers)\n\nfrom the pool of first-time visitors to the store.\nFor obvious reasons, not all first-time visitors to the merchandise store are alike and reaching out to visitors is a costly process. With this mind, the marketing team would like to design an appropriate marketing campaign to help achive the business’ objectives. With a strong preference to spend marketing funds (budget) wisely, the marketing team wants to interact with such first-time visitors through focused and relevant recommendations, reminders and other types of marketing promotions after their first visit to the store.\nA logical approach to developing targeted promotions to grow customers or repeat customers is to offer a promotion based on a visitor’s likelihood of making a purchase during a future visit to the merchandise store. Knowing this likelihood is useful in knowing which visitors the marketing team should be focused on and, subsequently, how much funding can be allocated to communicating with those visitors. Accordingly, the business can determine the type of promotion that should be offered. For example, a minimal discount could be offered to visitors with a high likelihood of making a future purchase. Similarly, more loyalty points, coupon giveaways or free shipping could be offered to visitors who are deemed less likely to make a future purchase from the store. These promotions can be offered after a visitor’s first visit to the store with the aim of persuading them to make a purchase during a future visit.\n\n\n\nWithout knowing visitors’ likelihood of making a purchase on a return visit, it is not possible to segment visitors into audience groups (eg. visitors with a high, medium or low likelihood of making a purchase on a return visit) after their first visit. If these groups and cohorts were known, the marketing team could test how they respond to marketing strategies. Naive random guesses at visitor likelihood groupings are costly and unlikely to get buy-in for funding requests from business management. Furthermore, a test and control cohort is needed within each group in order to quantitatively determine how each group responds to the chosen marketing campaign (i.e. these cohorts are needed to evaluate the performance of the campaign).\nWe will assume that the marketing team has not yet designed any approaches to address this problem. With this in mind, both the size of the audience groups and the size of these cohorts are currently completely unknown."
  },
  {
    "objectID": "references/Scope.html#project-client-and-definition-of-objective",
    "href": "references/Scope.html#project-client-and-definition-of-objective",
    "title": "Project Scope",
    "section": "Project Client and Definition of Objective",
    "text": "Project Client and Definition of Objective\n\nBusiness Client\nThe client for this project is a marketing team responsible for managing marketing campaigns related to the Google merchandise store.\n\n\nProject Goal\nThis project exists to help the marketing team (client) interact with first-time visitors to the merchandise store, with the hopes of increasing the likelihood that these visitors will make a purchase (convert) or repeat-purchase during a future visit to the store. If this can be done, then it will help the team address the business’ objectives of growing new and repeat customers as mentioned above.\nThe objective of this project is to increase the number of first-time visitors to the merchandise store who are converted into new or repeat customers."
  },
  {
    "objectID": "references/Scope.html#actions-that-need-to-be-taken",
    "href": "references/Scope.html#actions-that-need-to-be-taken",
    "title": "Project Scope",
    "section": "Actions that Need to be Taken",
    "text": "Actions that Need to be Taken\nThis project will facilitate develpoment of a proactive and targeted marketing strategy (eg. promotions) to grow new and repeat customers."
  },
  {
    "objectID": "references/Scope.html#analysis",
    "href": "references/Scope.html#analysis",
    "title": "Project Scope",
    "section": "Analysis",
    "text": "Analysis\n\nType of Analysis\nWe need to answer the important question: Which visitors should we prioritize through proactive marketing promotions. In orther words, we want to identify the visitors with a low, medium and high likelihood of making a purchase during a future (or return) visit to the store.\nSince we want to intervene before a visitor’s next visit to the store, we would predict the likelihood of every first-time visitor making a purchase during a subsequent visit. These predictions will used to create audience groups based on the likelihood of making a future purchase and prioritize and focus the marketing strategy per group.\nThe analysis to be performed here is a prediction task. We need to predict the likelihood (or propensity) of a purchase during a future visit.\n\n\nFormat of Data\nThe analysis will be performed using machine learning (ML). A ML model will be trained using attributes (features) of the visitors’ first visit and it will predict visitors’ propensity to make a purchase on a return visit to the store. The best-performing trained ML model will be the one that can make this prediction with the highest accuracy or some other evaluation metric (this will be discussed later in the Evaluation Metric sub-section below). This application of ML is called propensity modeling. The outcome to be predicted is binary (there are only two possible outcomes)\n\nthe visitor will make a purchase on a return visit\nthe visitor will not make a purchase on a return visit\n\nIn a ML context, this is a supervised learning problem. Attributes about the first visit made by visitors to the store site are retrieved from Google Analytics (GA) tracking data accumulated for visitors to the merchandise store. GA tracking code has been embeded in the store’s website in order to anonymously track visitor interactions on the site. These attributes, or characteristics, of visitors’ first visit are the independent variables or features in ML.\nFor the same visitors, the outcome (or label in ML) of return visits (whether a purchase on a return visit was made or not) is retrieved to determine if a purchase on a future visit was made by this visitor. This label is a forward-looking label since it references events from the future. By comparison, the features are from the past (historical data) since they reference attributes of the visitor’s first visit to the store. Both features and label refer to the same visitor.\n\n\nAnalysis Workflow Overview\nWith such a dataset of Google Analytics tracking data available for all visitors to the store between August 1, 2016 and August 1, 2017, a ML model will be trained to predict whether first-time visitors will make a purchase during a future (return) visit. The trained model then predicts probabilities (which are interpreted as likelihoods or propensities) for new visitors to make a purchase on a return (or future) visit to the store. These new visitors were not part of the model’s training data. The predicted probabilities are then used to generate marketing audience groups (low, medium and high propensity) and test (or treatment) and control cohorts within each group as described above.\nIn summary the steps of such a workflow are\n\ntrain ML model using historical data for first visit of visitors\n\nthis is the training data\n\nuse trained model to predict probabilities for first visit of visitors that are not part of the training data\n\nthis is the unseen data\n\nuse predicted probabilities to assign audience cohorts (test or control) to all visitors in the unseen data\nbuild a brief profile of the visitors in the test cohort in unseen data\n\nwhen building a marketing strategy, we are not allowed to look at the control cohort and so the profile will be required for the test cohort only\n\nprovide audience test cohorts and their associated profile summaries to the marketing team\n\n\n\nTimeframes for Study\n\nIn order to avoid data leakage (or lookahead bias), the data splits are created in chronological order\n\ntraining data\n\nSeptember 1, 2016 to December 31, 2016\n\nvalidation data\n\nJanuary 1 - 31, 2017\n\ntest data\n\nFebruary 1 - 28, 2017\n\nunseen data\n\nMarch 1 - 31, 2017\n\n\nWe will assume that\n\nthe current date is March 1, 2017\nML model development can be performed between March 1 - 31, 2017\n\nThe marketing team is interested in growing new and repeat customers from visitors who made their first visit to the store during March 1 - 31, 2017 (unseen data). With this in mind, they want to\n\nbuild their campaign around these visitors\nlaunch their campaign on April 10, 2017\n\nThere are two constraints facing the client (marketing team)\n\nthe first visit data covering this period (unseen data) is only available on March 31, 2017 and this is close to the proposed campaign start date of April 10, 2017\ndesigning a typical marketing campaign takes 1 - 12 weeks\ncampaign launch windows occur every month\n\ncampaigns can be launched on April 10, 2017, May 10, 2017, June 10, 2017, etc.\n\n\nWith this in mind, the marketing team wants to start designing their campaign today (March 1, 2017). They do not want to wait until March 31, 2017 to receive recommended audience cohorts from the data science team and begin their campaign design. So, instead of waiting until March 31, 2017, the marketing team will start their campaign design using the audience cohorts recommended by the data science team using the test data split, which covers February 1 - 28, 2017.\nOn March 31, 2017, the marketing team will receive the audience cohorts from the data science team for the visits who made their first visit to the store during March 1 - 31, 2017. Between April 1, 2017 and April 9, 2017, the marketing team will start making adjustments to the campaign strategy by using the audience cohorts recommended by the data science team using the unseen data period (covering March 1 - 31, 2017). This will allow the marketing team to meet the proposed campaign start date of April 10, 2017.\nIf the datascience team is unable to generate a sufficiently accurate ML model to meet the April 10, 2017 campaign launch date then they will need to improve their analysis in order to try to meet the next available launch date (May 10, 2017).\n\n\n\nNotes\n\nRegarding ML labels (y)\n\nas mentioned earlier, these are forward-looking labels\n\nthe ML features (X) are attributes of a visitor’s first visit to the store\nthe ML labels (y) are the outcome (whether a purchase occurred or not) of that same visitor’s future visits to the store\n\na purchase is allowed to occur during any future visit to the store, not just the next visit\n\n\nin the period covering train, validation and test data splits, if a visitor has\n\nmade at least one purchase of a product during their return visit, then the label is set to True (or 1)\nnot made at least one purchase of a product during their return visit, then the label is set to False (or 0)\n\n\nThe data science team’s recommended audience cohorts (test and control) of visitors will be accepted by the marketing team if the ML model’s performance during evaluation (using the test data split) is better than that of a random model."
  },
  {
    "objectID": "references/Scope.html#how-do-actions-follow-from-the-analysis",
    "href": "references/Scope.html#how-do-actions-follow-from-the-analysis",
    "title": "Project Scope",
    "section": "How do Actions Follow From the Analysis",
    "text": "How do Actions Follow From the Analysis\nBased on visitors’ predicted likelihood of making a purchase on a future visit, marketing audience test and control groups (cohorts) will be created. Each group will contain a visitor ID as well as all the attributes of the visitor’s first visit that were used to predict the likelihood of a purchase during a return visit to the store.\nThese groups can be used by the marketing team to\n\ndesign appropriate strategies that can be implemented during activation\nestimate campaign costs"
  },
  {
    "objectID": "references/Scope.html#validation",
    "href": "references/Scope.html#validation",
    "title": "Project Scope",
    "section": "Validation",
    "text": "Validation\n\nDuring Development\nDevelopment covers September 1, 2016 - February 28, 2017.\nSince the current date is assumed to be March 1, 2017 and the training, validation and test data splits end no later than February 28, 2017, ML model predictions during validation (using validation split) and evaluation (using test split) can be scored before March 31, 2017.\nScoring is performed using evalaution metrics discussed in the ML Evaluation Metric sub-section below.\n\n\nDuring Production\nProduction covers March 1 - 31, 2017.\nThe model’s predictions will be scored against the outcome (whether the visitor makes a purchase on their return/future visit to the store) at the end of the marketing campaign.\n\n\nDifferences between Development and Production\n\nDuring production, the predictions are used to inform a marketing audience cohorts (test and control). By definition, the marketing strategy will be applied to the test cohort. It will not be applied to the control group. With this in mind, during the production period (March 1 - 31, 2017), we can only evaluate the predictions of the trained ML model that are associated with first-time visitors to the store during this period if those visitors are placed in the control cohort.\nAs mentioned earlier, the marketing team will only accept the data science team’s recommended audiente cohorts if the ML model outperforms a random model. At the same time, the data science team should also be checking for drift between attributes (features) of the first visit using the test split (development) and using the unseen data (production). If drift in features is observed outside a pre-defined threshold, then the data science team will need to repeat\n\nML model training using more training data (earlier start date than September 1, 2016)\nevaluation using the test split\nevaluation using the unseen data\n\nuntil drift is no longer observed. Feature drift checking will need to be done on April 1, 2017, before marketing audience visitor cohorts are given to the marketing team.\n\n\n\nWorkflow in Production\n\nThe trained model will make predictions of probability (propensity or likelihood) for all first-time visitors to the store during March 1 - 31, 2017. Predictions are used to identify marketing audience test (or treatment) and control cohorts.\nA marketing strategy is applied to all first-time visitors in the test group\nLength of marketing promotion campaign is to be determined by marketing team\nAt the end of the marketing campaign\n\nwe will know which visitors who were predicted to make a purchase on a return visit did actually make a purchase\nwe can evaluate the predictions made by the trained ML model on first visits that occurred during March 1 - 31, 2017\nwe can calculate a suitable KPI for this project\n\nKPI = number of purchases made by visitors in the test cohort - number of purchases made by visitors in the control cohort\n\nif this KPI is larger than zero, then we have successfully grown our customer base, which was the objective of the task that the business has given to the marketing team\n\nadditional KPIs can also be considered\n\n\n\nWe mentioned that scoring predictions of first visits that occurred during the unseen data period (production) of March 1 - 31, 2017 cannot be performed until the end of the marketing campaign. This was also mentioned in the during production sub-section above. It is worth emphasizing that until the end of the marketing campaign, we are unable to evalute the ML model’s predictions of data during the unseen data period (production). For this reason, it is improtant to check for drift in ML features between the unseen data (March 1 - 31, 2017) and test data (February 1 - 28, 2017) before the predictions are made and the audience cohorts are generated.\nGenerally, marketing campaigns run for approximately three months but this depends on numerous factors including\n\nmessage\ncall to action (CTA)\nfunds available (marketing budget)\nexpectations (desired uplift, etc.)\n\nThe duration, design and structure of the campaign will be determined by the marketing team starting on March 1, 2017 (today) and it will be finalized between April 1 - 9, 2019. On April 9, 2017, if it is determined that it is not feasible to design a campaign based on the audience cohorts recommended by the data science team (eg. cohorts are too large, etc.) then\n\nthe next available campaign launch window (May 10, 2017) will have to be targeted\nthe new unseen data (production) period will cover April 1 - 30, 2017\nthe datascience team will have to improve ML model performance between April 10 - 30, 2017\n\n\n\nML Evaluation Metric\nFalse negatives (tweets that should have been responded to but were predicted to not need a response) and false positives (tweets that did not need review by a team member but were predicted as requiring a review) are the most important types of errors. So the candidate metrics to be used to assess ML model performance are\n\nF1-score (if false negatives and false positives are equally important)\nF2-score (if false negatives are more important)\nF0.5-score (if false positives are more important)\n\nFor the current predicton task, there are two possible outcomes indicate whether a visitor did or did not make a purchase on a return visit to the store and these are\n\nactual\n\nis the true outcome\nthis is known after a visitor’s first visit to the store\nthis indicates that action that the marketing team should have taken\n\npredicted\n\nis the predicted outcome\nthis is predicted after the visitor’s first visit to the store\nthis indicates that action that the marketing team was predicted to have taken\n\n\nThe four possible ML prediction scenarios are listed below for the prediction of the outcome [whether a first-time visitor will, or will not, make a purchase on a return (future) visit to the merchandise store]\n\nTP: actual = makes purchase on return visit, predicted = makes purchase on return visit\n\npredicted marketing strategy matches what should be the actual marketing strategy\n\nTN: actual = does not make purchase on return visit, predicted = does not make purchase on return visit\n\npredicted marketing strategy matches what should be the actual marketing strategy\n\nFN: actual = makes purchase on return visit, predicted = does not make purchase on return visit\n\npredicted marketing strategy\n\npredicted to offer minimal promotion\n\nactual marketing strategy\n\nactually should have offered a stronger promotion\n\neg. more loyalty points, more frequent free/shipping, etc.\n\n\nthese errors in prediction lead to missed opportunities to correctly target first-time visitors since the predicted promotion offered is an underestimate of the true promotion that the team should have offered to these visitors\nthese errors lead to underspending on promotions to first-time visitors who are likely to benefit from them\n\nFP: actual = does not make purchase on return visit, predicted = makes purchase on return visit\n\npredicted marketing strategy\n\npredicted to offer a stronger promotion\n\nactual marketing strategy\n\nactually should have offered minimal promotion\n\nthese errors lead to overspending on promotions to first-time visitors who are not likely to benefit from them\nthis is the most expensive type of prediction error\nthis scenario must be avoided\n\n\nSince FP (false positives) are more costly than FN (false negatives), the scoring metric chosen to evaluate predictions made using the ML model is F0.5-score."
  },
  {
    "objectID": "references/Scope.html#data",
    "href": "references/Scope.html#data",
    "title": "Project Scope",
    "section": "Data",
    "text": "Data\nAn important factor that is driving propensity modeling in marketing is the need to do more with first-party customer data. This is data that comes directly from the customer and not from third-party sources. For marketing use-cases, effective propensity models use customer attributes from online and offline first-party data sources, including site analytics (online) and CRM (offline) data.\nHere, we have access to online data only Google Analytics tracking data (see the dataset and its documentation). This will be used to build a ML model to predict visitors’ propensity to make a purchase during a future visit to the store.\nVisit data for the merchandise store is available for the period of August 1, 2016 to August 1, 2017. This data provides information such as\n\nvisitor ID\nvisit date\nvist datetime\nactions performed during visit\n\nadd to cart\nremove from cart\nmake purchase\nview product details\netc.\n\ntotal time spent viewing pages during each visit\netc."
  },
  {
    "objectID": "references/Scope.html#analysis-notebooks",
    "href": "references/Scope.html#analysis-notebooks",
    "title": "Project Scope",
    "section": "Analysis Notebooks",
    "text": "Analysis Notebooks\n\nGet data and EDA Part 1\n\nconnect to raw visit data generated by Google Analytics tracking embedded in the merchandise store’s site\n\ndata is stored as Google BigQuery public dataset\nuse Python client to connect to dataset\nget overview of the columns in the raw visit data\n\nunderstand underlying patterns and stats about the visit-level data\nEDA part 1/2\n01_get_data.ipynb\n\nEDA Part 2\n\nEDA part 2/2\n02_eda.ipynb\n\nTransform data\n\n03_transform.ipynb\nextract the first visit per visitor (features, or X) and align with whether they made a purchase on a return (future) visit to the store (labels, or y)\n\nBaseline model development\n\ndevelop a baseline model to predict probability of purchase during future visit\n\nthis will be fast to train and will demonstrate the end-to-end project workflow, but will likely be over-simplified and so will underperform relative to a ML-based approach\n\n04_development.ipynb\n\nML model development\n\nrepeat baseline model development, but use a ML model instead\n05_dev_v2.ipynb"
  },
  {
    "objectID": "references/Scope.html#limitations",
    "href": "references/Scope.html#limitations",
    "title": "Project Scope",
    "section": "Limitations",
    "text": "Limitations\n\nBusiness Use Case\nThe analysis implemented here is only possible if Google Analytics tracking is embedded into an e-commerce website. Guides for embedding GA tracking code are documented below\n\nchartio blog post\nGoogle Support documentation\n\nFor the current use-case, this was already done for the Google Merchandise store’s website and so valuable tracking data could be collected and used. However, if such a solution is to be adopted for other digital marketplaces, then the Google Analytics tracking code must be embedded into those websites.\n\n\nData\n\nThe analytics dataset used in this project is based on a version of Google Analytics (GA360) that is deprecated as of July 1 2023 or 2024.\n\n\n\nOthers\nFor other limitations, please see the Limitations section in each notebook."
  },
  {
    "objectID": "references/Scope.html#assumptions",
    "href": "references/Scope.html#assumptions",
    "title": "Project Scope",
    "section": "Assumptions",
    "text": "Assumptions\n\nBusiness Use Case\n\nFor visitors who made a purchase on a return visit, we will include those who could have bought on their first as well. These are repeat customers, who we have assumed are one of the two types of visitors that we want to grow. For this reason, we will include their visits in the data.\nThe marketing team has does not have a preliminary idea as to the size of the audience groups (low, medium, high likelihood or propensity to make a purchase on a return visit) or cohorts and the strategy they will deploy as part of a campaign. As such, they have not yet designed any approaches to address this problem.\nDeployment-related assumptions (see point 4. in Timeframes for Study)\n\nwe have assumed that the current date is March 1, 2017\nwe have assumed that a trained ML model will make predictions for all first-time visitors to the store between March 1 - 31, 2017\n\n\nFor other assumptions, please see the Assumptions section in each notebook."
  },
  {
    "objectID": "notebooks/02-train/notebooks/04_train.html",
    "href": "notebooks/02-train/notebooks/04_train.html",
    "title": "ML Development",
    "section": "",
    "text": "Train a ML model to predict first-time US-based visitors’ propensity to make a purchase on a return visit to the Google merchandise store.\n\n\n\nVisit data is taken from Google Cloud’s public dataset with Google Analytics visitor transaction data.\n\n\n\nThe study timeline covers several months. During the study period, the visitor ID of every visitor who made a return visit to the store is retrieved. The ML model will be developed using the first visit by these visitors only, since there isn’t any useful information to be learnt from visitors who made a single visit to the store without returning. The attributes of the first visit to the store by the return visitors will be used when splitting the data for use in ML model development. Three data splits are created for ML model development, which includes model training, validation and evaluation. As mentioned above, the data splits combine to cover the study period and they occur in chronological order.\nThe training data split covers the first several months of available data and it captures attributes of the first-time visitors who were found to make a return visit to the store during the study period. The first visit occurs during these pre-determined months. The validation split covers first-time visitors to the store during the month that follows the last month of training data. Similarly, the test data split covers visitors during the month following the validation data period.\n\n\n\nThe ML model is trained using the training data and scored on the validation data. Features are engineered and pre-processed using the training and validation data splits. Due to the class imbalance with most visitors not making a purchase during a return visit, over- and under-sampling techniques are used to help the model learn from attributes of first-time visits that resulted in a purchase. A ML pipeline is developed using the scikit-learn ML framework for tabular data with the Pipeline class. The pipeline covers all the steps that are part of typical ML training, including feature engineering, pre-processing, feature selection and re-sampling (over- or under-sampling) and classification steps. If over-sampling is chosen, then this step is performed after the pre-processing step and before the classification step. If under-sampling is performed, then re-sampling is performed as the first step of the pipeline.\n\n\n\nAs mentioned in the project scope, a single evaluation metric is preferred for this project. For comparison purposes, several metrics will be computed. Model selection (next step) will be performed using the primary (preferred) metric only.\n\n\n\nDuring this step, one or more runs of a single or multiple ML experiments will be executed.\nA single run of a single experiment consists of training and evaluating a single pipeline (model). A grid of model hyperparameters for the pipeline is defined in the User Inputs section. These hyperparameters will be optimized (tuned) by training the pipeline on the training data split and scoring its predictions on the validation data. The best hyperparameters for the pipeline will be the ones that return the best evaluation metric on the validation data. The pipeline with the best hyperparameters will then be re-trained using the combined training and validation data and it will be used to predict and evaluate observations in the test data split. At the end of a single experiment run, all the metadata (eg. features used, best pipeline hyperparameters, metrics, etc.) and metrics associated with that run are tracked using the MLFlow ML lifecycle management platform.\nDuring the next step, the pipeline (model) with the best metric (score) on the test data split will be determined by retrieving the metadata for all experiment runs and ranking the runs based on the scoring metric on the test data split. The best model will then be tagged for further use (to generate a marketing audience) during a later step in this project."
  },
  {
    "objectID": "notebooks/02-train/notebooks/04_train.html#about",
    "href": "notebooks/02-train/notebooks/04_train.html#about",
    "title": "ML Development",
    "section": "",
    "text": "Train a ML model to predict first-time US-based visitors’ propensity to make a purchase on a return visit to the Google merchandise store.\n\n\n\nVisit data is taken from Google Cloud’s public dataset with Google Analytics visitor transaction data.\n\n\n\nThe study timeline covers several months. During the study period, the visitor ID of every visitor who made a return visit to the store is retrieved. The ML model will be developed using the first visit by these visitors only, since there isn’t any useful information to be learnt from visitors who made a single visit to the store without returning. The attributes of the first visit to the store by the return visitors will be used when splitting the data for use in ML model development. Three data splits are created for ML model development, which includes model training, validation and evaluation. As mentioned above, the data splits combine to cover the study period and they occur in chronological order.\nThe training data split covers the first several months of available data and it captures attributes of the first-time visitors who were found to make a return visit to the store during the study period. The first visit occurs during these pre-determined months. The validation split covers first-time visitors to the store during the month that follows the last month of training data. Similarly, the test data split covers visitors during the month following the validation data period.\n\n\n\nThe ML model is trained using the training data and scored on the validation data. Features are engineered and pre-processed using the training and validation data splits. Due to the class imbalance with most visitors not making a purchase during a return visit, over- and under-sampling techniques are used to help the model learn from attributes of first-time visits that resulted in a purchase. A ML pipeline is developed using the scikit-learn ML framework for tabular data with the Pipeline class. The pipeline covers all the steps that are part of typical ML training, including feature engineering, pre-processing, feature selection and re-sampling (over- or under-sampling) and classification steps. If over-sampling is chosen, then this step is performed after the pre-processing step and before the classification step. If under-sampling is performed, then re-sampling is performed as the first step of the pipeline.\n\n\n\nAs mentioned in the project scope, a single evaluation metric is preferred for this project. For comparison purposes, several metrics will be computed. Model selection (next step) will be performed using the primary (preferred) metric only.\n\n\n\nDuring this step, one or more runs of a single or multiple ML experiments will be executed.\nA single run of a single experiment consists of training and evaluating a single pipeline (model). A grid of model hyperparameters for the pipeline is defined in the User Inputs section. These hyperparameters will be optimized (tuned) by training the pipeline on the training data split and scoring its predictions on the validation data. The best hyperparameters for the pipeline will be the ones that return the best evaluation metric on the validation data. The pipeline with the best hyperparameters will then be re-trained using the combined training and validation data and it will be used to predict and evaluate observations in the test data split. At the end of a single experiment run, all the metadata (eg. features used, best pipeline hyperparameters, metrics, etc.) and metrics associated with that run are tracked using the MLFlow ML lifecycle management platform.\nDuring the next step, the pipeline (model) with the best metric (score) on the test data split will be determined by retrieving the metadata for all experiment runs and ranking the runs based on the scoring metric on the test data split. The best model will then be tagged for further use (to generate a marketing audience) during a later step in this project."
  },
  {
    "objectID": "notebooks/02-train/notebooks/04_train.html#user-inputs",
    "href": "notebooks/02-train/notebooks/04_train.html#user-inputs",
    "title": "ML Development",
    "section": "User Inputs",
    "text": "User Inputs\nDefine the following\n\nstart and end dates for train, validation and test data\nlist of categorical features\nlist of categorical features present in the raw data as integers\nlist of numerical features\ndatetime features\nfeatures to exclude\nname of column containing label (outcome)\nresampling approach (over- or under-sampling)\nprimary ML scoring metric\nhyperparameter grid\ntype of scikit-learn ML model\nMLFlow-related inputs for experiment tracking\n\n\n# 1. start and end dates\ntrain_start_date = \"20160901\"\ntrain_end_date = \"20161231\"\nval_start_date = \"20170101\"\nval_end_date = \"20170131\"\ntest_start_date = \"20170201\"\ntest_end_date = \"20170228\"\n\n# 2. categorical column\ncategorical_features = [\n    \"bounces\",\n    \"last_action\",\n    \"source\",\n    \"medium\",\n    \"channelGrouping\",\n    \"browser\",\n    \"os\",\n    \"deviceCategory\",\n]\n\n# 3. categorical columns present as integers\ncategorical_features_ints = [\"bounces\"]\n\n# 4. numerical columns\nnumerical_features = [\n    \"hits\",\n    \"promos_displayed\",\n    \"promos_clicked\",\n    \"product_views\",\n    \"product_clicks\",\n    \"pageviews\",\n    \"time_on_site\",\n]\n\n# 5. datetime features\ndatetime_features = [\"quarter\", \"month\", \"day_of_month\", \"day_of_week\", \"hour\"]\n\n# 6. features to exclude\ncols_to_drop = [\"fullvisitorid\", \"visitId\", \"visitNumber\", \"visitStartTime\"]\n\n# 7. label column\nlabel = \"made_purchase_on_future_visit\"\n\n# 8. resampling\nresampling_approach = \"os\"\n\n# 9. scoring metric\nprimary_metric = \"fbeta2\"\n\n# 10. define hyperParameter grid\nparam_grid = {\n    \"resampler__sampling_strategy\": [0.1],\n    \"select__threshold\": [0.7],\n    \"preprocessor__cat__rarecats__fe__tol\": [0.10],\n    \"preprocessor__cat__rarecats__fe__n_categories\": [1],\n    \"preprocessor__cat__rarecats__fe__ignore_format\": [True],\n    \"preprocessor__cat__rarecats__fe__replace_with\": [\"other\"],\n    # \"clf__strategy\": [\"stratified\"],  # model_type = DummyClassifier\n    \"clf__a\": [0.2, 0.3, 0.4, 0.35, 0.15, 0.45],  # model_type = BetaDistClassifier\n    \"clf__b\": [2.31, 2.25, 2.35, 2.4, 2.5],  # model_type = BetaDistClassifier\n}\n\n# 11. type of model\nmodel_type = \"BetaDistClassifier\"\n\n# 12. mlflow\nmlflow_expt_name = \"My Demo Experiment 2\"\nmlflow_unwanted_tracking_cols = [\n    \"tags.mlflow.source.name\",\n    \"tags.mlflow.source.type\",\n    \"tags.mlflow.user\",\n    \"artifact_uri\",\n    \"start_time\",\n    \"end_time\",\n]\ncleanup_mlflow = True\n\nA helper function will be created to programmatically load data from BigQuery based on the desired start and end dates. The function accepts the following\n\nstart and end dates for which data is to be retrieved\n\nthese dates will be different for the training, validation and test splits\n\nstart date for the training data and end date for the test data\n\nthese two dates define the period over which ML model development will occur\nthese are used to retrieve visitors who made a purchase on a return (future) visit to the store during this period\n\n\nThe function is defined in src/sql_helpers.py."
  },
  {
    "objectID": "notebooks/02-train/notebooks/04_train.html#get-data",
    "href": "notebooks/02-train/notebooks/04_train.html#get-data",
    "title": "ML Development",
    "section": "Get Data",
    "text": "Get Data\n\nTrain\n\nquery_train = sqlh.get_sql_query(\n    train_start_date, train_end_date, train_start_date, test_end_date\n)\ndf_train, _ = th.extract_data(query_train, gcp_auth_dict).pipe(\n    th.transform_data,\n    datatypes_dict=dtypes_dict,\n    duplicate_cols=[\"fullvisitorid\"],\n    column_mapper_dict={'last_action': action_mapper},\n    # categoricals=list(set(categorical_features) - set(categorical_features_ints)),\n)\n# df_train.pipe(th.load_data, processed_data_dir, \"train\")\n\nQuery execution start time = 2023-06-05 18:48:53.870...done at 2023-06-05 18:49:10.040 (16.170 seconds).\nQuery returned 92,859 rows\nGot 92,551 rows and 28 columns after dropping duplicates\nTransformed data has 92,551 rows & 28 columns\nCPU times: user 3.72 s, sys: 335 ms, total: 4.06 s\nWall time: 16.3 s\n\n\n\n\nValidation\n\nquery_val = sqlh.get_sql_query(val_start_date, val_end_date, train_start_date, test_end_date)\ndf_val, _ = th.extract_data(query_val, gcp_auth_dict).pipe(\n    th.transform_data,\n    datatypes_dict=dtypes_dict,\n    duplicate_cols=[\"fullvisitorid\"],\n    column_mapper_dict={'last_action': action_mapper},\n)\n# df_val.pipe(th.load_data, processed_data_dir, \"val\")\n\nQuery execution start time = 2023-06-05 18:49:10.140...done at 2023-06-05 18:49:14.773 (4.633 seconds).\nQuery returned 21,208 rows\nGot 21,177 rows and 28 columns after dropping duplicates\nTransformed data has 21,177 rows & 28 columns\nCPU times: user 927 ms, sys: 97.1 ms, total: 1.02 s\nWall time: 4.65 s\n\n\n\n\nTest\n\nquery_test = sqlh.get_sql_query(test_start_date, test_end_date, train_start_date, test_end_date)\ndf_test, cat_mapper_dicts_test = th.extract_data(query_test, gcp_auth_dict).pipe(\n    th.transform_data,\n    datatypes_dict=dtypes_dict,\n    duplicate_cols=[\"fullvisitorid\"],\n    column_mapper_dict={'last_action': action_mapper},\n)\n# df_test.pipe(th.load_data, processed_data_dir, \"test\")\n\nQuery execution start time = 2023-06-05 18:49:14.814...done at 2023-06-05 18:49:19.494 (4.680 seconds).\nQuery returned 20,180 rows\nGot 20,164 rows and 28 columns after dropping duplicates\nTransformed data has 20,164 rows & 28 columns\nCPU times: user 864 ms, sys: 43.8 ms, total: 908 ms\nWall time: 4.7 s\n\n\n\n\nCombine Data and Split for ML Validation\nCreate combined training and validation data, where only the training data has been shuffled\n\ndf_train, df_train_val = th.create_combined_validation_data(df_train, df_val, dtypes_dict)\n\nCPU times: user 130 ms, sys: 1.19 ms, total: 131 ms\nWall time: 133 ms\n\n\n\n\nCombine Data for ML Evaluation\nCreate combined training and validation data that has been shuffled\n\ndf_train_val_eval = (\n    pd.concat([df_train, df_val])\n    .pipe(th.set_datatypes, dtypes_dict)\n    .pipe(th.shuffle_data)\n)\n\nCPU times: user 83.4 ms, sys: 64 µs, total: 83.4 ms\nWall time: 83 ms\n\n\n\n\nCombine Data for Inference\nShuffle all available data\n\ndf_all = (\n    pd.concat([df_train, df_val, df_test])\n    .pipe(th.set_datatypes, dtypes_dict)\n    .pipe(th.shuffle_data)\n)\n\nCPU times: user 69.1 ms, sys: 23 ms, total: 92 ms\nWall time: 91.6 ms\n\n\n\n\nSeparate Features and Label\n\n(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    X_train_val,\n    y_train_val,\n    X_train_val_eval,\n    y_train_val_eval,\n    X_test,\n    y_test,\n    X,\n    y,\n) = xyh.get_feats_label(\n    df_train,\n    df_val,\n    df_train_val,\n    df_train_val_eval,\n    df_test,\n    df_all,\n    cols_to_drop,\n    label,\n)\n\nCPU times: user 60.2 ms, sys: 281 µs, total: 60.4 ms\nWall time: 60 ms"
  },
  {
    "objectID": "notebooks/02-train/notebooks/04_train.html#exploratory-data-analysis",
    "href": "notebooks/02-train/notebooks/04_train.html#exploratory-data-analysis",
    "title": "ML Development",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nGet features that will be used in ML development (excludes ID features such as fullvisitorid, etc.)\n\n\n\n\n\n\n\n\n\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nhits\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntime_on_site\nbounces\nlast_action\nsource\nmedium\nchannelGrouping\nbrowser\nos\ndeviceCategory\n\n\n\n\n35089\n4\n12\n21\n4\n11\n5\n18\n0\n19\n0\n5\n144\n0\nUnknown\nbing\norganic\nOrganic Search\nChrome\nChrome OS\ndesktop\n\n\n52573\n4\n10\n12\n4\n18\n1\n0\n0\n0\n0\n1\n0\n1\nUnknown\nbaidu\norganic\nOrganic Search\nSafari (in-app)\niOS\nmobile\n\n\n40348\n4\n11\n10\n5\n12\n1\n0\n0\n0\n0\n1\n0\n1\nUnknown\ngoogle\ncpc\nPaid Search\nChrome\nWindows\ndesktop\n\n\n91912\n4\n11\n25\n6\n10\n1\n0\n0\n12\n0\n1\n0\n1\nUnknown\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\n\n\n9236\n4\n12\n16\n6\n21\n2\n9\n0\n9\n0\n2\n42\n0\nUnknown\nmall.googleplex.com\nreferral\nReferral\nChrome\nMacintosh\ndesktop\n\n\n\n\n\n\n\n\nMulti-Collinearity\nCheck for multi-collinearity among all features using feature correlation heatmap\n\n\n\n\n\n\n\n \nquarter\nmonth\nday_of_month\nday_of_week\nhour\nhits\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntime_on_site\nbounces\nlast_action\nsource\nmedium\nchannelGrouping\nbrowser\nos\n\n\n\n\nquarter\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nmonth\n0.749119\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nday_of_month\n-0.009950\n-0.058842\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nday_of_week\n-0.044626\n-0.011715\n-0.053364\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nhour\n0.008207\n0.008810\n-0.006907\n-0.022011\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nhits\n-0.016884\n-0.012355\n-0.012070\n-0.003050\n0.032749\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\npromos_displayed\n-0.022550\n-0.030020\n-0.032407\n-0.001648\n0.022424\n0.427202\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\npromos_clicked\n-0.051911\n-0.097672\n-0.043572\n0.006453\n0.026423\n0.254160\n0.483343\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nproduct_views\n-0.239557\n-0.187650\n-0.032899\n0.002638\n0.039686\n0.743049\n0.347107\n0.250329\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nproduct_clicks\n-0.036241\n-0.028998\n-0.005506\n-0.001997\n0.023777\n0.849215\n0.199485\n0.119974\n0.566139\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\npageviews\n-0.009374\n-0.002903\n-0.010889\n-0.003138\n0.034972\n0.981484\n0.460869\n0.228503\n0.768719\n0.774069\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\ntime_on_site\n0.012140\n0.015540\n-0.005688\n-0.004870\n0.016571\n0.688533\n0.326092\n0.163338\n0.538186\n0.503421\n0.715903\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nbounces\n-0.010927\n-0.014601\n0.013597\n0.007460\n-0.043026\n-0.342119\n-0.362501\n-0.204097\n-0.329338\n-0.205423\n-0.371699\n-0.304952\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nlast_action\n0.028807\n0.055180\n0.010347\n-0.004456\n0.027820\n0.632253\n0.191649\n0.060531\n0.372535\n0.483821\n0.647277\n0.511162\n-0.341094\nnan\nnan\nnan\nnan\nnan\nnan\n\n\nsource\n-0.006844\n-0.034052\n-0.018048\n-0.006850\n0.043814\n0.067108\n0.141754\n0.056011\n0.117656\n0.042449\n0.075385\n0.029814\n-0.198614\n0.004883\nnan\nnan\nnan\nnan\nnan\n\n\nmedium\n-0.041788\n-0.090519\n-0.018021\n0.000576\n0.024242\n0.008459\n0.117102\n0.053224\n0.072237\n0.004428\n0.010994\n-0.011637\n-0.123995\n-0.076290\n0.919819\nnan\nnan\nnan\nnan\n\n\nchannelGrouping\n-0.034776\n-0.076510\n-0.010927\n-0.002448\n0.005117\n-0.004237\n0.135169\n0.036611\n0.023897\n-0.006828\n-0.003178\n-0.017264\n-0.086592\n-0.070599\n0.930221\n0.843306\nnan\nnan\nnan\n\n\nbrowser\n-0.002034\n-0.000950\n0.019221\n0.013127\n0.033745\n-0.086703\n0.016967\n0.059330\n-0.057734\n-0.069180\n-0.093942\n-0.068708\n0.075460\n-0.112150\n-0.048849\n0.020474\n-0.015950\nnan\nnan\n\n\nos\n-0.007368\n-0.009790\n0.008857\n0.001032\n-0.002016\n-0.055942\n-0.006916\n-0.005981\n-0.026390\n-0.040013\n-0.058665\n-0.045087\n0.037829\n-0.079723\n0.014540\n0.099977\n0.061564\n0.507421\nnan\n\n\ndeviceCategory\n0.011509\n0.036309\n0.023685\n0.022469\n0.062551\n-0.074306\n0.067218\n0.144496\n-0.045580\n-0.073711\n-0.081569\n-0.056139\n0.063508\n-0.122404\n-0.109912\n-0.096721\n-0.153655\n0.450195\n0.087484\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\npageviews is\n\nhighly correlated (&gt;0.7) to hits, product_clicks and product_views\nmoderately correlated (&gt;0.5 and &lt;0.7) to last_action\n\nproduct_clicks and product_views are highly correlated to hits.\nlast_action is moderately correlated to time_on_site.\nTo allow ML model explainability, multi-collinearity must be removed from the features. So, highly-correlated features will have to be dropped. Moderately correlated features will be permitted.\n\n\n\n\n\nCorrelation to Label\n\n\n\n\n\n\n\n \nfeature\ncorr\nfeature_type\n\n\n\n\n0\nlast_action\n0.224859\ncategorical\n\n\n1\npageviews\n0.146607\nnumerical\n\n\n2\nhits\n0.142694\nnumerical\n\n\n3\ntime_on_site\n0.126974\nnumerical\n\n\n4\nproduct_clicks\n0.106253\nnumerical\n\n\n5\nproduct_views\n0.068049\nnumerical\n\n\n6\nmonth\n0.050306\ndatetime\n\n\n7\nquarter\n0.043374\ndatetime\n\n\n8\npromos_displayed\n0.032403\nnumerical\n\n\n9\nsource\n0.021992\ncategorical\n\n\n10\nday_of_month\n0.005510\ndatetime\n\n\n11\nhour\n0.002352\ndatetime\n\n\n12\nchannelGrouping\n-0.009920\ncategorical\n\n\n13\nday_of_week\n-0.013513\ndatetime\n\n\n14\npromos_clicked\n-0.018780\nnumerical\n\n\n15\nmedium\n-0.048501\ncategorical\n\n\n16\nos\n-0.073147\ncategorical\n\n\n17\nbounces\n-0.096606\ncategorical\n\n\n18\nbrowser\n-0.098377\ncategorical\n\n\n19\ndeviceCategory\n-0.108512\ncategorical\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nUnfortunately, the extracted features show a weak correlation (&lt;0.5) to the label.\nThe first-time visitors’ last_action (i.e. the last action they performed during their first visit) shows the highest correlation (0.23) to the label. This is a categorical feature (hilighted in yellow). The other categorcal features show a weak correlation to the label.\nThe numerical features (hilighted in light-green above) show the next strongest correlations (&gt;0.06 and &lt;0.15) to the label, but some of these features are highly correlated to eachother (as seen above) and so a subset of these numerical features will be excluded in order to support model explainability.\nThe datetime features (not hilighted above) are treated like categoricals in the extracted data, and they show a poor correlation to the label. Other approaches to encoding datetime attributes (eg. using trigonometric functions) should be explored during feature pre-processing.\n\n\n\n\n\nCardinality of Categorical Features\n\n\n\n\n\n\n\n\n\ncolumn\nlen_mapper_dict_train\nmapper_dict_train\n\n\n\n\n0\nsource\n116\n{'(direct)': 0, '(not set)': 1, '0.muppet1.frontend.gws.muppet-exp.ij-q.borg.google.com:14634': 2, '0.shared.bow.cat2.ads-bow.lf.borg.google.com:9...\n\n\n1\nbrowser\n26\n{'Amazon Silk': 0, 'Android Browser': 1, 'Android Webview': 2, 'Apple-iPhone7C2': 3, 'BlackBerry': 4, 'Chrome': 5, 'Coc Coc': 6, 'Edge': 7, 'Firef...\n\n\n2\nos\n15\n{'(not set)': 0, 'Android': 1, 'BlackBerry': 2, 'Chrome OS': 3, 'FreeBSD': 4, 'Linux': 5, 'Macintosh': 6, 'Nintendo Wii': 7, 'Nokia': 8, 'Samsung'...\n\n\n3\nchannelGrouping\n8\n{'(Other)': 0, 'Affiliates': 1, 'Direct': 2, 'Display': 3, 'Organic Search': 4, 'Paid Search': 5, 'Referral': 6, 'Social': 7}\n\n\n4\nlast_action\n7\n{'Unknown': 0, 'Click through of product lists': 1, 'Product detail views': 2, 'Add product(s) to cart': 3, 'Remove product(s) from cart': 4, 'Che...\n\n\n5\nmedium\n7\n{'(none)': 0, '(not set)': 1, 'affiliate': 2, 'cpc': 3, 'cpm': 4, 'organic': 5, 'referral': 6}\n\n\n6\ndeviceCategory\n3\n{'desktop': 0, 'mobile': 1, 'tablet': 2}\n\n\n7\nbounces\n2\n{0: 0, 1: 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nAt least three of the categorical features have a problem with high-cardinality (&gt;10 categories), which will lead to a sparse matrix and suffer from the curse of dimensionality. To overcome this, only the most commonly occurring values in each categorical feature will have to be kept and all the other (infrequenly occurring) values with will be replaced with an other value. Based on the training data, this will have to be done for the source, browser and os features.\n\n\n\nShow the frequency of categories for every categorical feature\n\n\n\n\n\n\n\n \nbounces\nproportion\nshowing_all_cats\n\n\n\n\n0\n0\n70.772871\nTrue\n\n\n1\n1\n29.227129\nTrue\n\n\n\n\n\n\n\n\n\n\n \nlast_action\nproportion\nshowing_all_cats\n\n\n\n\n0\nUnknown\n72.950049\nTrue\n\n\n1\nProduct detail views\n15.910147\nTrue\n\n\n2\nAdd product(s) to cart\n5.032901\nTrue\n\n\n3\nCompleted purchase\n3.344102\nTrue\n\n\n4\nCheck out\n1.765513\nTrue\n\n\n5\nRemove product(s) from cart\n0.952988\nTrue\n\n\n6\nClick through of product lists\n0.044300\nTrue\n\n\n\n\n\n\n\n\n\n\n \nsource\nproportion\nshowing_all_cats\n\n\n\n\n0\ngoogle\n51.571320\nFalse\n\n\n1\n(direct)\n25.527726\nFalse\n\n\n2\nmall.googleplex.com\n13.969926\nFalse\n\n\n3\nsites.google.com\n1.302120\nFalse\n\n\n4\nmoma.corp.google.com\n1.233338\nFalse\n\n\n5\nPartners\n1.064940\nFalse\n\n\n\n\n\n\n\n\n\n\n \nmedium\nproportion\nshowing_all_cats\n\n\n\n\n0\norganic\n41.693769\nTrue\n\n\n1\nreferral\n27.108297\nTrue\n\n\n2\n(none)\n23.258528\nTrue\n\n\n3\ncpc\n6.057201\nTrue\n\n\n4\naffiliate\n0.970276\nTrue\n\n\n5\ncpm\n0.909769\nTrue\n\n\n6\n(not set)\n0.002161\nTrue\n\n\n\n\n\n\n\n\n\n\n \nchannelGrouping\nproportion\nshowing_all_cats\n\n\n\n\n0\nOrganic Search\n41.693769\nTrue\n\n\n1\nDirect\n23.258528\nTrue\n\n\n2\nReferral\n17.463885\nTrue\n\n\n3\nSocial\n9.644412\nTrue\n\n\n4\nPaid Search\n6.057201\nTrue\n\n\n5\nAffiliates\n0.970276\nTrue\n\n\n6\nDisplay\n0.909769\nTrue\n\n\n7\n(Other)\n0.002161\nTrue\n\n\n\n\n\n\n\n\n\n\n \nbrowser\nproportion\nshowing_all_cats\n\n\n\n\n0\nChrome\n74.641671\nFalse\n\n\n1\nSafari\n16.544249\nFalse\n\n\n2\nFirefox\n2.806053\nFalse\n\n\n3\nInternet Explorer\n2.394989\nFalse\n\n\n4\nOpera\n1.210476\nFalse\n\n\n5\nEdge\n1.069848\nFalse\n\n\n\n\n\n\n\n\n\n\n \nos\nproportion\nshowing_all_cats\n\n\n\n\n0\nMacintosh\n33.073657\nTrue\n\n\n1\nWindows\n27.647459\nTrue\n\n\n2\niOS\n14.586552\nTrue\n\n\n3\nAndroid\n12.410455\nTrue\n\n\n4\nLinux\n7.023155\nTrue\n\n\n5\nChrome OS\n5.122581\nTrue\n\n\n6\n(not set)\n0.057266\nTrue\n\n\n7\nWindows Phone\n0.042139\nTrue\n\n\n8\nNintendo Wii\n0.011885\nTrue\n\n\n9\nBlackBerry\n0.010805\nTrue\n\n\n10\nXbox\n0.009724\nTrue\n\n\n11\nSamsung\n0.001080\nTrue\n\n\n12\nSunOS\n0.001080\nTrue\n\n\n13\nFreeBSD\n0.001080\nTrue\n\n\n14\nNokia\n0.001080\nTrue\n\n\n\n\n\n\n\n\n\n\n \ndeviceCategory\nproportion\nshowing_all_cats\n\n\n\n\n0\ndesktop\n72.776091\nTrue\n\n\n1\nmobile\n23.543776\nTrue\n\n\n2\ntablet\n3.680133\nTrue\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nEach of the top two to four categories account for &gt;10% of observations (visitors) in all nine of the categorical features. Four (channelGrouping, os, medium and last_action) of the nine categorical features have categories that occur with a frequency of between 5 - 10%. Based on the training data, a minimum required frequency of 10% appears to be a reasonable starting point for replacing infrequently occurring categories.\n\n\n\n\n\nDistribution of Numerical Features\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nFor numerical features among return visitors who did not make a purchase\n\nthere is a wider range of outliers\nthe IQR range (height of the box) is more well-defined (smaller)\naverage value (horizontal line inside the box) is generally higher\n\nthan for the same features among those visitors who did.\nBased on univariate analaysis, the overall numerical features contain approximately 8-11% of outliers. promotions clicked is the only feature with more outliers (15%).\n\n\n\n\n\nConversion Rates Among Categorical Features\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n(bounces) A smaller fraction of return purchasers bounced off the page than for non-return purchasers. It seems keeping visitors engaged during their first visit is a good indicator for getting their business during a return visit to the store.\n(desktop) The largest fraction of return purchasers used a desktop computer during their first visit. This dataset captures visitors from as early as August 2016 to as late as August 2017. With increased mobile phone use since then, these proportions might have changed since then.\n(operating system) The largest fraction of return purchasers used a desktop (Mac, Windows and Linux) to access the store during their first visit. This is not surprising based on the above observation about the high fraction desktop use among return purcasers.\n(browser) Chrome was the dominant browser, ahead of a grouping of Safari and Firefox. The Microsoft Edge browser was only released in July 2015, so it not surprising that Edge is a little further back from Safari and Firefox.\n(channel, medium and source) Based on source, medium and channel, return purchasers\n\ndirectly accessed the store’s site (manually typed in the URL of the store)\n\nmedium=None\nchannel=Direct\nsource=(direct)\n\nrelied on referrals (visitor clicked a link to the store on another site - 1, 2)\n\nchannel=Referral\nmedium=Referral\n\naccessed the store site by clicking on a link from a non-paid search result in a search engine\n\nchannel=Organic Search\nmedium=organic\n\nduring their first visit. From the chart for the source feature, we see that the dominant search engine (contributing to the organic search results) is Google Search.\n\n(last action) Qualitatively, the largest fraction (ratio of red bar to grey bar) of return purchasers\n\nchecked out\ncompleted a purchase\nremoved a product from their shopping cart\n\nas the last action during their first visit to the store. The largest number (length of red bar) of return purchasers\n\nperformed an action that could not be tracked by the embedded Google Analytics tracking script\nviewed product details\nadded a product to their shopping cart\n\nas the last action during their first visit to the store.\n\n\n\n\n\nNumerical Feature Interactions\n\n\n\n\n\n\nSlow Execution\n\n\n\nAs the length of the data grows (as more months of training data are used), it takes longer to retrieve feature interactions.\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nInter-feature correlations are observed between hits and product clicks or between hits and page views. Qualitatively, the other feature interactions do not show evidence of a correlation."
  },
  {
    "objectID": "notebooks/02-train/notebooks/04_train.html#ml-pipeline",
    "href": "notebooks/02-train/notebooks/04_train.html#ml-pipeline",
    "title": "ML Development",
    "section": "ML Pipeline",
    "text": "ML Pipeline\nAs indicated in the EDA section, all non-ID features will be passed into the ML pipeline. The ML pipeline will first perform feature engineering and preprocessing. This step is named preprocessor. The next step of the pipeline (named select) will select features based on feature-to-feature correlations.\n\nFeature Selection\nHighly correlated features will be dropped and all remaining features (low or moderately correlated to eachother) will be selected.\n\n\nFeature Engineering - SQL\nAs part of the data retrieval from the Google Analytics tracking data for the merchandise store’s site, the following features of the first visit were engineered using SQL\n\nlast_action\n\nlast action performed during the first visit\n\nadded_to_cart\n\nnumber of products added to shopping cart during the first visit\n\n\n\n\nFeature Engineering - Python\nFeature engineering is performed on the raw data, before pre-processing. For each numerical feature, we’ll extract three new features as the ratio to the mean. The intuition behind this transformation is that visitors with a higher-than-average number of pages viewed on their first visit have a higher likelihood of making a purchase on a return visit. This is implemented in src/feature_helpers.py using a scikit-learn custom transformer.\n\n\nFeature Pre-Processing\nA feature processing pipeline is defined with the following steps\n\nnumerical columns\n\nfeature engineering\nnormalization\n\ncategorical columns\n\nfrequency encoding\ndummy encoding\n\nfeature selection\n\ndrop highly correlated features (r &gt; 0.7)\nkeep weakly (r &lt; 0.5) and moderately (r &gt; 0.5 and r &lt; 0.7) corelated features\n\n\n\n\nPipeline\nIn the overall ML pipeline, preprocessing is performed in combination with under-sampling or over-sampling in order to handle the class imbalance. If the observations are undersampled, then this is done before preprocessing features. By comparison , if they are oversampled, then this is done after preprocessing features.\nAfter preprocessing, the class-imbalance increases in favor of the minority class. The class imbalances can be the same after preprocessing using under- and over-sampling. The sampling_strategy hyperparameter determines the imbalance in the label after resampling. If hyperparameter is the same for over- and under-sampling then both approaches produce the same class imbalance after resampling. For this reason, equal class imbalances is possible with both approaches. With the same sampling_strategy hyperparameter, the number of remaining observations after resampling can be different depending on whether under- or over-sampling is used, since both resampling approaches work differently.\nOver-sampling adds more observations in the minority class and so there are more observations in this class after over-sampling. For under-sampling, the number of such minority-class observations are unchanged, since observations are removed from the majority class.\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                                                     'last_action',\n                                                                                                     'source',\n                                                                                                     'medium',\n                                                                                                     'channelGrouping',\n                                                                                                     'browser',\n                                                                                                     'os',\n                                                                                                     'deviceCategory'])])),\n                                                                  ('dummy',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures()),\n                ('resampler', RandomOverSampler(random_state=88))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                                                     'last_action',\n                                                                                                     'source',\n                                                                                                     'medium',\n                                                                                                     'channelGrouping',\n                                                                                                     'browser',\n                                                                                                     'os',\n                                                                                                     'deviceCategory'])])),\n                                                                  ('dummy',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures()),\n                ('resampler', RandomOverSampler(random_state=88))])preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('aboveavg',\n                                                  AboveAveragePagePromoEngager(cols=['hits',\n                                                                                     'promos_displayed',\n                                                                                     'promos_clicked',\n                                                                                     'product_views',\n                                                                                     'product_clicks',\n                                                                                     'pageviews',\n                                                                                     'time_on_site'])),\n                                                 ('scaler', MinMaxScaler())]),\n                                 ['hits', 'promos_displayed', 'promos_clicked',\n                                  'product_views', 'product_clicks',\n                                  'pageviews', 'time_on_site']),\n                                ('cat',\n                                 P...\n                                                  ColumnTransformer(remainder='passthrough',\n                                                                    transformers=[('fe',\n                                                                                   RareLabelEncoder(variables=['source',\n                                                                                                               'browser']),\n                                                                                   ['bounces',\n                                                                                    'last_action',\n                                                                                    'source',\n                                                                                    'medium',\n                                                                                    'channelGrouping',\n                                                                                    'browser',\n                                                                                    'os',\n                                                                                    'deviceCategory'])])),\n                                                 ('dummy',\n                                                  OneHotEncoder(drop='first',\n                                                                dtype=&lt;class 'int'&gt;,\n                                                                handle_unknown='ignore'))]),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])num['hits', 'promos_displayed', 'promos_clicked', 'product_views', 'product_clicks', 'pageviews', 'time_on_site']AboveAveragePagePromoEngagerAboveAveragePagePromoEngager(cols=['hits', 'promos_displayed', 'promos_clicked',\n                                   'product_views', 'product_clicks',\n                                   'pageviews', 'time_on_site'])MinMaxScalerMinMaxScaler()cat['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']rarecats: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('fe',\n                                 RareLabelEncoder(variables=['source',\n                                                             'browser']),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])fe['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']RareLabelEncoderRareLabelEncoder(variables=['source', 'browser'])remainderpassthroughpassthroughOneHotEncoderOneHotEncoder(drop='first', dtype=&lt;class 'int'&gt;, handle_unknown='ignore')DropCorrelatedFeaturesDropCorrelatedFeatures()RandomOverSamplerRandomOverSampler(random_state=88)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/04_train.html#model-development",
    "href": "notebooks/02-train/notebooks/04_train.html#model-development",
    "title": "ML Development",
    "section": "Model Development",
    "text": "Model Development\n\nDefine Cross-Validator\nA single validation fold (the validation data split) is used to validate the ML pipeline. A cross-validator is defined using sklearn’s PredefinedSplit to train the pipeline on the training data split and evaluate predictions on the validation data split.\n\n\nAppend ML Model to Pipeline\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                                                     'channelGrouping',\n                                                                                                     'browser',\n                                                                                                     'os',\n                                                                                                     'deviceCategory'])])),\n                                                                  ('dummy',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures()),\n                ('resampler', RandomOverSampler(random_state=88)),\n                ['clf', BetaDistClassifier(random_state=88)]])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                                                     'channelGrouping',\n                                                                                                     'browser',\n                                                                                                     'os',\n                                                                                                     'deviceCategory'])])),\n                                                                  ('dummy',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures()),\n                ('resampler', RandomOverSampler(random_state=88)),\n                ['clf', BetaDistClassifier(random_state=88)]])preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('aboveavg',\n                                                  AboveAveragePagePromoEngager(cols=['hits',\n                                                                                     'promos_displayed',\n                                                                                     'promos_clicked',\n                                                                                     'product_views',\n                                                                                     'product_clicks',\n                                                                                     'pageviews',\n                                                                                     'time_on_site'])),\n                                                 ('scaler', MinMaxScaler())]),\n                                 ['hits', 'promos_displayed', 'promos_clicked',\n                                  'product_views', 'product_clicks',\n                                  'pageviews', 'time_on_site']),\n                                ('cat',\n                                 P...\n                                                  ColumnTransformer(remainder='passthrough',\n                                                                    transformers=[('fe',\n                                                                                   RareLabelEncoder(variables=['source',\n                                                                                                               'browser']),\n                                                                                   ['bounces',\n                                                                                    'last_action',\n                                                                                    'source',\n                                                                                    'medium',\n                                                                                    'channelGrouping',\n                                                                                    'browser',\n                                                                                    'os',\n                                                                                    'deviceCategory'])])),\n                                                 ('dummy',\n                                                  OneHotEncoder(drop='first',\n                                                                dtype=&lt;class 'int'&gt;,\n                                                                handle_unknown='ignore'))]),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])num['hits', 'promos_displayed', 'promos_clicked', 'product_views', 'product_clicks', 'pageviews', 'time_on_site']AboveAveragePagePromoEngagerAboveAveragePagePromoEngager(cols=['hits', 'promos_displayed', 'promos_clicked',\n                                   'product_views', 'product_clicks',\n                                   'pageviews', 'time_on_site'])MinMaxScalerMinMaxScaler()cat['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']rarecats: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('fe',\n                                 RareLabelEncoder(variables=['source',\n                                                             'browser']),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])fe['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']RareLabelEncoderRareLabelEncoder(variables=['source', 'browser'])remainderpassthroughpassthroughOneHotEncoderOneHotEncoder(drop='first', dtype=&lt;class 'int'&gt;, handle_unknown='ignore')DropCorrelatedFeaturesDropCorrelatedFeatures()RandomOverSamplerRandomOverSampler(random_state=88)BetaDistClassifierBetaDistClassifier(random_state=88)\n\n\n\n\nHyper-Parameter Tuning\nPerform hyper-parameter tuning\n\n\nFitting 1 folds for each of 30 candidates, totalling 30 fits\n[CV] END clf__a=0.2, clf__b=2.31, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   1.0s\n[CV] END clf__a=0.2, clf__b=2.25, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.2, clf__b=2.35, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.2, clf__b=2.4, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.2, clf__b=2.5, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.3, clf__b=2.31, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   1.0s\n[CV] END clf__a=0.3, clf__b=2.25, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.3, clf__b=2.35, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.3, clf__b=2.4, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.3, clf__b=2.5, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.4, clf__b=2.31, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.4, clf__b=2.25, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   1.0s\n[CV] END clf__a=0.4, clf__b=2.35, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   1.0s\n[CV] END clf__a=0.4, clf__b=2.4, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   1.0s\n[CV] END clf__a=0.4, clf__b=2.5, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.35, clf__b=2.31, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.35, clf__b=2.25, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.35, clf__b=2.35, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   1.0s\n[CV] END clf__a=0.35, clf__b=2.4, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   1.0s\n[CV] END clf__a=0.35, clf__b=2.5, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.15, clf__b=2.31, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.15, clf__b=2.25, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.15, clf__b=2.35, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.15, clf__b=2.4, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.15, clf__b=2.5, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.45, clf__b=2.31, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.45, clf__b=2.25, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.45, clf__b=2.35, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.45, clf__b=2.4, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\n[CV] END clf__a=0.45, clf__b=2.5, preprocessor__cat__rarecats__fe__ignore_format=True, preprocessor__cat__rarecats__fe__n_categories=1, preprocessor__cat__rarecats__fe__replace_with=other, preprocessor__cat__rarecats__fe__tol=0.1, resampler__sampling_strategy=0.1, select__threshold=0.7; total time=   0.9s\nDone with hyperparameter tuning\nCPU times: user 27.6 s, sys: 1.34 s, total: 29 s\nWall time: 29 s\n\n\n\n\nGet Validation Hyper-Parameters and Metrics\n\n\n\n\n\n\n\n\n\nresampling_approach\nclf\nparam_clf__a\nparam_clf__b\nparam_preprocessor__cat__rarecats__fe__ignore_format\nparam_preprocessor__cat__rarecats__fe__n_categories\nparam_preprocessor__cat__rarecats__fe__replace_with\nparam_preprocessor__cat__rarecats__fe__tol\nparam_resampler__sampling_strategy\nparam_select__threshold\nmean_test_accuracy\nmean_test_balanced_accuracy\nmean_test_precision\nmean_test_recall\nmean_test_roc_auc\nmean_test_f1\nmean_test_fbeta05\nmean_test_fbeta2\nmean_test_pr_auc\nmean_test_avg_precision\nmean_fit_time\nmean_score_time\n\n\n\n\n0\nos\nBetaDistClassifier\n0.2\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.937904\n0.501539\n0.501726\n0.501539\n0.501539\n0.501568\n0.501646\n0.501536\n0.034591\n0.034205\n0.836648\n0.118831\n\n\n1\nos\nBetaDistClassifier\n0.2\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.935543\n0.496308\n0.496082\n0.496308\n0.496308\n0.496183\n0.496119\n0.496255\n0.032233\n0.033905\n0.812090\n0.115707\n\n\n2\nos\nBetaDistClassifier\n0.2\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.939179\n0.502867\n0.503343\n0.502867\n0.502867\n0.502983\n0.503166\n0.502888\n0.035375\n0.034318\n0.802264\n0.122692\n\n\n3\nos\nBetaDistClassifier\n0.2\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.940171\n0.501376\n0.501676\n0.501376\n0.501376\n0.501346\n0.501493\n0.501327\n0.035538\n0.034193\n0.800810\n0.117762\n\n\n4\nos\nBetaDistClassifier\n0.2\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.94206\n0.496341\n0.495071\n0.496341\n0.496341\n0.495448\n0.495144\n0.495935\n0.034335\n0.033921\n0.789521\n0.116408\n\n\n5\nos\nBetaDistClassifier\n0.3\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.919866\n0.498214\n0.498741\n0.498214\n0.498214\n0.497814\n0.498251\n0.497846\n0.034406\n0.033984\n0.829881\n0.126483\n\n\n6\nos\nBetaDistClassifier\n0.3\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.91741\n0.498947\n0.499295\n0.498947\n0.498947\n0.498194\n0.498695\n0.498343\n0.035797\n0.034027\n0.815624\n0.116943\n\n\n7\nos\nBetaDistClassifier\n0.3\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.921188\n0.497562\n0.498231\n0.497562\n0.497562\n0.497366\n0.497787\n0.497321\n0.033549\n0.033949\n0.822759\n0.117708\n\n\n8\nos\nBetaDistClassifier\n0.3\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.923785\n0.49757\n0.498133\n0.49757\n0.49757\n0.497508\n0.497816\n0.497444\n0.03559\n0.03395\n0.800312\n0.122950\n\n\n9\nos\nBetaDistClassifier\n0.3\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.928413\n0.505978\n0.504998\n0.505978\n0.505978\n0.505274\n0.505064\n0.505637\n0.032603\n0.034598\n0.794850\n0.117124\n\n\n10\nos\nBetaDistClassifier\n0.4\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.906266\n0.507876\n0.50426\n0.507876\n0.507876\n0.502949\n0.503289\n0.504715\n0.035126\n0.034734\n0.799199\n0.126786\n\n\n11\nos\nBetaDistClassifier\n0.4\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.901214\n0.499916\n0.499957\n0.499916\n0.499916\n0.496766\n0.498245\n0.497401\n0.035747\n0.034088\n0.857989\n0.126014\n\n\n12\nos\nBetaDistClassifier\n0.4\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.908013\n0.503436\n0.501928\n0.503436\n0.503436\n0.500249\n0.500895\n0.501258\n0.034785\n0.034344\n0.846374\n0.121489\n\n\n13\nos\nBetaDistClassifier\n0.4\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.910091\n0.499835\n0.499903\n0.499835\n0.499835\n0.498023\n0.498867\n0.498445\n0.031577\n0.034083\n0.829011\n0.126152\n\n\n14\nos\nBetaDistClassifier\n0.4\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.914624\n0.502849\n0.501792\n0.502849\n0.502849\n0.500863\n0.501182\n0.501556\n0.033816\n0.0343\n0.822242\n0.116907\n\n\n15\nos\nBetaDistClassifier\n0.35\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.912877\n0.493928\n0.496221\n0.493928\n0.493928\n0.493954\n0.495131\n0.493554\n0.034369\n0.033777\n0.804587\n0.123040\n\n\n16\nos\nBetaDistClassifier\n0.35\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.911224\n0.50443\n0.502615\n0.50443\n0.50443\n0.501491\n0.501851\n0.502526\n0.034113\n0.034427\n0.802549\n0.117778\n\n\n17\nos\nBetaDistClassifier\n0.35\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.915758\n0.503436\n0.502203\n0.503436\n0.503436\n0.50147\n0.501686\n0.502194\n0.034001\n0.034348\n0.839375\n0.129014\n\n\n18\nos\nBetaDistClassifier\n0.35\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.91708\n0.498108\n0.498739\n0.498108\n0.498108\n0.497504\n0.498086\n0.497565\n0.031889\n0.033978\n0.835043\n0.127018\n\n\n19\nos\nBetaDistClassifier\n0.35\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.920905\n0.497415\n0.498135\n0.497415\n0.497415\n0.497227\n0.497671\n0.497171\n0.034196\n0.033941\n0.795141\n0.119308\n\n\n20\nos\nBetaDistClassifier\n0.15\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.943618\n0.493139\n0.489924\n0.493139\n0.493139\n0.491285\n0.49039\n0.492354\n0.03575\n0.033903\n0.823570\n0.117448\n\n\n21\nos\nBetaDistClassifier\n0.15\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.943476\n0.500415\n0.50058\n0.500415\n0.500415\n0.500044\n0.500224\n0.500184\n0.034897\n0.034122\n0.782312\n0.117310\n\n\n22\nos\nBetaDistClassifier\n0.15\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.94546\n0.498101\n0.49705\n0.498101\n0.498101\n0.496981\n0.496824\n0.497554\n0.034968\n0.03399\n0.794685\n0.119732\n\n\n23\nos\nBetaDistClassifier\n0.15\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.946829\n0.500146\n0.500239\n0.500146\n0.500146\n0.49931\n0.499559\n0.499671\n0.033555\n0.034103\n0.787629\n0.117135\n\n\n24\nos\nBetaDistClassifier\n0.15\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.948387\n0.49828\n0.49689\n0.49828\n0.49828\n0.496613\n0.496417\n0.497471\n0.032713\n0.034001\n0.785883\n0.116174\n\n\n25\nos\nBetaDistClassifier\n0.45\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.893469\n0.492567\n0.496543\n0.492567\n0.492567\n0.490987\n0.493861\n0.490442\n0.034786\n0.033696\n0.788639\n0.120124\n\n\n26\nos\nBetaDistClassifier\n0.45\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.890494\n0.497707\n0.498981\n0.497707\n0.497707\n0.493601\n0.496246\n0.494012\n0.034961\n0.033951\n0.794289\n0.119293\n\n\n27\nos\nBetaDistClassifier\n0.45\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.898239\n0.506393\n0.503091\n0.506393\n0.506393\n0.500318\n0.501405\n0.502142\n0.03569\n0.034585\n0.822991\n0.118166\n\n\n28\nos\nBetaDistClassifier\n0.45\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.900647\n0.498954\n0.49947\n0.498954\n0.498954\n0.496052\n0.497671\n0.496538\n0.036071\n0.034027\n0.807132\n0.117922\n\n\n29\nos\nBetaDistClassifier\n0.45\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.906313\n0.493203\n0.496208\n0.493203\n0.493203\n0.492893\n0.494611\n0.492408\n0.034326\n0.033739\n0.790924\n0.117878\n\n\n\n\n\n\n\n\n\nGet Best Hyper-Parameters\n\n\n\n\n\n\n\n\n\nresampling_approach\nclf\nparam_clf__a\nparam_clf__b\nparam_preprocessor__cat__rarecats__fe__ignore_format\nparam_preprocessor__cat__rarecats__fe__n_categories\nparam_preprocessor__cat__rarecats__fe__replace_with\nparam_preprocessor__cat__rarecats__fe__tol\nparam_resampler__sampling_strategy\nparam_select__threshold\n...\nmean_test_precision\nmean_test_recall\nmean_test_roc_auc\nmean_test_f1\nmean_test_fbeta05\nmean_test_fbeta2\nmean_test_pr_auc\nmean_test_avg_precision\nmean_fit_time\nmean_score_time\n\n\n\n\n9\nos\nBetaDistClassifier\n0.3\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n...\n0.504998\n0.505978\n0.505978\n0.505274\n0.505064\n0.505637\n0.032603\n0.034598\n0.79485\n0.117124\n\n\n\n\n1 rows × 23 columns\n\n\n\n\n\nSet Best Hyper-Parameters in Pipeline\nSet best hyperparameters in new Pipeline\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                  ('dummy',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures(threshold=0.7)),\n                ('resampler',\n                 RandomOverSampler(random_state=88, sampling_strategy=0.1)),\n                ['clf', BetaDistClassifier(a=0.3, b=2.5, random_state=88)]])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                  ('dummy',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures(threshold=0.7)),\n                ('resampler',\n                 RandomOverSampler(random_state=88, sampling_strategy=0.1)),\n                ['clf', BetaDistClassifier(a=0.3, b=2.5, random_state=88)]])preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('aboveavg',\n                                                  AboveAveragePagePromoEngager(cols=['hits',\n                                                                                     'promos_displayed',\n                                                                                     'promos_clicked',\n                                                                                     'product_views',\n                                                                                     'product_clicks',\n                                                                                     'pageviews',\n                                                                                     'time_on_site'])),\n                                                 ('scaler', MinMaxScaler())]),\n                                 ['hits', 'promos_displayed', 'promos_clicked',\n                                  'product_views', 'product_clicks',\n                                  'pageviews', 'time_on_site']),\n                                ('cat',\n                                 P...\n                                                                                                    n_categories=1,\n                                                                                                    replace_with='other',\n                                                                                                    tol=0.1,\n                                                                                                    variables=['source',\n                                                                                                               'browser']),\n                                                                                   ['bounces',\n                                                                                    'last_action',\n                                                                                    'source',\n                                                                                    'medium',\n                                                                                    'channelGrouping',\n                                                                                    'browser',\n                                                                                    'os',\n                                                                                    'deviceCategory'])])),\n                                                 ('dummy',\n                                                  OneHotEncoder(drop='first',\n                                                                dtype=&lt;class 'int'&gt;,\n                                                                handle_unknown='ignore'))]),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])num['hits', 'promos_displayed', 'promos_clicked', 'product_views', 'product_clicks', 'pageviews', 'time_on_site']AboveAveragePagePromoEngagerAboveAveragePagePromoEngager(cols=['hits', 'promos_displayed', 'promos_clicked',\n                                   'product_views', 'product_clicks',\n                                   'pageviews', 'time_on_site'])MinMaxScalerMinMaxScaler()cat['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']rarecats: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('fe',\n                                 RareLabelEncoder(ignore_format=True,\n                                                  n_categories=1,\n                                                  replace_with='other', tol=0.1,\n                                                  variables=['source',\n                                                             'browser']),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])fe['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']RareLabelEncoderRareLabelEncoder(ignore_format=True, n_categories=1, replace_with='other',\n                 tol=0.1, variables=['source', 'browser'])remainderpassthroughpassthroughOneHotEncoderOneHotEncoder(drop='first', dtype=&lt;class 'int'&gt;, handle_unknown='ignore')DropCorrelatedFeaturesDropCorrelatedFeatures(threshold=0.7)RandomOverSamplerRandomOverSampler(random_state=88, sampling_strategy=0.1)BetaDistClassifierBetaDistClassifier(a=0.3, b=2.5, random_state=88)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/04_train.html#ml-evaluation-1",
    "href": "notebooks/02-train/notebooks/04_train.html#ml-evaluation-1",
    "title": "ML Development",
    "section": "ML Evaluation",
    "text": "ML Evaluation\n\nTrain Best Pipeline Using Combined and Shuffled Training and Validation Data\n\n_ = pipe.fit(X_train_val_eval, y_train_val_eval)\n\nCPU times: user 959 ms, sys: 52.4 ms, total: 1.01 s\nWall time: 1.01 s\n\n\n\n\nMake Predictions\nSoft predictions\n\ny_train_val_eval_pred_proba = pd.DataFrame(\n    pipe.predict_proba(X_train_val_eval)[:, 1], index=X_train_val_eval.index\n).astype(pd.Float32Dtype())\ny_test_pred_proba = pd.DataFrame(\n    pipe.predict_proba(X_test)[:, 1], index=X_test.index\n).astype(pd.Float32Dtype())\n\nConvert to hard labels, using discrimination threshold of 0.5\n\ny_train_val_eval_pred = (y_train_val_eval_pred_proba.squeeze() &gt;= 0.5).astype(\n    pd.Int8Dtype()\n)\ny_test_pred = (y_test_pred_proba.squeeze() &gt;= 0.5).astype(pd.Int8Dtype())\n\n\n\nGet Evaluation Metrics\n\n\n\n\n\n\n\nsplit\nmetric\ntrain_val\ntest\n\n\n\n\n0\naccuracy\n0.919483\n0.940538\n\n\n1\nbalanced_accuracy\n0.501968\n0.499218\n\n\n2\nprecision\n0.047763\n0.022135\n\n\n3\nrecall\n0.501968\n0.499218\n\n\n4\nroc_auc\n0.501968\n0.499218\n\n\n5\nf1\n0.502007\n0.498453\n\n\n6\nfbeta05\n0.502060\n0.498946\n\n\n7\nfbeta2\n0.501977\n0.498594\n\n\n8\npr_auc\n0.043754\n0.023227\n\n\n9\navg_precision\n0.043898\n0.023027\n\n\n\n\n\nCPU times: user 385 ms, sys: 0 ns, total: 385 ms\nWall time: 385 ms\n\n\n\n\nOptimal Discrimination Threshold Tuning\nDiscrimination or decision threshold is the probability at which the minority class is chosen over the majority class, when converting soft predictions to hard labels. By default, the threshold is set at 0.5. For imbalanced data, the default threshold of 0.5 can give poor model performance. One possible approach to improving the performance of a model when working with imbalanced data is to vary the discrimination threshold (or threshold tuning), which in-turn varies the sensitivity of the model to false positives, etc. In this way, it is possible to use a brute-force grid search to find the optimal threshold for a particular use-case, based on one or more evaluation metrics.\nFrom varying the threshold using predictions made using the best pipeline, show\n\nmetrics for default threshold of 0.5\ndescriptive statistics for metrics as threshold is varied\n\nShow a visualization of evaluation metrics relative to the discrimination threshold\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nTo be done.\n\n\n\n\n\nTrain Best Pipeline Using All Available Combined and Shuffled Data\n\n_ = pipe.fit(X, y)\n\nCPU times: user 1.12 s, sys: 64.3 ms, total: 1.19 s\nWall time: 1.19 s\n\n\n\n\nGet Validation and Evaluation Metrics, Feature Metadata and Observation Metadata\nGet metrics and metadata for model evaluation\n\n\n\n\n\n\n\n\n\nresampling_approach\nclf\nparam_clf__a\nparam_clf__b\nparam_preprocessor__cat__rarecats__fe__ignore_format\nparam_preprocessor__cat__rarecats__fe__n_categories\nparam_preprocessor__cat__rarecats__fe__replace_with\nparam_preprocessor__cat__rarecats__fe__tol\nparam_resampler__sampling_strategy\nparam_select__threshold\n...\ntrain_val_f1\ntest_f1\ntrain_val_fbeta05\ntest_fbeta05\ntrain_val_fbeta2\ntest_fbeta2\ntrain_val_pr_auc\ntest_pr_auc\ntrain_val_avg_precision\ntest_avg_precision\n\n\n\n\n0\nos\nBetaDistClassifier\n0.3\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n...\n0.502007\n0.498453\n0.50206\n0.498946\n0.501977\n0.498594\n0.043754\n0.023227\n0.043898\n0.023027\n\n\n\n\n1 rows × 31 columns\n\n\n\nCombine metrics and metadata for model validation and evaluation, and append experiment ID and run ID\n\n\n\n\n\n\n\n\n\nresampling_approach\nclf\nparam_clf__a\nparam_clf__b\nparam_preprocessor__cat__rarecats__fe__ignore_format\nparam_preprocessor__cat__rarecats__fe__n_categories\nparam_preprocessor__cat__rarecats__fe__replace_with\nparam_preprocessor__cat__rarecats__fe__tol\nparam_resampler__sampling_strategy\nparam_select__threshold\nparams\ntest_accuracy\ntest_balanced_accuracy\ntest_precision\ntest_recall\ntest_roc_auc\ntest_f1\ntest_fbeta05\ntest_fbeta2\ntest_pr_auc\ntest_avg_precision\nfit_time\nscore_time\nexperiment_run_type\ntrain_val_accuracy\ntrain_val_balanced_accuracy\ntrain_val_precision\ntrain_val_recall\ntrain_val_roc_auc\ntrain_val_f1\ntrain_val_fbeta05\ntrain_val_fbeta2\ntrain_val_pr_auc\ntrain_val_avg_precision\ntrain_start_date\ntest_end_date\nnum_observations\nnum_columns\ncolumn_names\n\n\n\n\n0\nos\nBetaDistClassifier\n0.2\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.2, \"clf__b\": 2.31, \"preprocessor_...\n0.937904\n0.501539\n0.501726\n0.501539\n0.501539\n0.501568\n0.501646\n0.501536\n0.034591\n0.034205\n0.836648\n0.118831\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n1\nos\nBetaDistClassifier\n0.2\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.2, \"clf__b\": 2.25, \"preprocessor_...\n0.935543\n0.496308\n0.496082\n0.496308\n0.496308\n0.496183\n0.496119\n0.496255\n0.032233\n0.033905\n0.81209\n0.115707\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n2\nos\nBetaDistClassifier\n0.2\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.2, \"clf__b\": 2.35, \"preprocessor_...\n0.939179\n0.502867\n0.503343\n0.502867\n0.502867\n0.502983\n0.503166\n0.502888\n0.035375\n0.034318\n0.802264\n0.122692\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n3\nos\nBetaDistClassifier\n0.2\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.2, \"clf__b\": 2.4, \"preprocessor__...\n0.940171\n0.501376\n0.501676\n0.501376\n0.501376\n0.501346\n0.501493\n0.501327\n0.035538\n0.034193\n0.80081\n0.117762\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n4\nos\nBetaDistClassifier\n0.2\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.2, \"clf__b\": 2.5, \"preprocessor__...\n0.94206\n0.496341\n0.495071\n0.496341\n0.496341\n0.495448\n0.495144\n0.495935\n0.034335\n0.033921\n0.789521\n0.116408\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n5\nos\nBetaDistClassifier\n0.3\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.3, \"clf__b\": 2.31, \"preprocessor_...\n0.919866\n0.498214\n0.498741\n0.498214\n0.498214\n0.497814\n0.498251\n0.497846\n0.034406\n0.033984\n0.829881\n0.126483\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n6\nos\nBetaDistClassifier\n0.3\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.3, \"clf__b\": 2.25, \"preprocessor_...\n0.91741\n0.498947\n0.499295\n0.498947\n0.498947\n0.498194\n0.498695\n0.498343\n0.035797\n0.034027\n0.815624\n0.116943\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n7\nos\nBetaDistClassifier\n0.3\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.3, \"clf__b\": 2.35, \"preprocessor_...\n0.921188\n0.497562\n0.498231\n0.497562\n0.497562\n0.497366\n0.497787\n0.497321\n0.033549\n0.033949\n0.822759\n0.117708\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n8\nos\nBetaDistClassifier\n0.3\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.3, \"clf__b\": 2.4, \"preprocessor__...\n0.923785\n0.49757\n0.498133\n0.49757\n0.49757\n0.497508\n0.497816\n0.497444\n0.03559\n0.03395\n0.800312\n0.12295\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n9\nos\nBetaDistClassifier\n0.3\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.3, \"clf__b\": 2.5, \"preprocessor__...\n0.928413\n0.505978\n0.504998\n0.505978\n0.505978\n0.505274\n0.505064\n0.505637\n0.032603\n0.034598\n0.79485\n0.117124\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n10\nos\nBetaDistClassifier\n0.4\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.4, \"clf__b\": 2.31, \"preprocessor_...\n0.906266\n0.507876\n0.50426\n0.507876\n0.507876\n0.502949\n0.503289\n0.504715\n0.035126\n0.034734\n0.799199\n0.126786\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n11\nos\nBetaDistClassifier\n0.4\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.4, \"clf__b\": 2.25, \"preprocessor_...\n0.901214\n0.499916\n0.499957\n0.499916\n0.499916\n0.496766\n0.498245\n0.497401\n0.035747\n0.034088\n0.857989\n0.126014\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n12\nos\nBetaDistClassifier\n0.4\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.4, \"clf__b\": 2.35, \"preprocessor_...\n0.908013\n0.503436\n0.501928\n0.503436\n0.503436\n0.500249\n0.500895\n0.501258\n0.034785\n0.034344\n0.846374\n0.121489\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n13\nos\nBetaDistClassifier\n0.4\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.4, \"clf__b\": 2.4, \"preprocessor__...\n0.910091\n0.499835\n0.499903\n0.499835\n0.499835\n0.498023\n0.498867\n0.498445\n0.031577\n0.034083\n0.829011\n0.126152\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n14\nos\nBetaDistClassifier\n0.4\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.4, \"clf__b\": 2.5, \"preprocessor__...\n0.914624\n0.502849\n0.501792\n0.502849\n0.502849\n0.500863\n0.501182\n0.501556\n0.033816\n0.0343\n0.822242\n0.116907\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n15\nos\nBetaDistClassifier\n0.35\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.35, \"clf__b\": 2.31, \"preprocessor...\n0.912877\n0.493928\n0.496221\n0.493928\n0.493928\n0.493954\n0.495131\n0.493554\n0.034369\n0.033777\n0.804587\n0.12304\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n16\nos\nBetaDistClassifier\n0.35\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.35, \"clf__b\": 2.25, \"preprocessor...\n0.911224\n0.50443\n0.502615\n0.50443\n0.50443\n0.501491\n0.501851\n0.502526\n0.034113\n0.034427\n0.802549\n0.117778\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n17\nos\nBetaDistClassifier\n0.35\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.35, \"clf__b\": 2.35, \"preprocessor...\n0.915758\n0.503436\n0.502203\n0.503436\n0.503436\n0.50147\n0.501686\n0.502194\n0.034001\n0.034348\n0.839375\n0.129014\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n18\nos\nBetaDistClassifier\n0.35\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.35, \"clf__b\": 2.4, \"preprocessor_...\n0.91708\n0.498108\n0.498739\n0.498108\n0.498108\n0.497504\n0.498086\n0.497565\n0.031889\n0.033978\n0.835043\n0.127018\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n19\nos\nBetaDistClassifier\n0.35\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.35, \"clf__b\": 2.5, \"preprocessor_...\n0.920905\n0.497415\n0.498135\n0.497415\n0.497415\n0.497227\n0.497671\n0.497171\n0.034196\n0.033941\n0.795141\n0.119308\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n20\nos\nBetaDistClassifier\n0.15\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.15, \"clf__b\": 2.31, \"preprocessor...\n0.943618\n0.493139\n0.489924\n0.493139\n0.493139\n0.491285\n0.49039\n0.492354\n0.03575\n0.033903\n0.82357\n0.117448\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n21\nos\nBetaDistClassifier\n0.15\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.15, \"clf__b\": 2.25, \"preprocessor...\n0.943476\n0.500415\n0.50058\n0.500415\n0.500415\n0.500044\n0.500224\n0.500184\n0.034897\n0.034122\n0.782312\n0.11731\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n22\nos\nBetaDistClassifier\n0.15\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.15, \"clf__b\": 2.35, \"preprocessor...\n0.94546\n0.498101\n0.49705\n0.498101\n0.498101\n0.496981\n0.496824\n0.497554\n0.034968\n0.03399\n0.794685\n0.119732\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n23\nos\nBetaDistClassifier\n0.15\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.15, \"clf__b\": 2.4, \"preprocessor_...\n0.946829\n0.500146\n0.500239\n0.500146\n0.500146\n0.49931\n0.499559\n0.499671\n0.033555\n0.034103\n0.787629\n0.117135\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n24\nos\nBetaDistClassifier\n0.15\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.15, \"clf__b\": 2.5, \"preprocessor_...\n0.948387\n0.49828\n0.49689\n0.49828\n0.49828\n0.496613\n0.496417\n0.497471\n0.032713\n0.034001\n0.785883\n0.116174\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n25\nos\nBetaDistClassifier\n0.45\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.45, \"clf__b\": 2.31, \"preprocessor...\n0.893469\n0.492567\n0.496543\n0.492567\n0.492567\n0.490987\n0.493861\n0.490442\n0.034786\n0.033696\n0.788639\n0.120124\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n26\nos\nBetaDistClassifier\n0.45\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.45, \"clf__b\": 2.25, \"preprocessor...\n0.890494\n0.497707\n0.498981\n0.497707\n0.497707\n0.493601\n0.496246\n0.494012\n0.034961\n0.033951\n0.794289\n0.119293\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n27\nos\nBetaDistClassifier\n0.45\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.45, \"clf__b\": 2.35, \"preprocessor...\n0.898239\n0.506393\n0.503091\n0.506393\n0.506393\n0.500318\n0.501405\n0.502142\n0.03569\n0.034585\n0.822991\n0.118166\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n28\nos\nBetaDistClassifier\n0.45\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.45, \"clf__b\": 2.4, \"preprocessor_...\n0.900647\n0.498954\n0.49947\n0.498954\n0.498954\n0.496052\n0.497671\n0.496538\n0.036071\n0.034027\n0.807132\n0.117922\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n29\nos\nBetaDistClassifier\n0.45\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.45, \"clf__b\": 2.5, \"preprocessor_...\n0.906313\n0.493203\n0.496208\n0.493203\n0.493203\n0.492893\n0.494611\n0.492408\n0.034326\n0.033739\n0.790924\n0.117878\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n\n\n30\nos\nBetaDistClassifier\n0.3\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.3, \"clf__b\": 2.5, \"preprocessor__...\n0.940538\n0.499218\n0.022135\n0.499218\n0.499218\n0.498453\n0.498946\n0.498594\n0.023227\n0.023027\n&lt;NA&gt;\n&lt;NA&gt;\nparent\n0.919483\n0.501968\n0.047763\n0.501968\n0.501968\n0.502007\n0.50206\n0.501977\n0.043754\n0.043898\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v..."
  },
  {
    "objectID": "notebooks/02-train/notebooks/04_train.html#ml-experiment-tracking-1",
    "href": "notebooks/02-train/notebooks/04_train.html#ml-experiment-tracking-1",
    "title": "ML Development",
    "section": "ML Experiment Tracking",
    "text": "ML Experiment Tracking\n\nSet up MLFlow Experiment\n\nclient = MlflowClient(tracking_uri=mlflow.get_tracking_uri())\nif not client.get_experiment_by_name(mlflow_expt_name):\n    experiment_id = client.create_experiment(\n        mlflow_expt_name,\n        artifact_location=mlflow_artifact_fpath,\n        tags={\"version\": \"v1\", \"priority\": \"P1\"},\n    )\n    client.set_experiment_tag(experiment_id, \"demo\", \"mydemo\")\n    print(f\"Created new experiment with ID = {experiment_id}\")\nelse:\n    experiment_id = client.get_experiment_by_name(mlflow_expt_name).experiment_id\n    print(f\"Retrieved existing experiment with ID = {experiment_id}\")\n\n\n\nTrack Experiment Run Outputs (Metrics and Metadata)\n\nwith mlflow.start_run(experiment_id=experiment_id) as run:\n    run_id = run.info.run_id\n    expt_run_fpath = os.path.join(\n        raw_data_dir, f'ml__run_{run_id}__expt_{experiment_id}.parquet.gzip'\n    )\n\n    # export validation metrics to file and log as MLFlow artifact\n    (\n        df_expt_run_metrics.assign(experiment_id=experiment_id)\n        .assign(run_id=run_id)\n        .to_parquet(expt_run_fpath, index=False, compression='gzip', engine='pyarrow')\n    )\n    mlflow.log_artifact(expt_run_fpath)\n    print(\n        \"Logged metrics and metadata DataFrame as artifact in file \"\n        f\"{os.path.basename(expt_run_fpath)}\"\n    )\n\n    # export combined processed data to file and log as MLFlow artifact\n    proc_data_fpath = os.path.join(\n        processed_data_dir, f'processed_data__run_{run_id}__expt_{experiment_id}.parquet.gzip'\n    )\n    df_train_val_eval_test.to_parquet(\n        proc_data_fpath, index=False, compression='gzip', engine='pyarrow'\n    )\n    mlflow.log_artifact(proc_data_fpath)\n    print(\n        f\"Logged processed data used during training and evaluation in file \"\n        f\"{os.path.basename(proc_data_fpath)}\"\n    )\n\n    # export threshold tuning outputs to file and log as MLFlow artifact\n    threshold_data_fpath = os.path.join(\n        processed_data_dir,\n        f'threshold_tuning_data__run_{run_id}__expt_{experiment_id}.parquet.gzip',\n    )\n    df_thresholds.to_parquet(\n        threshold_data_fpath, index=False, compression='gzip', engine='pyarrow'\n    )\n    mlflow.log_artifact(threshold_data_fpath)\n    print(\n        f\"Logged data from threshold tuning used test data in file \"\n        f\"{os.path.basename(threshold_data_fpath)}\"\n    )\n\n    # log evaluation metrics\n    mlflow.log_metrics(\n        df_eval_metrics_metadata.select_dtypes(pd.Float64Dtype()).transpose().to_dict(\n            orient=\"dict\"\n        )[0]\n    )\n    print(\"Logged metrics evaluated against combined train+validation and test data splits\")\n\n    curr_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    model_name = (\n        f\"{model_type}_{train_start_date}_{test_end_date}_{X.shape[0]}_feats\"\n        f\"__{curr_datetime}\"\n    )\n    _ = mlflow.sklearn.log_model(\n        sk_model=pipe,\n        await_registration_for=None,\n        artifact_path=\"model\",\n        serialization_format='cloudpickle',\n        registered_model_name=model_name,\n    )\n    print(\n        f\"Logged best {model_type} model (with score of {best_model_eval_score:.3f}), \"\n        \"after training on all data\"\n    )\n\n\n\nCPU times: user 2.06 ms, sys: 3.38 ms, total: 5.44 ms\nWall time: 11.3 ms\n\n\n\n\nNon-MLFlow Monitoring\nGet untrained version of ML Pipeline\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                  ('dummy',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures(threshold=0.7)),\n                ('resampler',\n                 RandomOverSampler(random_state=88, sampling_strategy=0.1)),\n                ['clf', BetaDistClassifier(a=0.3, b=2.5, random_state=88)]])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                  ('dummy',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures(threshold=0.7)),\n                ('resampler',\n                 RandomOverSampler(random_state=88, sampling_strategy=0.1)),\n                ['clf', BetaDistClassifier(a=0.3, b=2.5, random_state=88)]])preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('aboveavg',\n                                                  AboveAveragePagePromoEngager(cols=['hits',\n                                                                                     'promos_displayed',\n                                                                                     'promos_clicked',\n                                                                                     'product_views',\n                                                                                     'product_clicks',\n                                                                                     'pageviews',\n                                                                                     'time_on_site'])),\n                                                 ('scaler', MinMaxScaler())]),\n                                 ['hits', 'promos_displayed', 'promos_clicked',\n                                  'product_views', 'product_clicks',\n                                  'pageviews', 'time_on_site']),\n                                ('cat',\n                                 P...\n                                                                                                    n_categories=1,\n                                                                                                    replace_with='other',\n                                                                                                    tol=0.1,\n                                                                                                    variables=['source',\n                                                                                                               'browser']),\n                                                                                   ['bounces',\n                                                                                    'last_action',\n                                                                                    'source',\n                                                                                    'medium',\n                                                                                    'channelGrouping',\n                                                                                    'browser',\n                                                                                    'os',\n                                                                                    'deviceCategory'])])),\n                                                 ('dummy',\n                                                  OneHotEncoder(drop='first',\n                                                                dtype=&lt;class 'int'&gt;,\n                                                                handle_unknown='ignore'))]),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])num['hits', 'promos_displayed', 'promos_clicked', 'product_views', 'product_clicks', 'pageviews', 'time_on_site']AboveAveragePagePromoEngagerAboveAveragePagePromoEngager(cols=['hits', 'promos_displayed', 'promos_clicked',\n                                   'product_views', 'product_clicks',\n                                   'pageviews', 'time_on_site'])MinMaxScalerMinMaxScaler()cat['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']rarecats: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('fe',\n                                 RareLabelEncoder(ignore_format=True,\n                                                  n_categories=1,\n                                                  replace_with='other', tol=0.1,\n                                                  variables=['source',\n                                                             'browser']),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])fe['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']RareLabelEncoderRareLabelEncoder(ignore_format=True, n_categories=1, replace_with='other',\n                 tol=0.1, variables=['source', 'browser'])remainderpassthroughpassthroughOneHotEncoderOneHotEncoder(drop='first', dtype=&lt;class 'int'&gt;, handle_unknown='ignore')DropCorrelatedFeaturesDropCorrelatedFeatures(threshold=0.7)RandomOverSamplerRandomOverSampler(random_state=88, sampling_strategy=0.1)BetaDistClassifierBetaDistClassifier(a=0.3, b=2.5, random_state=88)\n\n\nExport untrained model object to disk\n\nmodel_filepath = os.path.join(model_rub_dir_path, 'model.joblib')\n_ = joblib.dump(pipe_new, model_filepath)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/04_train.html#next-step",
    "href": "notebooks/02-train/notebooks/04_train.html#next-step",
    "title": "ML Development",
    "section": "Next Step",
    "text": "Next Step\nThe trained model associated with the best experiment run (having the best evaluation metric) will be retrieved. This model will be registered in the MLFlow model registry so that it can be accessed in production to make inference predictions."
  },
  {
    "objectID": "notebooks/02-train/notebooks/05_get_best_model.html",
    "href": "notebooks/02-train/notebooks/05_get_best_model.html",
    "title": "Register Best Model in MLFlow Model Registry",
    "section": "",
    "text": "This step retrieves the best ML model across all the MLFlow experiment runs that were tracked during ML development. This best model is then registered in the MLFlow Model Registry."
  },
  {
    "objectID": "notebooks/02-train/notebooks/05_get_best_model.html#about",
    "href": "notebooks/02-train/notebooks/05_get_best_model.html#about",
    "title": "Register Best Model in MLFlow Model Registry",
    "section": "",
    "text": "This step retrieves the best ML model across all the MLFlow experiment runs that were tracked during ML development. This best model is then registered in the MLFlow Model Registry."
  },
  {
    "objectID": "notebooks/02-train/notebooks/05_get_best_model.html#user-inputs",
    "href": "notebooks/02-train/notebooks/05_get_best_model.html#user-inputs",
    "title": "Register Best Model in MLFlow Model Registry",
    "section": "User Inputs",
    "text": "User Inputs\nDefine the primary ML scoring metric\n\nprimary_metric = \"fbeta2\""
  },
  {
    "objectID": "notebooks/02-train/notebooks/05_get_best_model.html#manage-ml-experiments",
    "href": "notebooks/02-train/notebooks/05_get_best_model.html#manage-ml-experiments",
    "title": "Register Best Model in MLFlow Model Registry",
    "section": "Manage ML Experiments",
    "text": "Manage ML Experiments\n\nInspect Experiment Run Outputs\nGet all runs of all experiments\n\ndf_expt_runs = modh.get_all_experiment_runs()\n\n\n\nGet Outputs of Best Experiment Run\n\ndf_best_expt_run = modh.get_best_experiment_run(\n    df_expt_runs, \"experiment_run_type == 'parent'\", f\"test_{primary_metric}\"\n)\n\n\n\nGet Parameters Associated With Best Experiment Run\nGet the metadata and metrics for all available data and experiment run ID for best performing run\n\nfeatures\n\nlist of column names\n\nmetrics\n\nprimary metric score on the test split, during ML evaluation\n\nrun ID\n\nMLFlow experiment run ID\n\n\n\ncols_best_expt_run = json.loads(df_best_expt_run[\"column_names\"])\nbest_model_eval_score = df_best_expt_run[f\"test_{primary_metric}\"]\nbest_run_id = df_best_expt_run[\"run_id\"]\n\n\n\nGet Name of Logged Model Associated with Best Experiment Run\nGet name of model associated with best run\n\ndf_best_run_model = modh.get_single_registered_model(f\"run_id == '{best_run_id}'\")\nbest_run_model_name = df_best_run_model.squeeze()[\"name\"]\n\n\n\n\n\n\n\n\n\n\nname\nrun_id\ndescription\nsource\nversion\nstatus\n\n\n\n\n0\nBetaDistClassifier_20160901_20170228_133892_feats__20230605_225228\n742cf152d5094be0b686edf910c7d398\nBest BetaDistClassifier model with fbeta2 score of 0.4985941872\n/home/jovyan/notebooks/mlruns/742cf152d5094be0b686edf910c7d398/artifacts/model\n1\nREADY\n\n\n\n\n\n\n\nBetaDistClassifier_20160901_20170228_133892_feats__20230605_225228\n\n\n\n\nLoad Best Deployment Candidate Model from Model Registry\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                  ('dummy',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures(threshold=0.7)),\n                ('resampler',\n                 RandomOverSampler(random_state=88, sampling_strategy=0.1)),\n                ['clf', BetaDistClassifier(a=0.3, b=2.5, random_state=88)]])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                  ('dummy',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures(threshold=0.7)),\n                ('resampler',\n                 RandomOverSampler(random_state=88, sampling_strategy=0.1)),\n                ['clf', BetaDistClassifier(a=0.3, b=2.5, random_state=88)]])preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('aboveavg',\n                                                  AboveAveragePagePromoEngager(cols=['hits',\n                                                                                     'promos_displayed',\n                                                                                     'promos_clicked',\n                                                                                     'product_views',\n                                                                                     'product_clicks',\n                                                                                     'pageviews',\n                                                                                     'time_on_site'])),\n                                                 ('scaler', MinMaxScaler())]),\n                                 ['hits', 'promos_displayed', 'promos_clicked',\n                                  'product_views', 'product_clicks',\n                                  'pageviews', 'time_on_site']),\n                                ('cat',\n                                 P...\n                                                                                                    n_categories=1,\n                                                                                                    replace_with='other',\n                                                                                                    tol=0.1,\n                                                                                                    variables=['source',\n                                                                                                               'browser']),\n                                                                                   ['bounces',\n                                                                                    'last_action',\n                                                                                    'source',\n                                                                                    'medium',\n                                                                                    'channelGrouping',\n                                                                                    'browser',\n                                                                                    'os',\n                                                                                    'deviceCategory'])])),\n                                                 ('dummy',\n                                                  OneHotEncoder(drop='first',\n                                                                dtype=&lt;class 'int'&gt;,\n                                                                handle_unknown='ignore'))]),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])num['hits', 'promos_displayed', 'promos_clicked', 'product_views', 'product_clicks', 'pageviews', 'time_on_site']AboveAveragePagePromoEngagerAboveAveragePagePromoEngager(cols=['hits', 'promos_displayed', 'promos_clicked',\n                                   'product_views', 'product_clicks',\n                                   'pageviews', 'time_on_site'])MinMaxScalerMinMaxScaler()cat['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']rarecats: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('fe',\n                                 RareLabelEncoder(ignore_format=True,\n                                                  n_categories=1,\n                                                  replace_with='other', tol=0.1,\n                                                  variables=['source',\n                                                             'browser']),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])fe['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']RareLabelEncoderRareLabelEncoder(ignore_format=True, n_categories=1, replace_with='other',\n                 tol=0.1, variables=['source', 'browser'])remainderpassthroughpassthroughOneHotEncoderOneHotEncoder(drop='first', dtype=&lt;class 'int'&gt;, handle_unknown='ignore')DropCorrelatedFeaturesDropCorrelatedFeatures(threshold=0.7)RandomOverSamplerRandomOverSampler(random_state=88, sampling_strategy=0.1)BetaDistClassifierBetaDistClassifier(a=0.3, b=2.5, random_state=88)\n\n\n\n\nAdd MLFlow Model Associated with Best Experiment Run to Model Registry\nCreate a new registered model, with a version\n\nclient = MlflowClient(tracking_uri=mlflow.get_tracking_uri())\nresult = client.create_model_version(\n    name=best_run_model_name,\n    await_creation_for=None,\n    tags={'deployment-candidate': \"yes\"},\n    description=(\n        f\"Best Model based on {primary_metric} score of \"\n        f\"{best_model_eval_score:.10f}\"\n    ),\n    source=f\"mlruns/{best_run_id}/artifacts/model\",\n    run_id=best_run_id,\n)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/05_get_best_model.html#next-step",
    "href": "notebooks/02-train/notebooks/05_get_best_model.html#next-step",
    "title": "Register Best Model in MLFlow Model Registry",
    "section": "Next Step",
    "text": "Next Step\nThe registered model will be used during downstream steps such as inference to make predictions for first-time visitors to the store during the production period (following the end of the test data split)."
  },
  {
    "objectID": "notebooks/02-train/notebooks/06_design_experiment.html",
    "href": "notebooks/02-train/notebooks/06_design_experiment.html",
    "title": "Post-Processing",
    "section": "",
    "text": "As discussed for the overall use-case, a marketing strategy is to be developed based on first-time visitors’ propenisty to make a purchase during a future (return) visit to the merchandise store website. The aim of the strategy is to convert these first time visitors into customers (if they did not make a purchase during their first visit), or repeat customers (if they did make a purchase during their first visit). This covers steps 2. and 3. from a typical A/B Testing workflow.\nThis step can be run during ML model development since it requires\n\ncombined training+validation\ntest\n\ndata splits to be created.\n\n\n\nTwo marketing strategies to consider for first-time visitors to the store during the inference period are\n\nwhen conducting the marketing campaign, use all first-time visitors, grouped by their predicted propensity to make a purchase during a future visit, where the strategy is modified based on the group\nwhen conducting the marketing campaign, use the group of first-time visitors with the highest predicted propensity to make a purchase during a future visit, where a single strategy can be used for all visitors in the group\n\nEach group will be referred to as a marketing audience group. For the multi-group strategy (strategy 1), the groups of visitors are created based on predicted propensities. If three such groups are preferred, then the group with the visitors who are predicted to have the highest propensity to make a purchase on a return visit would be named as the High propensity group. Similarly, the other two groups would be named the Medium and Low propensity groups. The size of each group would be as equal as possible. For 15,000 first-time visitors, each group would consist of 5,000 visitors. Similarly, if 10 such groups are preferred then each group would consist of 1,500 visitors. The groups are created by binning the predicted propensities using their quantiles.\nFor the single-group strategy (strategy 2), a single group is required. Again, the quantiles are used to create the groups (or bins or segments or buckets) of visitors based on their predicted propensity. However, only the visitors in the top group will be chosen for the marketing campaign. For the case where 10 groups (quantiles) are preferred, then only the top group is chosen.\n\n\n\nRegardless of the number of groups used in this strategy, a rank is assigned to each group where the lowest rank corresponds to the group with the highest predicted propensities to make a purchase on a future (return) visit. For the case where 10 groups (quantiles) are preferred, then the top group would be the one consisting of visitors with a propensity higher than 90% of all first-time visitors to the store during the inference data period. Similarly, the bottom group would be the one consisting of visitors with a propensity higher than 10% of all such visitors. If three groups are preferred, then the top group captures visitors with a predicted propensity higher than 66.667% of all first-time visitors, while the visitors in the bottom group are predicted to have a propensity to make a purchase on a return visit that is higher than 33.333% of all visitors.\n\n\n\nVisitors placed in each group will then be randomly placed into test and control cohorts to run an A/B test for quantifying the impact of the marketing campaign. So, the sample size required for the test and control cohort must also be determined for both these strategies. The KPI to be maximized is the future conversion rate since we want visitors to become customers during a future (return) visit to the store. The required sample sizes will be estimated for a combinations of conversion rate, uplift, power and confidence level (1, 2)."
  },
  {
    "objectID": "notebooks/02-train/notebooks/06_design_experiment.html#about",
    "href": "notebooks/02-train/notebooks/06_design_experiment.html#about",
    "title": "Post-Processing",
    "section": "",
    "text": "As discussed for the overall use-case, a marketing strategy is to be developed based on first-time visitors’ propenisty to make a purchase during a future (return) visit to the merchandise store website. The aim of the strategy is to convert these first time visitors into customers (if they did not make a purchase during their first visit), or repeat customers (if they did make a purchase during their first visit). This covers steps 2. and 3. from a typical A/B Testing workflow.\nThis step can be run during ML model development since it requires\n\ncombined training+validation\ntest\n\ndata splits to be created.\n\n\n\nTwo marketing strategies to consider for first-time visitors to the store during the inference period are\n\nwhen conducting the marketing campaign, use all first-time visitors, grouped by their predicted propensity to make a purchase during a future visit, where the strategy is modified based on the group\nwhen conducting the marketing campaign, use the group of first-time visitors with the highest predicted propensity to make a purchase during a future visit, where a single strategy can be used for all visitors in the group\n\nEach group will be referred to as a marketing audience group. For the multi-group strategy (strategy 1), the groups of visitors are created based on predicted propensities. If three such groups are preferred, then the group with the visitors who are predicted to have the highest propensity to make a purchase on a return visit would be named as the High propensity group. Similarly, the other two groups would be named the Medium and Low propensity groups. The size of each group would be as equal as possible. For 15,000 first-time visitors, each group would consist of 5,000 visitors. Similarly, if 10 such groups are preferred then each group would consist of 1,500 visitors. The groups are created by binning the predicted propensities using their quantiles.\nFor the single-group strategy (strategy 2), a single group is required. Again, the quantiles are used to create the groups (or bins or segments or buckets) of visitors based on their predicted propensity. However, only the visitors in the top group will be chosen for the marketing campaign. For the case where 10 groups (quantiles) are preferred, then only the top group is chosen.\n\n\n\nRegardless of the number of groups used in this strategy, a rank is assigned to each group where the lowest rank corresponds to the group with the highest predicted propensities to make a purchase on a future (return) visit. For the case where 10 groups (quantiles) are preferred, then the top group would be the one consisting of visitors with a propensity higher than 90% of all first-time visitors to the store during the inference data period. Similarly, the bottom group would be the one consisting of visitors with a propensity higher than 10% of all such visitors. If three groups are preferred, then the top group captures visitors with a predicted propensity higher than 66.667% of all first-time visitors, while the visitors in the bottom group are predicted to have a propensity to make a purchase on a return visit that is higher than 33.333% of all visitors.\n\n\n\nVisitors placed in each group will then be randomly placed into test and control cohorts to run an A/B test for quantifying the impact of the marketing campaign. So, the sample size required for the test and control cohort must also be determined for both these strategies. The KPI to be maximized is the future conversion rate since we want visitors to become customers during a future (return) visit to the store. The required sample sizes will be estimated for a combinations of conversion rate, uplift, power and confidence level (1, 2)."
  },
  {
    "objectID": "notebooks/02-train/notebooks/06_design_experiment.html#implementation",
    "href": "notebooks/02-train/notebooks/06_design_experiment.html#implementation",
    "title": "Post-Processing",
    "section": "Implementation",
    "text": "Implementation\nThis step will used the best ML model to estimate the size of the group, for both such strategies. The model will be used with the test data split (last month of data that was used during ML model development) to estimate the required size of marketing audience (test and control) cohorts for both strategies. In the next step, the same ML model will be used to predict the propensity for first-time visitors to the store during the inference period, assign these visitors to one or more audience groups based on the type of strategy (single group or multiple groups) and finally select random test and control cohorts from each group. The cohort (sample) sizes estimated in this step will be used to create the test and control cohorts in the next step."
  },
  {
    "objectID": "notebooks/02-train/notebooks/06_design_experiment.html#user-inputs",
    "href": "notebooks/02-train/notebooks/06_design_experiment.html#user-inputs",
    "title": "Post-Processing",
    "section": "User Inputs",
    "text": "User Inputs\nDefine the following\n\nname of column containing label (outcome)\nprimary ML scoring metric\naudience_groups_strategy_n\n\ndesired audience groups into which the first-time visitors propensities will be placed\n\nnum_propens_groups specifies the number of desired audience propensity groups\npropens_group_labels specifies names of the desired audience propensity groups (low, medium and high, etc.)\n\n\ngrid of inputs for which sample sizes are required (uplift, power, confidence level)\n\n\n# 1. label column\nlabel = \"made_purchase_on_future_visit\"\n\n# 2. scoring metric\nprimary_metric = \"fbeta2\"\n\n# 3. mapping dictionaries\naudience_groups_strategy_1 = {\n    \"num_propens_groups\": 3,\n    \"propens_group_labels\": [\"High\", \"Medium\", \"Low\"],\n}\naudience_groups_strategy_2 = {\n    \"num_propens_groups\": 3,\n    \"propens_group_labels\": [\"High\", \"High-Medium\", \"High-Medium-Low\"],\n}\n\n# 4. grid of inputs for estimating sample sizes\nuplift_ranges = (10, 12, 15)\npower_ranges = (55, 57, 60, 63, 65, 66, 67, 68, 69, 70, 80, 90)\nci_level_ranges = (55, 57, 60, 63, 65, 66, 67, 68, 69, 70, 75, 85, 90, 95)\n\nCreate a mapping between audience group number (0, 1, 2) and name (high, medium, low), where\n\n0 is mapped to high\n1 is mapped to medium\n2 is mapped to low\n\nsince it is standard to assign a label the top percentile (highest propensity) with the smallest number (0)\n\nmapper_dict_audience_strategy_1 = dict(\n    zip(\n        range(audience_groups_strategy_1[\"num_propens_groups\"]),\n        audience_groups_strategy_1[\"propens_group_labels\"],\n    )\n)\nmapper_dict_audience_strategy_2 = dict(\n    zip(\n        range(audience_groups_strategy_1[\"num_propens_groups\"]),\n        audience_groups_strategy_2[\"propens_group_labels\"],\n    )\n)\nprint(mapper_dict_audience_strategy_1)\nprint(mapper_dict_audience_strategy_2)\n\n{0: 'High', 1: 'Medium', 2: 'Low'}\n{0: 'High', 1: 'High-Medium', 2: 'High-Medium-Low'}\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe group integer-to-name mapping dictionary contains keys that start at the lowest group number (0) for the High propensity audience group. For this audience strategy, a single group is used, so 0 is the only key in this dictionary."
  },
  {
    "objectID": "notebooks/02-train/notebooks/06_design_experiment.html#get-best-mlflow-model-from-model-registry",
    "href": "notebooks/02-train/notebooks/06_design_experiment.html#get-best-mlflow-model-from-model-registry",
    "title": "Post-Processing",
    "section": "Get Best MLFlow Model from Model Registry",
    "text": "Get Best MLFlow Model from Model Registry\n\nFetch Latest Version of Best Deployment Candidate Model\nGet all MLFlow deployment candidate models from MLFlow model registry\n\ndf_deployment_candidate_mlflow_models = modh.get_all_deployment_candidate_models()\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThese are registered models that have been assigned a tag (tags={deployment-candidate: 'yes'}), as opposed to models that are not deployment candidates and do not have any tag (tags={}).\n\n\n\nGet name of best deployment candidate model\n\nbest_run_model_name = modh.get_best_deployment_candidate_model(\n    df_deployment_candidate_mlflow_models\n)\n\n\n\nGet Data Used to Develop Best Deployment Candidate Model\nGet all available data used during model development of the best deployment candidate model\n\ndf_all = modh.get_data_for_run_id(df_deployment_candidate_mlflow_models, 'processed_data')\n\nSeparate features and label\n\nX, y = [df_all.drop(columns=[label, 'split_type']), df_all[label]]\n\nGet test split from the data used during development of the best deployment candidate model\n\ndf_prediction_best_run = (\n    df_all\n    .query(\"split_type == 'test'\")\n    .rename(columns={label: 'label'})\n    .drop(columns=['split_type'])\n)\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe fullvisitorid column is the ID of each visitor who\n\nmade a purchase on a return visit to the store\nmade their first visit to the store during the dates covered by the test data split\n\nThe score column is the predicted probability (using .pred_proba()), which is the propensity of a visitor to make a purchase on a return visit.\nThe predicted_score_label column is the predicted label using ML. A discrmination threshold of 0.5 is used to convert the predicted score (probability) into a label. The true ML label will not be known until a later date. As of the current date, only the predicted ML label can be known. See the project scope for details.\n\n\n\n\n\nLoad Best Deployment Candidate Model from Model Registry\n\nbest_model_uri = f\"models:/{best_run_model_name}/latest\"\nmodel = mlflow.sklearn.load_model(model_uri=best_model_uri)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/06_design_experiment.html#explore-best-model-performance",
    "href": "notebooks/02-train/notebooks/06_design_experiment.html#explore-best-model-performance",
    "title": "Post-Processing",
    "section": "Explore Best Model Performance",
    "text": "Explore Best Model Performance\nMake predictions on same data that best model was trained on\n\ny_pred, y_pred_proba = modh.make_inference(model, X, y.name)\n\nGet evaluation metrics on same data that best model was trained on\n\ndf_metrics_all = mh.calculate_metrics(y, y_pred, y_pred_proba, None, None, None, [\"all\"])\n\n\nOptimal Discrimination Threshold Tuning\nShow a visualization of evaluation metrics relative to the discrimination threshold\n\nvh.plot_multi_line_threshold_chart(\n    df_thresholds_best_run.set_index(\"t\"),\n    ptitle_str=\"Threshold Plot\",\n    xlabel=\"Threshold (t)\",\n    ylabel=\"Score\",\n    title_fontsize=12,\n    axis_label_fontsize=12,\n    figsize=(8, 4),\n)\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nTo be completed."
  },
  {
    "objectID": "notebooks/02-train/notebooks/06_design_experiment.html#get-size-of-marketing-audience-for-campaign",
    "href": "notebooks/02-train/notebooks/06_design_experiment.html#get-size-of-marketing-audience-for-campaign",
    "title": "Post-Processing",
    "section": "Get Size of Marketing Audience For Campaign",
    "text": "Get Size of Marketing Audience For Campaign\n\nGet Marketing Audience Using All Propensity Groups (Strategy 1)\nPerform the following to get the sample size estimates for each audience propensity group, using this approach\n\nSort predictions in ascending order of the predicted probability (propensity) (score)\n\nwhen predictions are then assigned to groups (bins), this order of sorting means the lowest group number will be assigned to samples (predictions) with the lowest probability, so this ordering is non-standard\nthe group integer-to-name mapping dictionary contains keys that start at the lowest group number (0) for the High propensity audience group\n\nSeparate the predictions into group, using the predicted probability, based on the required number of groups\nGet conversion rates (KPI) per group\nget sample sizes per group, based on\n\nKPI per group\ndesired magnitude of the input (uplift, power, confidence level)\n\nreverse order of groups and sort by group number\nassign meaningful names to groups\nset datatypes for columns\n(optional) move column with meaningful names to second from front (for display purposes only)\n\n\ndf_sample_sizes_strategy_1 = (\n    df_prediction_best_run.copy()\n    .pipe(ash.sort_scores, True)\n    .pipe(ash.get_audience_groups_by_propensity, audience_groups_strategy_1[\"num_propens_groups\"])\n    .pipe(ash.get_kpi_per_audience_group)\n    .pipe(\n        ash.calculate_multi_group_sample_sizes,\n        uplift_ranges,\n        power_ranges,\n        ci_level_ranges,\n    )\n    .pipe(ash.invert_group_numbers, audience_groups_strategy_1[\"num_propens_groups\"])\n    .pipe(ash.map_audience_group_number_to_name, mapper_dict_audience_strategy_1)\n    .pipe(\n        ash.set_datatypes,\n        {\n            \"group_number\": pd.Int8Dtype(),\n            \"maudience\": pd.StringDtype(),\n            \"group_size\": pd.Int16Dtype(),\n            \"group_min_propensity\": pd.Float32Dtype(),\n            \"group_conv_rate\": pd.Float32Dtype(),\n            \"uplift\": pd.Int8Dtype(),\n            \"power\": pd.Int8Dtype(),\n            \"ci_level\": pd.Int8Dtype(),\n            \"required_sample_size\": pd.Int32Dtype(),\n        },\n    ).pipe(ash.move_cols_to_front, [\"group_number\", \"maudience\"])\n)\ndf_sample_sizes_strategy_1\n\nSet all specified datatypes.\n\n\n\n\n\n\n\n\n\ngroup_number\nmaudience\ngroup_size\ngroup_min_propensity\ngroup_conv_rate\nuplift\npower\nci_level\nrequired_sample_size\n\n\n\n\n0\n0\nHigh\n6721\n0.092311\n2.157417\n15\n90\n95\n21180\n\n\n1\n0\nHigh\n6721\n0.092311\n2.157417\n10\n90\n66\n22586\n\n\n2\n0\nHigh\n6721\n0.092311\n2.157417\n10\n90\n67\n23003\n\n\n3\n0\nHigh\n6721\n0.092311\n2.157417\n10\n90\n68\n23430\n\n\n4\n0\nHigh\n6721\n0.092311\n2.157417\n10\n90\n69\n23869\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1507\n2\nLow\n6722\n0.0\n2.261232\n15\n55\n67\n2098\n\n\n1508\n2\nLow\n6722\n0.0\n2.261232\n15\n55\n68\n2205\n\n\n1509\n2\nLow\n6722\n0.0\n2.261232\n15\n55\n69\n2313\n\n\n1510\n2\nLow\n6722\n0.0\n2.261232\n12\n90\n70\n16097\n\n\n1511\n2\nLow\n6722\n0.0\n2.261232\n10\n55\n55\n2103\n\n\n\n\n1512 rows × 9 columns\n\n\n\n\n\nGet Marketing Audience Using Top Propensity Group (Strategy 2)\nPerform the following to get the sample size estimates for the top audience propensity group, using this approach\n\nSort predictions in descending order of the predicted probability (propensity) (score)\nget group (bin) size that separates the predictions into groups (bins) of equal size, using the predicted probability, based on the required number of groups\nseparate the predictions into groups, using the predicted probability, based on the\n\nrequired number of groups\ncalculated group size from above step\n\nget conversion rates (KPI) per group\nget sample sizes per group, based on\n\nKPI per group\ndesired magnitude of the input (uplift, power, confidence level)\n\nsubtract one from group number\nassign meaningful names to groups\nset datatypes for columns\n(optional) move column with meaningful names to second from front (for display purposes only)\n\n\ndf_sample_sizes_strategy_2 = (\n    df_prediction_best_run.copy()\n    .pipe(ash.sort_scores, False)\n    .pipe(\n        ash.calculate_single_group_sample_sizes,\n        audience_groups_strategy_2[\"num_propens_groups\"],\n        ash.get_group_size(df_prediction_best_run, audience_groups_strategy_2[\"num_propens_groups\"]),\n        uplift_ranges,\n        power_ranges,\n        ci_level_ranges,\n    )\n    .pipe(ash.subtract_one_from_group_numbers, \"group_number\")\n    .pipe(\n        ash.map_audience_group_number_to_name,\n        mapper_dict_audience_strategy_2,\n        \"group_number\",\n    )\n    .pipe(\n        ash.set_datatypes,\n        {\n            \"group_number\": pd.Int8Dtype(),\n            \"maudience\": pd.StringDtype(),\n            \"group_size\": pd.Int16Dtype(),\n            \"group_size_proportion\": pd.Float32Dtype(),\n            \"group_min_propensity\": pd.Float32Dtype(),\n            \"group_conv_rate\": pd.Float32Dtype(),\n            \"uplift\": pd.Int8Dtype(),\n            \"power\": pd.Int8Dtype(),\n            \"ci_level\": pd.Int8Dtype(),\n            \"required_sample_size\": pd.Int32Dtype(),\n        },\n    )\n    .pipe(ash.move_cols_to_front, [\"group_number\", \"maudience\"])\n)\ndisplay(df_sample_sizes_strategy_2.dtypes.rename(\"dtype\").to_frame().transpose())\ndf_sample_sizes_strategy_2\n\nSet all specified datatypes.\n\n\n\n\n\n\n\n\n\ngroup_number\nmaudience\ngroup_size\ngroup_size_proportion\ngroup_min_propensity\ngroup_conv_rate\nuplift\npower\nci_level\nrequired_sample_size\n\n\n\n\ndtype\nInt8\nstring[python]\nInt16\nFloat32\nFloat32\nFloat32\nInt8\nInt8\nInt8\nInt32\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup_number\nmaudience\ngroup_size\ngroup_size_proportion\ngroup_min_propensity\ngroup_conv_rate\nuplift\npower\nci_level\nrequired_sample_size\n\n\n\n\n0\n0\nHigh\n6721\n33.33168\n0.092205\n2.157417\n10\n55\n55\n2206\n\n\n1\n0\nHigh\n6721\n33.33168\n0.092205\n2.157417\n10\n55\n57\n2644\n\n\n2\n0\nHigh\n6721\n33.33168\n0.092205\n2.157417\n10\n55\n60\n3310\n\n\n3\n0\nHigh\n6721\n33.33168\n0.092205\n2.157417\n10\n55\n63\n3995\n\n\n4\n0\nHigh\n6721\n33.33168\n0.092205\n2.157417\n10\n55\n65\n4466\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1507\n2\nHigh-Medium-Low\n20163\n99.995041\n0.0\n2.306204\n15\n90\n70\n10097\n\n\n1508\n2\nHigh-Medium-Low\n20163\n99.995041\n0.0\n2.306204\n15\n90\n75\n11126\n\n\n1509\n2\nHigh-Medium-Low\n20163\n99.995041\n0.0\n2.306204\n15\n90\n85\n13940\n\n\n1510\n2\nHigh-Medium-Low\n20163\n99.995041\n0.0\n2.306204\n15\n90\n90\n16124\n\n\n1511\n2\nHigh-Medium-Low\n20163\n99.995041\n0.0\n2.306204\n15\n90\n95\n19783\n\n\n\n\n1512 rows × 10 columns"
  },
  {
    "objectID": "notebooks/02-train/notebooks/06_design_experiment.html#export-audience-sample-sizes-to-disk",
    "href": "notebooks/02-train/notebooks/06_design_experiment.html#export-audience-sample-sizes-to-disk",
    "title": "Post-Processing",
    "section": "Export Audience Sample Sizes to Disk",
    "text": "Export Audience Sample Sizes to Disk\nConcatenate the two DataFrames with the required sample sizes to acheive a KPI using different combinations of inputs (uplift, confidence level and power) and then export to disk\n\n(\n    df_combined_sample_sizes,\n    aud_sizes_fpath,\n) = ash.combine_and_export_sample_size_estimates(\n    df_sample_sizes_strategy_1,\n    df_sample_sizes_strategy_2,\n    df_deployment_candidate_mlflow_models,\n    processed_data_dir,\n)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/06_design_experiment.html#ml-experiment-tracking",
    "href": "notebooks/02-train/notebooks/06_design_experiment.html#ml-experiment-tracking",
    "title": "Post-Processing",
    "section": "ML Experiment Tracking",
    "text": "ML Experiment Tracking\nGet best run ID\n\nbest_run_id = df_deployment_candidate_mlflow_models.squeeze()[\"run_id\"]\n\nLog audience sample sizes file as MLFlow artifact\n\nwith mlflow.start_run(run_id=best_run_id) as run:\n    mlflow.log_artifact(aud_sizes_fpath)\nprint(\n    \"Logged required audience sizes as artifact in file \"\n    f\"{os.path.basename(aud_sizes_fpath)}\"\n)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/06_design_experiment.html#next-step",
    "href": "notebooks/02-train/notebooks/06_design_experiment.html#next-step",
    "title": "Post-Processing",
    "section": "Next Step",
    "text": "Next Step\nNext, test and control cohorts will be created using the inference data at the end of the production period (one month). The size of each cohort will be chosen based on the required sizes that were found in this step."
  },
  {
    "objectID": "notebooks/02-train/notebooks/07_get_audience_cohorts.html",
    "href": "notebooks/02-train/notebooks/07_get_audience_cohorts.html",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "",
    "text": "This step generates the randomized test and control cohorts needed to run a marketing campaign. The impact of the campaign on conversions (KPI) will be assessed using an A/B test at the end of the campaign. The randomized cohorts are needed in order to conduct this test. This is step 4. from a typical A/B Testing workflow.\n\n\n\nThis step can be run prospectively at the end of the inference period, just before the start of the campaign, when all the inference data (first-time visitors to the store) becomes available.\nFor the current use-case, the required sizes of one or more audience groups have been determined in the previous step. Recall that an audience strategy determines if one of more groups are used. For a strategy consisting of a single audience group, only the visitors predicted to have a high propensity to make a purchase on a return visit are selected to participate in the campaign and so randomized cohorts are to be drawn from this single group. For multiple audience groups, namely visitors predicted to have a low, medium or high propensity, randomized cohorts are to be drawn from each such group.\nWith this in mind, this step first assigns all first-time visitors to the store during the inference period to an audience group based on the requied audience strategy (single or multiple audience groups).\nNext, before generating audience cohorts, features in the unseen (inference) data are checked for data drift relative to the test data used during ML model development.\n\nUse statistical tests, that are called as part of EvidentlyAI’s data drift monitor (1, 2) for tabular data. The following statistical tests and logic are used by EvidentlyAI and are also implemented here\n\nnumerical features with less than or equal to 1,000 observations (this is not used here since the inference data consists of more than 1,000 observations during the inference data period)\n\nKolmogorov-Smirnov goodness-of-fit Test (two-sample version)\n\nnumerical features with more than 1,000 observations (this is the case here since the inference data has more than 1,000 first-time visitors during the inference data period)\n\nWasserstein distance\n\ncategorical features with less than or equal to 1,000 observations (not used here)\n\nchi-squared goodness-of-fit test\n\ncategorical features with more than 1,000 observations (used here)\n\nJensen-Shannon distance\n\n\nHere, a custom function is defined to manually implement these tests on the unseen (inference) data.\nPerform sanity checks on the data, by directly using EvidentlyAI’s data test suite (1, 2). A suite consisting of the following tests is applied to the unseen (inference) data in order to test that the number of\n\ncolumns with missing values (columns with at least one missing value)\nrows with missing values\nconstant (single-valued) columns\nduplicated rows (rows that are duplicates)\nduplicated columns (columns that are duplicates)\ncolumn datatypes\nempty columns (all values in a column are missing)\nempty rows (all values in a row are missing)\nrows with missing values (rows with at least one missing value)\nnumber of columns\n\nin the unseen (inference) data equals the number in the reference data (test split from the data used during ML model development).\n\nFinally, within each audience group, visitors are randomly assigned to test or control cohorts. A brief profile of each audience group is then performed using attributes of the visitors’ first visit. The profile and the inference data with the audience group and cohort for each first-time visitor during the inference period can be used by the marketing team to design and implement the marketing campaign aimed at growing the customer base.\n\n\n\nA custom python module in src/cohorts.py has been developed to create the test and control audience cohorts based on required sample sizes that were estimated in the previous step and logged as a MLFlow artifact (file) for the best MLFlow deployment candidate model.\nThe procedure followed in this step is briefle outlined below\n\nthe MLFlow artifact (file) is loaded in order to access the required sample sizes for each\n\naudience strategy\ndesired combination of effect sizes (power, confidence level and uplift)\n\nthe file is filtered based on the\n\naudience strategy\ndesired combination of effect sizes (see wanted_effect_sizes from User Inputs)\n\ninference data (first-time visitors) is retrieved and the best ML model is used to make inference predictions of the propensity of these visitors to make a purchase on a return (future) visit to the store.\npropensities are used to assign first-time visitors to audience groups (or bins)\n\nif the desired audience strategy is for a single audience group, then visitors are placed into a single bin\nif the desired audience strategy is for multiple audience groups, then multiple bins are creaed\n\nfor each bin (audience group), visitors are randomly placed into test and control cohorts\nthe inference data with\n\npredicted propensity (probability)\naudience group\ncohort (test or control)\n\nis then logged as a MLFlow artifact for use by the marketing team to build and design the campaign."
  },
  {
    "objectID": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#about",
    "href": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#about",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "",
    "text": "This step generates the randomized test and control cohorts needed to run a marketing campaign. The impact of the campaign on conversions (KPI) will be assessed using an A/B test at the end of the campaign. The randomized cohorts are needed in order to conduct this test. This is step 4. from a typical A/B Testing workflow.\n\n\n\nThis step can be run prospectively at the end of the inference period, just before the start of the campaign, when all the inference data (first-time visitors to the store) becomes available.\nFor the current use-case, the required sizes of one or more audience groups have been determined in the previous step. Recall that an audience strategy determines if one of more groups are used. For a strategy consisting of a single audience group, only the visitors predicted to have a high propensity to make a purchase on a return visit are selected to participate in the campaign and so randomized cohorts are to be drawn from this single group. For multiple audience groups, namely visitors predicted to have a low, medium or high propensity, randomized cohorts are to be drawn from each such group.\nWith this in mind, this step first assigns all first-time visitors to the store during the inference period to an audience group based on the requied audience strategy (single or multiple audience groups).\nNext, before generating audience cohorts, features in the unseen (inference) data are checked for data drift relative to the test data used during ML model development.\n\nUse statistical tests, that are called as part of EvidentlyAI’s data drift monitor (1, 2) for tabular data. The following statistical tests and logic are used by EvidentlyAI and are also implemented here\n\nnumerical features with less than or equal to 1,000 observations (this is not used here since the inference data consists of more than 1,000 observations during the inference data period)\n\nKolmogorov-Smirnov goodness-of-fit Test (two-sample version)\n\nnumerical features with more than 1,000 observations (this is the case here since the inference data has more than 1,000 first-time visitors during the inference data period)\n\nWasserstein distance\n\ncategorical features with less than or equal to 1,000 observations (not used here)\n\nchi-squared goodness-of-fit test\n\ncategorical features with more than 1,000 observations (used here)\n\nJensen-Shannon distance\n\n\nHere, a custom function is defined to manually implement these tests on the unseen (inference) data.\nPerform sanity checks on the data, by directly using EvidentlyAI’s data test suite (1, 2). A suite consisting of the following tests is applied to the unseen (inference) data in order to test that the number of\n\ncolumns with missing values (columns with at least one missing value)\nrows with missing values\nconstant (single-valued) columns\nduplicated rows (rows that are duplicates)\nduplicated columns (columns that are duplicates)\ncolumn datatypes\nempty columns (all values in a column are missing)\nempty rows (all values in a row are missing)\nrows with missing values (rows with at least one missing value)\nnumber of columns\n\nin the unseen (inference) data equals the number in the reference data (test split from the data used during ML model development).\n\nFinally, within each audience group, visitors are randomly assigned to test or control cohorts. A brief profile of each audience group is then performed using attributes of the visitors’ first visit. The profile and the inference data with the audience group and cohort for each first-time visitor during the inference period can be used by the marketing team to design and implement the marketing campaign aimed at growing the customer base.\n\n\n\nA custom python module in src/cohorts.py has been developed to create the test and control audience cohorts based on required sample sizes that were estimated in the previous step and logged as a MLFlow artifact (file) for the best MLFlow deployment candidate model.\nThe procedure followed in this step is briefle outlined below\n\nthe MLFlow artifact (file) is loaded in order to access the required sample sizes for each\n\naudience strategy\ndesired combination of effect sizes (power, confidence level and uplift)\n\nthe file is filtered based on the\n\naudience strategy\ndesired combination of effect sizes (see wanted_effect_sizes from User Inputs)\n\ninference data (first-time visitors) is retrieved and the best ML model is used to make inference predictions of the propensity of these visitors to make a purchase on a return (future) visit to the store.\npropensities are used to assign first-time visitors to audience groups (or bins)\n\nif the desired audience strategy is for a single audience group, then visitors are placed into a single bin\nif the desired audience strategy is for multiple audience groups, then multiple bins are creaed\n\nfor each bin (audience group), visitors are randomly placed into test and control cohorts\nthe inference data with\n\npredicted propensity (probability)\naudience group\ncohort (test or control)\n\nis then logged as a MLFlow artifact for use by the marketing team to build and design the campaign."
  },
  {
    "objectID": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#user-inputs",
    "href": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#user-inputs",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "User Inputs",
    "text": "User Inputs\nDefine the following\n\nstart and end dates for inference data\nname of column containing label (outcome)\nlist of categorical features\nlist of numerical features, including categorical features present in the raw data as integers\naudience_groups\n\ndesired audience groups into which the first-time visitors propensities will be placed\n\nnum_propens_groups specifies the number of desired audience propensity groups (low, medium and high)\npropens_group_labels specifies names of the desired audience propensity groups\n\n\ninputs (uplift, powerm confidence level) for which random cohort sizes are to be created\ntype of audience strategy (single- or multi- group) from which to create cohorts\n\n\n# 1. start and end dates\ninfer_start_date = \"20170301\"\ninfer_end_date = \"20170331\"\n\n# 2. label column\nlabel = \"made_purchase_on_future_visit\"\n\n# 3. categorical column\ncategoricals=[\n    'bounces',\n    'source',\n    'medium',\n    'channelGrouping',\n    'last_action',\n    'browser',\n    'os',\n    'deviceCategory',\n]\n\n# 4. numerical columns\nnumericals=[\n    'hits',\n    'pageviews',\n    'promos_displayed',\n    'product_views',\n    'product_clicks',\n    'time_on_site',\n]\n\n# 5. mapping dictionaries\naudience_groups_strategy_1 = {\n    \"num_propens_groups\": 3,\n    \"propens_group_labels\": [\"High\", \"Medium\", \"Low\"],\n}\naudience_groups_strategy_2 = {\n    \"num_propens_groups\": 3,\n    \"propens_group_labels\": [\"High\", \"High-Medium\", \"High-Medium-Low\"],\n}\n\n# 6. wanted inputs for estimating sample sizes\nwanted_inputs = {\n    \"uplift_percentage\": 10,\n    \"power_percentage\": 55,\n    \"confidence_level_percentage\": 55,\n}\n\n# 7. type of audience strategy to use when creating groups\naudience_strategy = 1\n\nCreate a mapping between audience group number (0, 1, 2) and group name\n\nmapper_dict_audience_strategy_1 = dict(\n    zip(\n        range(audience_groups_strategy_1[\"num_propens_groups\"]),\n        audience_groups_strategy_1[\"propens_group_labels\"],\n    )\n)\nmapper_dict_audience_strategy_2 = dict(\n    zip(\n        range(audience_groups_strategy_2[\"num_propens_groups\"]),\n        audience_groups_strategy_2[\"propens_group_labels\"],\n    )\n)\nprint(mapper_dict_audience_strategy_1)\nprint(mapper_dict_audience_strategy_2)\n\n{0: 'High', 1: 'Medium', 2: 'Low'}\n{0: 'High', 1: 'High-Medium', 2: 'High-Medium-Low'}\n\n\nGet desired effect size queries and number of audience groups\n\nif audience_strategy == 1:\n    query_inputs = (\n        f\"(uplift == {wanted_inputs['uplift_percentage']}) & \"\n        f\"(power == {wanted_inputs['power_percentage']}) & \"\n        f\"(ci_level == {wanted_inputs['confidence_level_percentage']})\"\n    )\nelse:\n    query_inputs = (\n        f\"(group_size_proportion &lt; 34) & \"\n        f\"(uplift == {wanted_inputs['uplift_percentage']}) & \"\n        f\"(power == {wanted_inputs['power_percentage']}) & \"\n        f\"(ci_level == {wanted_inputs['confidence_level_percentage']})\"\n    )\n\nnum_bins = (\n    audience_groups_strategy_1[\"num_propens_groups\"]\n    if audience_strategy == 1\n    else audience_groups_strategy_2[\"num_propens_groups\"]\n)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#get-inference-data",
    "href": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#get-inference-data",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "Get Inference Data",
    "text": "Get Inference Data\n\nquery_infer = sqlh.get_sql_query_infer(infer_start_date, infer_end_date)\nX_infer, _ = th.extract_data(query_infer, gcp_auth_dict).pipe(\n    th.transform_data,\n    datatypes_dict={k:v for k,v in dtypes_dict.items() if k != label},\n    duplicate_cols=[\"fullvisitorid\"],\n    column_mapper_dict={'last_action': action_mapper},\n)\nX_infer = X_infer.pipe(th.shuffle_data)\n\nQuery execution start time = 2023-06-05 19:14:19.839...done at 2023-06-05 19:14:25.363 (5.524 seconds).\nQuery returned 21,768 rows\nGot 21,752 rows and 27 columns after dropping duplicates\nTransformed data has 21,752 rows & 27 columns\n\n\nGet the size of each audience cohort in the inference data\n\nbin_size_infer = len(X_infer) / num_bins\nbin_size_infer_control = int(bin_size_infer / 2)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#get-model",
    "href": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#get-model",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "Get Model",
    "text": "Get Model\n\nFetch Latest Version of Best Deployment Candidate Model\nGet name of best deployment candidate model from MLFlow model registry\n\ndf_deployment_candidate_mlflow_models = modh.get_all_deployment_candidate_models()\nbest_run_model_name = modh.get_best_deployment_candidate_model(\n    df_deployment_candidate_mlflow_models\n)\nwith pd.option_context(\"display.max_colwidth\", None):\n    display(df_deployment_candidate_mlflow_models)\n\n\n\n\n\n\n\n\nname\ndescription\nrun_id\ntags\nversion\nscore\n\n\n\n\n0\nBetaDistClassifier_20160901_20170228_133892_feats__20230605_225228\nBest Model based on fbeta2 score of 0.4985941947\n742cf152d5094be0b686edf910c7d398\n{'deployment-candidate': 'yes'}\n2\n0.498594\n\n\n\n\n\n\n\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                  ('dummy',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures(threshold=0.7)),\n                ('resampler',\n                 RandomOverSampler(random_state=88, sampling_strategy=0.1)),\n                ['clf', BetaDistClassifier(a=0.3, b=2.5, random_state=88)]])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                  ('dummy',\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures(threshold=0.7)),\n                ('resampler',\n                 RandomOverSampler(random_state=88, sampling_strategy=0.1)),\n                ['clf', BetaDistClassifier(a=0.3, b=2.5, random_state=88)]])preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('aboveavg',\n                                                  AboveAveragePagePromoEngager(cols=['hits',\n                                                                                     'promos_displayed',\n                                                                                     'promos_clicked',\n                                                                                     'product_views',\n                                                                                     'product_clicks',\n                                                                                     'pageviews',\n                                                                                     'time_on_site'])),\n                                                 ('scaler', MinMaxScaler())]),\n                                 ['hits', 'promos_displayed', 'promos_clicked',\n                                  'product_views', 'product_clicks',\n                                  'pageviews', 'time_on_site']),\n                                ('cat',\n                                 P...\n                                                                                                    n_categories=1,\n                                                                                                    replace_with='other',\n                                                                                                    tol=0.1,\n                                                                                                    variables=['source',\n                                                                                                               'browser']),\n                                                                                   ['bounces',\n                                                                                    'last_action',\n                                                                                    'source',\n                                                                                    'medium',\n                                                                                    'channelGrouping',\n                                                                                    'browser',\n                                                                                    'os',\n                                                                                    'deviceCategory'])])),\n                                                 ('dummy',\n                                                  OneHotEncoder(drop='first',\n                                                                dtype=&lt;class 'int'&gt;,\n                                                                handle_unknown='ignore'))]),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])num['hits', 'promos_displayed', 'promos_clicked', 'product_views', 'product_clicks', 'pageviews', 'time_on_site']AboveAveragePagePromoEngagerAboveAveragePagePromoEngager(cols=['hits', 'promos_displayed', 'promos_clicked',\n                                   'product_views', 'product_clicks',\n                                   'pageviews', 'time_on_site'])MinMaxScalerMinMaxScaler()cat['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']rarecats: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('fe',\n                                 RareLabelEncoder(ignore_format=True,\n                                                  n_categories=1,\n                                                  replace_with='other', tol=0.1,\n                                                  variables=['source',\n                                                             'browser']),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])fe['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']RareLabelEncoderRareLabelEncoder(ignore_format=True, n_categories=1, replace_with='other',\n                 tol=0.1, variables=['source', 'browser'])remainderpassthroughpassthroughOneHotEncoderOneHotEncoder(drop='first', dtype=&lt;class 'int'&gt;, handle_unknown='ignore')DropCorrelatedFeaturesDropCorrelatedFeatures(threshold=0.7)RandomOverSamplerRandomOverSampler(random_state=88, sampling_strategy=0.1)BetaDistClassifierBetaDistClassifier(a=0.3, b=2.5, random_state=88)\n\n\n\n\nGet Data Used to Develop Best Deployment Candidate Model\nGet all available data used during model development of the best deployment candidate model\n\ndf_all = modh.get_data_for_run_id(df_deployment_candidate_mlflow_models, 'processed_data')\n\n\n\nCheck Data Drift and Stability\nGet test split from the data used during development of the best deployment candidate model. This was the last month of data used during ML model development and it will be considered as the reference data.\n\nX_test_best_run = (\n    df_all\n    .query(\"split_type == 'test'\")\n    .drop(columns=['split_type', label])\n)\n\nCheck numerical features for drift between\n\nML development data (test split)\nunseen data (inference)\n\nusing the Wasserstein test\n\ndf_num_drift, _ = dch.detect_features_drift(\n    X_infer,\n    X_test_best_run,\n    numericals=numericals,\n    categoricals=[],\n    kst_threshold=0.05,\n    kst_params=dict(\n        method='exact',\n        N=max(len(X_test_best_run), len(X_infer)),\n        alternative='two-sided',\n    ),\n    wsd_threshold=1,\n)\ndisplay(df_num_drift)\n\n\n\n\n\n\n\n\nfeature_type\nfeature\nnunique_ref\nnunique_curr\npct_diff_mean\npct_diff_std\nabs_diff_mean\nabs_diff_std\nmetric_value\nmetric_threshold\ndrift_detected\ntest_type\n\n\n\n\n0\nnumerical\nhits\n104\n112\n-2.788887\n-16.201796\n-0.172507\n-1.568929\n0.222779\n1\nFalse\nWasserstein\n\n\n1\nnumerical\npageviews\n78\n94\n-2.221251\n-19.111907\n-0.117295\n-1.415531\n0.162935\n1\nFalse\nWasserstein\n\n\n2\nnumerical\npromos_displayed\n21\n18\n-9.664244\n2.995538\n-0.754956\n0.316693\n0.786333\n1\nFalse\nWasserstein\n\n\n3\nnumerical\nproduct_views\n273\n286\n1.210796\n-5.643018\n0.284302\n-2.017041\n0.721632\n1\nFalse\nWasserstein\n\n\n4\nnumerical\nproduct_clicks\n30\n35\n-6.762275\n-13.531232\n-0.042843\n-0.246337\n0.042843\n1\nFalse\nWasserstein\n\n\n5\nnumerical\ntime_on_site\n1583\n1702\n-7.346253\n-6.512765\n-13.136182\n-25.665576\n13.500904\n1\nTrue\nWasserstein\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nFor descriptive statistics, consider drift being present if both of the following conditions are met\n\npct_diff_mean &gt; 5\n\npercent difference between mean value in reference (test data) and current (inference) data is larger than 5%\n\npct_diff_std &gt; 15\n\npercent difference between standard deviation (which provides an indication of variability) in reference (test data) and current (inference) data is larger than 15%\n\n\nBoth percent differnces are calculated relative to the reference data.\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThe percent differences in the descriptive statistics (mean and standard deviation) suggest that drift is minimal for the following numerical features\n\npromos_displayed\nproduct_views\ntime_on_site\nproduct_clicks\n\nThe pageviews and hits features show a drift from their higher variability, as suggested by their higher standard deviation, which in-turn indicates the presence of outliers. Both features do not show any drift in the mean percent difference.\nThe last four columns are related to the statistical test comparing the same feature in two datasets. A threshold of 1 was arbitrarily chosen. The limitation of this test is due to the difficulty in setting a reasonable threshold against which the metric can be compared. This test of drift could be useful if multiple inference periods were present during the production period. In such a scenario, the percent change in the metric could be tracked from one period to the next in order to determine if the feature is showing the presence of drift. These four columns are of less use in drift monitoring for a single infernce period.\n\n\n\nPerform sanity checks on all features between\n\nML development data (test split)\nunseen data (inference)\n\n\ndtypes_eai = dict(zip(categoricals, ['str']*len(categoricals)))\ndf_stability = dch.check_data_stability(\n    X_infer.astype(dtypes_eai),\n    X_test_best_run.astype(dtypes_eai),\n    numericals=numericals,\n    categoricals=categoricals,\n)\ndf_stability\n\nTest suite start time = 2023-06-05 19:14:26.354...done at 2023-06-05 19:14:39.566 (13.212 seconds).\n\n\n\n\n\n\n\n\n\nall_passed\ntotal_tests\nsuccess_tests\nfailed_tests\nsuccess\nlen_curr_data\nlen_ref_data\nfeatures_checked\nnum_features_checked\n\n\n\n\n0\nTrue\n10\n10\n0\n10\n21752\n20164\n[\"bounces\", \"source\", \"medium\", \"channelGroupi...\n14\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe EvidentlyAI TestSuite module requires string features to have the object datatype. So, a datatype mapping dictionary was defined (dtypes_eai) to change the datatypes for these features.\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nIt is reassuring that all data stability tests have passed. This indicates that features in the unseen (inference) data are stable in the 10 attributes listed in the About section, relative to those in the data used during ML development."
  },
  {
    "objectID": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#make-inference-predictions",
    "href": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#make-inference-predictions",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "Make Inference Predictions",
    "text": "Make Inference Predictions\nMake inference predictions with best deployment candidate model, using features extracted from the inference data\n\ny_infer_pred, y_infer_pred_proba = modh.make_inference(model, X_infer, label)\ndisplay(\n    y_infer_pred.value_counts(normalize=True).rename('proportion').to_frame().merge(\n        y_infer_pred.value_counts(normalize=False).rename('number').to_frame(),\n        left_index=True,\n        right_index=True,\n        how='left',\n    ).reset_index()\n)\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPer the business use-case, these are predictions of whether a visitor will make a purchase during a later (return) visit to the merchandise store. Such predictions are made using attributes (features) of the first visit by visitors to the store and they can only be evaluated at a later time (after the outcome of the same visitor’s later visit is known)."
  },
  {
    "objectID": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#create-cohorts",
    "href": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#create-cohorts",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "Create Cohorts",
    "text": "Create Cohorts\nPerform the following to prepare the inference observations for extracting cohorts from the audience group(s)\n\ncombine inference features and predicted hard and soft labels into single DataFrame\nsort predictions in ascending order of the predicted probability (propensity) (score)\nseparate the predictions into bins, using the predicted probability, based on the required number of bins\nrename bin_number column (for use in downstream step)\nset datatypes for columns\n\n\ndf_infer_pred = (\n    ch.combine_infer_data(X_infer, y_infer_pred, y_infer_pred_proba)\n    .pipe(ash.sort_scores, False)\n    .pipe(ash.get_audience_groups_by_propensity, num_bins)\n    .pipe(ch.rename_columns, {\"group_number\": \"maudience\"})\n    .pipe(\n        ash.set_datatypes,\n        {\n            \"row_number\": pd.Int16Dtype(),\n            \"fullvisitorid\": pd.StringDtype(),\n            \"score\": pd.Float32Dtype(),\n            \"predicted_score_label\": pd.BooleanDtype(),\n            \"maudience\": pd.Int8Dtype(),\n        },\n    )\n)\nwith pd.option_context(\"display.max_columns\", 1000):\n    display(df_infer_pred.dtypes.rename(\"dtype\").to_frame().T)\n    display(df_infer_pred)\n\nSet all specified datatypes.\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nsource\nmedium\nchannelGrouping\nhits\nbounces\nlast_action\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntime_on_site\nbrowser\nos\ndeviceCategory\nadded_to_cart\nscore\npredicted_score_label\nrow_number\nmaudience\n\n\n\n\ndtype\nstring[python]\nstring[python]\nInt8\ndatetime64[ns]\nInt8\nInt8\nInt8\nInt8\nInt8\nInt8\nInt8\ncategory\ncategory\ncategory\nInt16\ncategory\ncategory\nInt16\nInt16\nInt16\nInt16\nInt16\nInt16\ncategory\ncategory\ncategory\nInt16\nFloat32\nboolean\nInt16\nInt8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nsource\nmedium\nchannelGrouping\nhits\nbounces\nlast_action\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntime_on_site\nbrowser\nos\ndeviceCategory\nadded_to_cart\nscore\npredicted_score_label\nrow_number\nmaudience\n\n\n\n\n5667\n0680963722765513682\n1490246407\n1\n2017-03-22 22:20:07\n1\n3\n22\n4\n22\n20\n7\ngoogle\norganic\nOrganic Search\n10\n0\nProduct detail views\n9\n0\n12\n2\n8\n127\nChrome\nMacintosh\ndesktop\n0\n0.955219\nFalse\n0\n0\n\n\n966\n5015398141797883848\n1488954219\n1\n2017-03-07 22:23:39\n1\n3\n7\n3\n22\n23\n39\ngoogle\norganic\nOrganic Search\n20\n0\nProduct detail views\n9\n0\n163\n1\n19\n663\nChrome\nWindows\ndesktop\n0\n0.939487\nFalse\n1\n0\n\n\n2368\n4256619380024888759\n1490567835\n1\n2017-03-26 15:37:15\n1\n3\n26\n1\n15\n37\n15\ngoogle\norganic\nOrganic Search\n1\n1\nUnknown\n0\n0\n12\n0\n1\n0\nChrome\nWindows\ndesktop\n0\n0.932821\nFalse\n2\n0\n\n\n9009\n3214011699506166608\n1489445124\n1\n2017-03-13 15:45:24\n1\n3\n13\n2\n15\n45\n24\n(direct)\n(none)\nDirect\n15\n0\nCompleted purchase\n9\n0\n16\n1\n13\n414\nChrome\nChrome OS\ndesktop\n1\n0.932164\nFalse\n3\n0\n\n\n10507\n0716469409246179583\n1489155359\n1\n2017-03-10 06:15:59\n1\n3\n10\n6\n6\n15\n59\ngoogle\norganic\nOrganic Search\n1\n1\nUnknown\n0\n0\n12\n0\n1\n0\nChrome\nWindows\ndesktop\n0\n0.931829\nFalse\n4\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1271\n2297134143716418806\n1489463792\n1\n2017-03-13 20:56:32\n1\n3\n13\n2\n20\n56\n32\ngoogle\norganic\nOrganic Search\n1\n1\nUnknown\n0\n0\n0\n0\n1\n0\nChrome\nAndroid\nmobile\n0\n0.0\nFalse\n21747\n2\n\n\n16990\n4875088489598899615\n1489358551\n1\n2017-03-12 15:42:31\n1\n3\n12\n1\n15\n42\n31\ngoogle\ncpc\nPaid Search\n8\n0\nUnknown\n18\n0\n72\n0\n8\n677\nChrome\nAndroid\nmobile\n0\n0.0\nFalse\n21748\n2\n\n\n832\n504996867976621481\n1488752321\n1\n2017-03-05 14:18:41\n1\n3\n5\n1\n14\n18\n41\n(direct)\n(none)\nDirect\n12\n0\nProduct detail views\n18\n0\n37\n1\n11\n898\nChrome\nChrome OS\ndesktop\n0\n0.0\nFalse\n21749\n2\n\n\n4867\n3772471281235004535\n1488495634\n1\n2017-03-02 15:00:34\n1\n3\n2\n5\n15\n0\n34\nyoutube.com\nreferral\nSocial\n3\n0\nUnknown\n9\n0\n2\n0\n3\n101\nInternet Explorer\nWindows\ndesktop\n0\n0.0\nFalse\n21750\n2\n\n\n7383\n5311563555589439877\n1488382048\n1\n2017-03-01 07:27:28\n1\n3\n1\n4\n7\n27\n28\ngoogle\norganic\nOrganic Search\n4\n0\nUnknown\n9\n0\n24\n0\n4\n66\nChrome\nWindows\ndesktop\n0\n0.0\nFalse\n21751\n2\n\n\n\n\n21752 rows × 31 columns\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThis is the same data preparation that was used during the (previous) sample size estimation step step in order to prepare the test data split for estimating the required sample size.\n\n\n\nPerform the following to get the test and control cohorts from the inference data\n\nload estimates for required sample sizes associated with best deployment candidate model\nget required sample sizes for the chosen audience strategy\nget required sample sizes that can be supported by size of inference data\nget required sample sizes that support all required audience groups (bins)\nget required sample sizes that capture required effect sizes\nget random test and control cohorts\n\n\ndf_infer_audience_groups = (\n    ch.load_file_from_mlflow_artifact(\n        df_deployment_candidate_mlflow_models, \"audience_sample_sizes\"\n    )\n    .pipe(ch.get_sample_sizes_by_strategy, audience_strategy)\n    .pipe(ch.get_suitable_sample_sizes, bin_size_infer_control)\n    .pipe(ch.get_sample_sizes_with_all_audience_groups, num_bins)\n    .pipe(ch.get_required_inputs, query_inputs)\n    .pipe(\n        ch.create_cohorts,\n        df_infer_pred,\n        mapper_dict_audience_strategy_1\n        if audience_strategy == 1\n        else mapper_dict_audience_strategy_2,\n        audience_strategy,\n    )\n    .pipe(\n        ash.set_datatypes,\n        {\n            \"maudience\": pd.StringDtype(),\n            \"cohort\": pd.StringDtype(),\n            \"audience_strategy\": pd.Int8Dtype(),\n        },\n    )\n)\nwith pd.option_context(\"display.max_columns\", 1000):\n    display(df_infer_audience_groups)\n\naudience=0: High, size=7,251,  excluded=2,839, wanted=2,206, control=2,206, test=2,206\naudience=1: Medium, size=7,250,  excluded=3,456, wanted=1,897, control=1,897, test=1,897\naudience=2: Low, size=7,251,  excluded=3,045, wanted=2,103, control=2,103, test=2,103\nFound suitable sample sizes and generated cohorts.\nSet all specified datatypes.\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nsource\nmedium\nchannelGrouping\nhits\nbounces\nlast_action\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntime_on_site\nbrowser\nos\ndeviceCategory\nadded_to_cart\nscore\npredicted_score_label\nmaudience\ncohort\naudience_strategy\n\n\n\n\n0\n0680963722765513682\n1490246407\n1\n2017-03-22 22:20:07\n1\n3\n22\n4\n22\n20\n7\ngoogle\norganic\nOrganic Search\n10\n0\nProduct detail views\n9\n0\n12\n2\n8\n127\nChrome\nMacintosh\ndesktop\n0\n0.955219\nFalse\nHigh\nControl\n1\n\n\n1\n3339620488023808675\n1489208703\n1\n2017-03-10 21:05:03\n1\n3\n10\n6\n21\n5\n3\n(direct)\n(none)\nDirect\n6\n0\nProduct detail views\n9\n0\n12\n2\n4\n34\nSafari\niOS\ntablet\n0\n0.873791\nFalse\nHigh\nControl\n1\n\n\n2\n4352137968854306424\n1489597665\n1\n2017-03-15 10:07:45\n1\n3\n15\n4\n10\n7\n45\ngoogle\norganic\nOrganic Search\n2\n0\nUnknown\n9\n0\n0\n0\n2\n7\nSafari\nMacintosh\ndesktop\n0\n0.865694\nFalse\nHigh\nControl\n1\n\n\n3\n415961985355681274\n1489621178\n1\n2017-03-15 16:39:38\n1\n3\n15\n4\n16\n39\n38\n(direct)\n(none)\nDirect\n4\n0\nUnknown\n9\n0\n39\n0\n4\n90\nChrome\nWindows\ndesktop\n0\n0.862547\nFalse\nHigh\nControl\n1\n\n\n4\n8333492141906527532\n1490084582\n1\n2017-03-21 01:23:02\n1\n3\n21\n3\n1\n23\n2\n(direct)\n(none)\nDirect\n1\n1\nUnknown\n9\n0\n0\n0\n1\n0\nSafari\niOS\ntablet\n0\n0.849589\nFalse\nHigh\nControl\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n21747\n6565680845296579697\n1490824341\n1\n2017-03-29 14:52:21\n1\n3\n29\n4\n14\n52\n21\ngoogle\norganic\nOrganic Search\n1\n1\nUnknown\n0\n0\n12\n0\n1\n0\nChrome\nChrome OS\ndesktop\n0\n0.0\nFalse\nLow\n&lt;NA&gt;\n1\n\n\n21748\n536791133490221570\n1490224587\n1\n2017-03-22 16:16:27\n1\n3\n22\n4\n16\n16\n27\n(direct)\n(none)\nDirect\n4\n0\nUnknown\n9\n0\n24\n0\n4\n108\nChrome\nMacintosh\ndesktop\n0\n0.0\nFalse\nLow\n&lt;NA&gt;\n1\n\n\n21749\n2449684517758558577\n1489676669\n1\n2017-03-16 08:04:29\n1\n3\n16\n5\n8\n4\n29\ngoogle\norganic\nOrganic Search\n2\n0\nUnknown\n0\n0\n12\n0\n2\n7\nSafari\niOS\nmobile\n0\n0.0\nFalse\nLow\n&lt;NA&gt;\n1\n\n\n21750\n3477856770102973339\n1489148447\n1\n2017-03-10 04:20:47\n1\n3\n10\n6\n4\n20\n47\n(direct)\n(none)\nDirect\n1\n1\nUnknown\n0\n0\n0\n0\n1\n0\nChrome\nLinux\ndesktop\n0\n0.0\nFalse\nLow\n&lt;NA&gt;\n1\n\n\n21751\n3772471281235004535\n1488495634\n1\n2017-03-02 15:00:34\n1\n3\n2\n5\n15\n0\n34\nyoutube.com\nreferral\nSocial\n3\n0\nUnknown\n9\n0\n2\n0\n3\n101\nInternet Explorer\nWindows\ndesktop\n0\n0.0\nFalse\nLow\n&lt;NA&gt;\n1\n\n\n\n\n21752 rows × 32 columns\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe final sample size used to generate the cohorts is the least number of samples (first-time visitors) to be included in each of the control and test cohorts of each audience group. These sizes come from the output of the previous step (designing a media or marketing experiment - 1, 2).\n\n\n\nThe treatment (or test) and control groups should be similar to each other for the property to be tested. This is a fundamental requirement of test and control groups. In this case, the probability (score column) is the property of interest. With this in mind, we now show selected descriptive statistics, for the score column, for both the test (treatment) and control cohorts within each desired audience group (low, medium and high propensity)\n\ndf_aud_stats = ch.get_cohort_stats(df_infer_audience_groups)\ndf_aud_stats\n\n\n\n\n\n\n\n\nmaudience\ncohort\nscore_count\nscore_min\nscore_mean\nscore_median\nscore_max\n\n\n\n\n0\nHigh\nControl\n2206\n0.093956\n0.281044\n0.225347\n0.955219\n\n\n1\nHigh\nTest\n2206\n0.094071\n0.278541\n0.225066\n0.920158\n\n\n2\nHigh\n&lt;NA&gt;\n2839\n0.094022\n0.280057\n0.226255\n0.939487\n\n\n3\nLow\nControl\n2103\n0.0\n0.001912\n0.000802\n0.008468\n\n\n4\nLow\nTest\n2103\n0.0\n0.001858\n0.000675\n0.00845\n\n\n5\nLow\n&lt;NA&gt;\n3045\n0.0\n0.001973\n0.000881\n0.008467\n\n\n6\nMedium\nControl\n1897\n0.008473\n0.039143\n0.033061\n0.093524\n\n\n7\nMedium\nTest\n1897\n0.00847\n0.039329\n0.033066\n0.093933\n\n\n8\nMedium\n&lt;NA&gt;\n3456\n0.008468\n0.03929\n0.033022\n0.093914\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nIt is reassuring that there is agreement in the statistics for the probabilities per Test-Control cohort within the same audience group.\n\n\n\nFinally, the audience groups in the inference data are now briefly profiled, in order to identify characteristics that might help the marketing team build an appropriate strategy to be implemented during the campaign. For each audience group, the profile consists of the following\n\nproportion of visitors who displayed the following behavior\n\nwhose last action during thier first visit was\n\nlast_action == 'Add product(s) to cart'\nlast_action == 'Product detail_views'\nlast_action == 'Click through of product lists'\n\nwho\n\nused a medium == 'referral' to get to the store’s website on their first visit\nadded one item to their shopping cart (added_to_cart &gt; 0) on their first visit\n\nwhose\n\nfirst visit occurred on a weekend (day_of_week.isin(@weekend_days))\n\n\nbounce rate during all first visits\nfollowing descriptive statistics\n\nhour, day_of_week\n\nstatistic: mean\n\nsource, medium, channelGrouping, last_action, browser, os, deviceCategory\n\nstatistic: mode (most common value)\n\npromos_displayed, promos_clicked, product_views, product_clicks, pageviews, added_to_cart\n\nstatistics: mean, max\n\n\n\n\ndf_profile = pa.get_audience_profile(df_infer_audience_groups)\nwith pd.option_context(\"display.max_columns\", 1000):\n    display(df_profile)\n\nSet all specified datatypes.\nSet all specified datatypes.\n\n\n\n\n\n\n\n\nmaudience\nstat\nstat_type\nHigh\nLow\nMedium\ncolumn\n\n\n\n\n0\nhour__mean\nmean\n13.030892290718521\n12.915735760584747\n13.075586206896551\nhour\n\n\n1\nday_of_week__mean\nmean\n4.009515928837402\n3.9891049510412357\n4.013379310344828\nday_of_week\n\n\n2\nsource__mode\nmode\ngoogle\ngoogle\ngoogle\nsource\n\n\n3\nmedium__mode\nmode\norganic\norganic\norganic\nmedium\n\n\n4\nchannelGrouping__mode\nmode\nOrganic Search\nOrganic Search\nOrganic Search\nchannelGrouping\n\n\n5\nlast_action__mode\nmode\nUnknown\nUnknown\nUnknown\nlast_action\n\n\n6\nbrowser__mode\nmode\nChrome\nChrome\nChrome\nbrowser\n\n\n7\nos__mode\nmode\nMacintosh\nMacintosh\nMacintosh\nos\n\n\n8\ndeviceCategory__mode\nmode\ndesktop\ndesktop\ndesktop\ndeviceCategory\n\n\n9\nhits__mean\nmean\n6.39167011446697\n6.0924010481312925\n6.590068965517242\nhits\n\n\n10\nhits__max\nmax\n271\n157\n500\nhits\n\n\n11\npromos_displayed__mean\nmean\n8.802647910633016\n8.409184940008275\n8.488551724137931\npromos_displayed\n\n\n12\npromos_displayed__max\nmax\n189\n171\n144\npromos_displayed\n\n\n13\npromos_clicked__mean\nmean\n0.0\n0.00013791201213625708\n0.0\npromos_clicked\n\n\n14\npromos_clicked__max\nmax\n0\n1\n0\npromos_clicked\n\n\n15\nproduct_views__mean\nmean\n23.350572334850366\n22.487243138877396\n23.75103448275862\nproduct_views\n\n\n16\nproduct_views__max\nmax\n670\n395\n987\nproduct_views\n\n\n17\nproduct_clicks__mean\nmean\n0.6669424906909391\n0.654530409598676\n0.7077241379310345\nproduct_clicks\n\n\n18\nproduct_clicks__max\nmax\n79\n50\n48\nproduct_clicks\n\n\n19\npageviews__mean\nmean\n5.445593711212247\n5.184112536201903\n5.564\npageviews\n\n\n20\npageviews__max\nmax\n249\n101\n466\npageviews\n\n\n21\nadded_to_cart__mean\nmean\n0.1998345055854365\n0.18673286443249207\n0.20220689655172414\nadded_to_cart\n\n\n22\nadded_to_cart__max\nmax\n22\n25\n23\nadded_to_cart\n\n\n23\nlast_action_added_to_cart\nbehavior\n4.9648324369052546\n4.840711625982623\n5.489655172413793\n&lt;NA&gt;\n\n\n24\nviewed_product_detail\nbehavior\n0.0\n0.0\n0.0\n&lt;NA&gt;\n\n\n25\nclicked_through_product_lists\nbehavior\n0.05516480485450283\n0.08274720728175423\n0.027586206896551724\n&lt;NA&gt;\n\n\n26\nused_referral\nbehavior\n24.741414977244517\n24.658667769962765\n23.66896551724138\n&lt;NA&gt;\n\n\n27\nadded_gt_1_to_cart\nbehavior\n3.847745138601572\n3.723624327678941\n4.3310344827586205\n&lt;NA&gt;\n\n\n28\nweekend_visitors\nbehavior\n17.51482554130465\n17.85960557164529\n18.689655172413794\n&lt;NA&gt;\n\n\n29\nbounce_rate\nbehavior\n33.48503654668322\n33.71948696731485\n32.855172413793106\n&lt;NA&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe audience profile consists of different types of statistics about the first visit, for each audience group.\nThe stat column shows the attribute from the first visit in the inference data for which a statistic is calculated.\nThe stat_type column indicates the type of statistic.\nThe High, Medium and Low columns (or just the High column, if the single-group audience strategy is being used) show the statistic of each attribute within each audience group."
  },
  {
    "objectID": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#export-to-disk-and-ml-experiment-tracking",
    "href": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#export-to-disk-and-ml-experiment-tracking",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "Export to Disk and ML Experiment Tracking",
    "text": "Export to Disk and ML Experiment Tracking\nGet the best MLFlow run ID\n\nbest_run_id = df_deployment_candidate_mlflow_models.squeeze()[\"run_id\"]\n\n\nAudience Cohorts\n\n\n\n\n\n\nNotes\n\n\n\n\nThe cohort column has missing values for visitors who were not assigned to either the test or control groups. This is expected.\n\n\n\nExport to disk and log exported file as MLFlow artifact\n\nut.export_and_track(\n    os.path.join(\n        processed_data_dir,\n        f\"audience_cohorts__run_\"\n        f\"{best_run_id}__\"\n        f\"infer_month_{month_name[1:][X_infer['month'].iloc[-1] - 1]}__\"\n        f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet.gzip\",\n    ),\n    df_infer_audience_groups,\n    (\n        \"inference audience cohorts for \"\n        f\"{month_name[1:][X_infer['month'].iloc[-1] - 1]}\"\n    ),\n    best_run_id,\n)\n\n\n\nAudience Profiles\n\n\n\n\n\n\nNotes\n\n\n\n\nThe behavioral attributes in the profile are not specific to an individual column. So, the column in the profile DataFrame has missing values for these attributes.\n\n\n\nExport to disk and log exported file as MLFlow artifact\n\nut.export_and_track(\n    os.path.join(\n        processed_data_dir,\n        f\"audience_profiles__run_\"\n        f\"{best_run_id}__\"\n        f\"infer_month_{month_name[1:][X_infer['month'].iloc[-1] - 1]}__\"\n        f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet.gzip\",\n    ),\n    df_profile,\n    (\n        \"inference audience profiles for \"\n        f\"{month_name[1:][X_infer['month'].iloc[-1] - 1]}\"\n    ),\n    best_run_id,\n)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#next-step",
    "href": "notebooks/02-train/notebooks/07_get_audience_cohorts.html#next-step",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "Next Step",
    "text": "Next Step\nThe next step will add to the audience profiles, by exploring the best ML model’s predictions of propensity for first-time visitors to the store during the unseen (inference) data period."
  },
  {
    "objectID": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html",
    "href": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html",
    "title": "Run Campaign and Analyze Results",
    "section": "",
    "text": "This step assess the impact of running the marketing campaign. This step can be performed retrospectively, at the end of the campaign, when the outcome of the return visit of the first-time visitors to the merchandise store (during the inference period) is known. This is step 5. from a typical A/B Testing workflow.\nFor the current use-case, if the marketing campaign results in more conversions in the control cohort compared to the test cohort, then this could suggest that the campaign has grown the customer base and thereby met the objective of this project. However, a test of statistical significance will be needed in order to ensure that this impact seen by running the campaign (growth in conversions) was not a random occurrence.\nThis step compares the proportions (conversions) taken from two independent samples (test and control cohorts). The purpose is to determine if the conversion rate (KPI) of the test cohort is statistically different from that of the control cohort. If the\n\nconversion rate is higher in the test cohort\ndifference in conversion rate between the test and control cohort is statistically significant at some level of confidence (eg. 95%)\n\nthen it is possible to say with 95% confidence that the campaign has grown the customer base.\n\n\n\nIn python, such a comparison is implemented using the statsmodels library in the proportions_chisquare() method where the count parameter represents the number of convertions in each cohort (test or control) and nobs represents the overall size of the same cohort."
  },
  {
    "objectID": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html#about",
    "href": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html#about",
    "title": "Run Campaign and Analyze Results",
    "section": "",
    "text": "This step assess the impact of running the marketing campaign. This step can be performed retrospectively, at the end of the campaign, when the outcome of the return visit of the first-time visitors to the merchandise store (during the inference period) is known. This is step 5. from a typical A/B Testing workflow.\nFor the current use-case, if the marketing campaign results in more conversions in the control cohort compared to the test cohort, then this could suggest that the campaign has grown the customer base and thereby met the objective of this project. However, a test of statistical significance will be needed in order to ensure that this impact seen by running the campaign (growth in conversions) was not a random occurrence.\nThis step compares the proportions (conversions) taken from two independent samples (test and control cohorts). The purpose is to determine if the conversion rate (KPI) of the test cohort is statistically different from that of the control cohort. If the\n\nconversion rate is higher in the test cohort\ndifference in conversion rate between the test and control cohort is statistically significant at some level of confidence (eg. 95%)\n\nthen it is possible to say with 95% confidence that the campaign has grown the customer base.\n\n\n\nIn python, such a comparison is implemented using the statsmodels library in the proportions_chisquare() method where the count parameter represents the number of convertions in each cohort (test or control) and nobs represents the overall size of the same cohort."
  },
  {
    "objectID": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html#user-inputs",
    "href": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html#user-inputs",
    "title": "Run Campaign and Analyze Results",
    "section": "User Inputs",
    "text": "User Inputs\nDefine the following\n\nstart and end dates for inference data\nconfidence levels at which the difference in the cohort conversion rates is to be checked\n\n\n# 1. start and end dates\ninfer_start_date = \"20170301\"\ninfer_end_date = \"20170331\"\n\n# 2. confidence levels to check difference in conversion rates\nci_levels = np.arange(0.20, 1.00, 0.05, dtype=float)\n\nThe following helper functions are defined in the module src/statistical_checks.py and are used here\n\nget_inference_data_with_cohorts()\n\nloads inference predictions data with audience groups and cohorts assigned as separate columns\n\nget_outcome_labels()\n\nloads the outcome (ML label) of the inference predictions data\nfor demonstration purposes only, this outcome is randomly generated here\n\nget_cohorts()\n\nfilters audience data to retrieve visitors that were placed in one of the two audience cohorts (test or control)\n\nget_overall_and_converted_cohort_sizes\n\ncalculate the size of the overall cohort and the conversions, for both test and control cohorts\n\ncheck_significance_using_chisq()\n\nchecks significance of the difference in conversions between test and control cohort"
  },
  {
    "objectID": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html#get-data",
    "href": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html#get-data",
    "title": "Run Campaign and Analyze Results",
    "section": "Get Data",
    "text": "Get Data\n\nFetch Latest Version of Best Deployment Candidate Model from Model Registry\nGet best deployment candidate model from model registry\n\ndf_candidate_mlflow_models = modh.get_all_deployment_candidate_models()\n\n\n\nGet Inference Data with Audience Cohorts, Associated with Best Model\nLoad inference data, with the audience groups and cohorts shown as separate columns. This data should contain the outcome for all these first-time visitors to the store during the inference period. Filter this data to only get visitors who were placed in a test or control cohorts, and exclude others. This is done below using random outcomes (for demonstration purposes)\n\ndf_infer_audience_cohorts = (\n    sc.get_inference_data_with_cohorts(df_candidate_mlflow_models, \"audience_cohorts\")\n    .pipe(sc.get_outcome_labels)\n    .pipe(sc.get_cohorts, \"cohort\")\n)\n\n\n\n\n\n\n\nNotes\n\n\n\n\nsc.get_outcome_labels() retrieves the campaign outcomes, which is the number of conversions in both the control and test cohorts. Here, for demonstration purposes, random values are used for the outcomes for both cohorts."
  },
  {
    "objectID": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html#compare-difference-in-kpi-between-cohorts",
    "href": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html#compare-difference-in-kpi-between-cohorts",
    "title": "Run Campaign and Analyze Results",
    "section": "Compare Difference in KPI Between Cohorts",
    "text": "Compare Difference in KPI Between Cohorts\nCheck if the difference between conversions across the two cohorts is statistically significant\n\noverall_sizes, conversion_sizes = sc.get_overall_and_converted_cohort_sizes(\n    df_infer_audience_cohorts, verbose=False\n)\ndf_sig_checks = sc.check_significance_using_chisq(\n    overall_sizes, conversion_sizes, ci_levels\n).pipe(\n    ash.set_datatypes,\n    {\n        \"check\": pd.StringDtype(),\n        \"p_value\": pd.Float32Dtype(),\n        \"ci_level\": pd.Int8Dtype(),\n        \"control_size\": pd.Int16Dtype(),\n        \"test_size\": pd.Int16Dtype(),\n        \"control_conversions\": pd.Int16Dtype(),\n        \"test_conversions\": pd.Int16Dtype(),\n        \"control_conversion_rate\": pd.Float32Dtype(),\n        \"test_conversion_rate\": pd.Float32Dtype(),\n    },\n)\ndf_sig_checks\n\nSet all specified datatypes.\n\n\n\n\n\n\n\n\n\ncheck\np_value\nci_level\ncontrol_size\ntest_size\ncontrol_conversions\ntest_conversions\ncontrol_conversion_rate\ntest_conversion_rate\n\n\n\n\n0\nnot statistically significant\n0.579219\n94\n6206\n6206\n330\n344\n5.317435\n5.543023\n\n\n1\nstatistically significant\n0.579219\n40\n6206\n6206\n330\n344\n5.317435\n5.543023\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe chi-squared test indicates if the difference between the conversion rate in the test and control cohort is statistically significant at a particular confidence level.\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThe top row shows the test of significance for the maximum specified confidence level (in this case 94%).\nThe bottom row shows the maximum confidence level (40%) for which the difference in conversion rates is significant."
  },
  {
    "objectID": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html#export-to-disk-and-ml-experiment-tracking",
    "href": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html#export-to-disk-and-ml-experiment-tracking",
    "title": "Run Campaign and Analyze Results",
    "section": "Export to Disk and ML Experiment Tracking",
    "text": "Export to Disk and ML Experiment Tracking\nGet the best MLFlow run ID\n\nbest_run_id = df_candidate_mlflow_models.squeeze()[\"run_id\"]\n\n\n\n\n\n\n\n\n\n\ncolumn\ndtype\nmissing\n\n\n\n\n0\ncheck\nstring[python]\n0\n\n\n1\np_value\nFloat32\n0\n\n\n2\nci_level\nInt8\n0\n\n\n3\ncontrol_size\nInt16\n0\n\n\n4\ntest_size\nInt16\n0\n\n\n5\ncontrol_conversions\nInt16\n0\n\n\n6\ntest_conversions\nInt16\n0\n\n\n7\ncontrol_conversion_rate\nFloat32\n0\n\n\n8\ntest_conversion_rate\nFloat32\n0\n\n\n\n\n\n\n\nExport to disk and log exported file as MLFlow artifact\n\nut.export_and_track(\n    os.path.join(\n        processed_data_dir,\n        f\"campaign_analysis__run_\"\n        f\"{best_run_id}__\"\n        f\"infer_month_{infer_month}__\"\n        f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet.gzip\",\n    ),\n    df_sig_checks,\n    f\"campaign outcome analysis for inference during {infer_month}\",\n    best_run_id,\n)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html#next-step",
    "href": "notebooks/02-train/notebooks/10_analyze_campaign_outputs.html#next-step",
    "title": "Run Campaign and Analyze Results",
    "section": "Next Step",
    "text": "Next Step\nThe next step will clean up all project resources related to MLFlow."
  },
  {
    "objectID": "notebooks/02-train/notebooks/11_cleanup_mlflow.html",
    "href": "notebooks/02-train/notebooks/11_cleanup_mlflow.html",
    "title": "Cleanup Model Monitoring Outputs",
    "section": "",
    "text": "This step cleans up all project resources that were created, including\n\nintermediate (non-MLFlow) files\nMLFlow outputs (artifacts, metrics, etc.)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/11_cleanup_mlflow.html#about",
    "href": "notebooks/02-train/notebooks/11_cleanup_mlflow.html#about",
    "title": "Cleanup Model Monitoring Outputs",
    "section": "",
    "text": "This step cleans up all project resources that were created, including\n\nintermediate (non-MLFlow) files\nMLFlow outputs (artifacts, metrics, etc.)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/11_cleanup_mlflow.html#user-inputs",
    "href": "notebooks/02-train/notebooks/11_cleanup_mlflow.html#user-inputs",
    "title": "Cleanup Model Monitoring Outputs",
    "section": "User Inputs",
    "text": "User Inputs\nDefine the file name prefix for the following MLFlow artifacts associated with the best deployment candidate MLFLow ML model\n\nprocessed data\nthreshold tuning data, where predictions per threshold were evaluated using the test data split\nestimated sample sizes, where sizes were estimated using the test data split\naudience cohorts created using the inference data split\naudience profiles using the inference data split\npost-campaign impact evaluation\n\n\nartifact_file_prefixes = [\n    'processed_data',\n    \"threshold_tuning_data\",\n    \"audience_sample_sizes\",\n    \"audience_cohorts\",\n    \"audience_profiles\",\n    \"campaign_analysis\",\n]\n\nDefine a helper function to get the filepath to an MLFlow artifact and the associated local (non-MLFlow) file\n\ndef show_artifact_and_local_filepath_for_run_id(\n    df: pd.DataFrame, file_prefix: str, local_dir: str\n) -&gt; pd.DataFrame:\n    \"\"\"Get artifact and local file associated with a run ID.\"\"\"\n    artifact_par_dir = mlflow.artifacts.download_artifacts(\n        run_id=df.squeeze()[\"run_id\"]\n    )\n\n    glob_str = f\"{file_prefix}__run_*.parquet.gzip\"\n    artifact_glob_dir_str = os.path.join(artifact_par_dir, glob_str)\n    artifact_fpath = glob(artifact_glob_dir_str)[-1]\n\n    artifact_dir = os.path.basename(artifact_fpath)\n    local_fpath = os.path.join(local_dir, artifact_dir)\n    return [artifact_fpath, local_fpath]"
  },
  {
    "objectID": "notebooks/02-train/notebooks/11_cleanup_mlflow.html#get-data",
    "href": "notebooks/02-train/notebooks/11_cleanup_mlflow.html#get-data",
    "title": "Cleanup Model Monitoring Outputs",
    "section": "Get Data",
    "text": "Get Data\n\nFetch Latest Version of Best Deployment Candidate Model\nGet best deployment candidate model from model registry\n\ndf_candidate_mlflow_models = modh.get_all_deployment_candidate_models()\n\n\n\nGet Filepaths to MLFlow File Artifacts\nGet the filepath to the MLFlow artifacts associated with the best deployment candidate model, as well as the filepath to the corresponding local file used when logging the artifact\n\nartifacts_dict = {\n    k: show_artifact_and_local_filepath_for_run_id(\n        df_candidate_mlflow_models, k, processed_data_dir\n    )\n    for k in artifact_file_prefixes\n}"
  },
  {
    "objectID": "notebooks/02-train/notebooks/11_cleanup_mlflow.html#cleanup",
    "href": "notebooks/02-train/notebooks/11_cleanup_mlflow.html#cleanup",
    "title": "Cleanup Model Monitoring Outputs",
    "section": "Cleanup",
    "text": "Cleanup\n\n(Non-MLFlow) Local Outputs\n\nfor k,v in artifacts_dict.items():\n    os.remove(v[1])\n    print(f\"Deleted: {os.path.basename(v[1]).split('__run')[0]}\")\n\n\n\n(Non-MLFlow) ML Development Experiment Outputs\n\nml_runs_glob = os.path.join(raw_data_dir, \"ml__run_*__expt_*.parquet*\")\nfor local_fpath in glob(ml_runs_glob):\n    os.remove(local_fpath)\n    print(f\"Deleted: {os.path.basename(local_fpath).split('__run')[0]}\")\n\n\n\nMLFlow Outputs\nDelete the MLFlow run-logging database file\n\nos.remove(mlruns_db_fpath)\nprint(f\"Removed MLFlow database at {os.path.basename(mlruns_db_fpath)}\")\n\nDelete the MLFlow artifact directory (mlruns), and all the artifact sub-directories contained in that directory\n\nshutil.rmtree(mlflow_artifact_fpath)\nprint(\n    \"Removed local MLFlow artifact logging directory at \"\n    f\"{os.path.basename(mlflow_artifact_fpath)}\"\n)"
  },
  {
    "objectID": "notebooks/02-train/notebooks/11_cleanup_mlflow.html#next-step",
    "href": "notebooks/02-train/notebooks/11_cleanup_mlflow.html#next-step",
    "title": "Cleanup Model Monitoring Outputs",
    "section": "Next Step",
    "text": "Next Step\nThe next step will clean up all project resources related to BigQuery."
  },
  {
    "objectID": "notebooks/03-explore/notebooks/08_explore_best_model.html",
    "href": "notebooks/03-explore/notebooks/08_explore_best_model.html",
    "title": "Explore Best Model Predictions",
    "section": "",
    "text": "As mentioned in the scope, understanding ML predictions of propensity can be helpful to characterize audience groups during the design of a marketing campaign. One example question could be answered by exploring the predictions of a ML model is what are the most and least important attributes of visitors predicted to have a high or low propensity. This can be one factor used to design a marketing strategy aimed at one or both groups of visitors.\nThis step explores the best ML model’s predictions of visitor propensity to make a purchase on a return visit using ML explainability packages (eg. SHAP). This exploration is done per audience group so the output of this step is a profile of each audience group based on the features that are most important to predicting the outcome for first-time visitors to the store in that group during the inference data period. This is performed as part of step 4. from a typical A/B Testing workflow.\nThis step can be run prospectively at the end of the inference period, just before the start of the campaign, when all the inference data (first-time visitors to the store) becomes available.\n\n\n\nModel exploration is supported for a single ML model. This is the best model found in an earlier step and registered in the MLFlow model registry. The model associated with a single experiment run (or run_id) is a sklearn Pipeline, consisting of data preprocessing and ML modeling steps. SHAP cannot work directly with a sklearn Pipelines to generate explanations. It can only accept the ML model (classifier, or last Pipeline step). So, we’ll need to retrieve the preliminary Pipeline steps (the data preprocessor) and the transformed data that is generated by the preprocessor. Then, we can make predictions on this transformed data using the last step (the ML classifier). Finally, SHAP can be used to explain predictions made by this classifier. These explanations are required for the first-time visitors to the store during the inference data period.\n\n\n\n\nSince extracting SHAP values can be time-consuming, they will be retrieved for a slice of the inference data and not for the entire inference dataset.\n\n\n\n\nFirst, the preprocessor steps of the Pipeline are used to get the transformed train and test data splits from the best model. A custom Python modeule src/model_explore_helpers.py has been developed to extract the transformed train and data. Next, the ML classifier step of the Pipeline is used to make and score predictions of the processed test data using this model.\nNext, the end-to-end Pipeline will be used to make and evaluate these predictions directly on the raw test data, without creating the intermediate transformed data splits.\nThen, the metrics using both approaches will be compared to ensure they are identical to each other. This is outlined below\n\nMake and evaluate predictions using the model (end-to-end sklearn Pipeline) associated with the specified run_id\nMake and evaluate predictions using the manual approach\nCompare predictions and evaluation metrics on the training and test data splits between the Pipeline-based and manual outputs\n\nIf the Pipeline and manual approaches are verified to give identical metrics for first-time visitors in the test data split during ML model development, then it is possible to reliably apply the same manual workflow to retrieve the transformed features during the inference period. So, next, this same manual transformation procedure is applied to transform the inference data. Recall that the inference data only covers the first visit during the inference period and so the outcome of the return visit for each first-time visitor is not known until after the entire inference (production) period (one month) has ended.\nFinally, a SHAP explainer will be used to explore a subset of the transformed inference data in order to get the feature importances for the audience group(s). The top N most important features for each audience group will be combined into a single DataFrame and exported to disk. This will represent the profile of the audience group(s) in terms of most important features."
  },
  {
    "objectID": "notebooks/03-explore/notebooks/08_explore_best_model.html#about",
    "href": "notebooks/03-explore/notebooks/08_explore_best_model.html#about",
    "title": "Explore Best Model Predictions",
    "section": "",
    "text": "As mentioned in the scope, understanding ML predictions of propensity can be helpful to characterize audience groups during the design of a marketing campaign. One example question could be answered by exploring the predictions of a ML model is what are the most and least important attributes of visitors predicted to have a high or low propensity. This can be one factor used to design a marketing strategy aimed at one or both groups of visitors.\nThis step explores the best ML model’s predictions of visitor propensity to make a purchase on a return visit using ML explainability packages (eg. SHAP). This exploration is done per audience group so the output of this step is a profile of each audience group based on the features that are most important to predicting the outcome for first-time visitors to the store in that group during the inference data period. This is performed as part of step 4. from a typical A/B Testing workflow.\nThis step can be run prospectively at the end of the inference period, just before the start of the campaign, when all the inference data (first-time visitors to the store) becomes available.\n\n\n\nModel exploration is supported for a single ML model. This is the best model found in an earlier step and registered in the MLFlow model registry. The model associated with a single experiment run (or run_id) is a sklearn Pipeline, consisting of data preprocessing and ML modeling steps. SHAP cannot work directly with a sklearn Pipelines to generate explanations. It can only accept the ML model (classifier, or last Pipeline step). So, we’ll need to retrieve the preliminary Pipeline steps (the data preprocessor) and the transformed data that is generated by the preprocessor. Then, we can make predictions on this transformed data using the last step (the ML classifier). Finally, SHAP can be used to explain predictions made by this classifier. These explanations are required for the first-time visitors to the store during the inference data period.\n\n\n\n\nSince extracting SHAP values can be time-consuming, they will be retrieved for a slice of the inference data and not for the entire inference dataset.\n\n\n\n\nFirst, the preprocessor steps of the Pipeline are used to get the transformed train and test data splits from the best model. A custom Python modeule src/model_explore_helpers.py has been developed to extract the transformed train and data. Next, the ML classifier step of the Pipeline is used to make and score predictions of the processed test data using this model.\nNext, the end-to-end Pipeline will be used to make and evaluate these predictions directly on the raw test data, without creating the intermediate transformed data splits.\nThen, the metrics using both approaches will be compared to ensure they are identical to each other. This is outlined below\n\nMake and evaluate predictions using the model (end-to-end sklearn Pipeline) associated with the specified run_id\nMake and evaluate predictions using the manual approach\nCompare predictions and evaluation metrics on the training and test data splits between the Pipeline-based and manual outputs\n\nIf the Pipeline and manual approaches are verified to give identical metrics for first-time visitors in the test data split during ML model development, then it is possible to reliably apply the same manual workflow to retrieve the transformed features during the inference period. So, next, this same manual transformation procedure is applied to transform the inference data. Recall that the inference data only covers the first visit during the inference period and so the outcome of the return visit for each first-time visitor is not known until after the entire inference (production) period (one month) has ended.\nFinally, a SHAP explainer will be used to explore a subset of the transformed inference data in order to get the feature importances for the audience group(s). The top N most important features for each audience group will be combined into a single DataFrame and exported to disk. This will represent the profile of the audience group(s) in terms of most important features."
  },
  {
    "objectID": "notebooks/03-explore/notebooks/08_explore_best_model.html#user-inputs",
    "href": "notebooks/03-explore/notebooks/08_explore_best_model.html#user-inputs",
    "title": "Explore Best Model Predictions",
    "section": "User Inputs",
    "text": "User Inputs\nDefine the following\n\nbest MLFlow run ID\nname of column containing label (outcome)\nlist of categorical features\nlist of numerical features, including categorical features present in the raw data as integers\nnumber of SHAP observations from the transformed inferece data that are to be explained\nnumber of features from the transformed inference data for which SHAP feature importances are required\naudience_groups_strategies\n\ndictionary of desired audience groups into which the first-time visitors propensities will be placed\n\nkey specifies the number of desired audience propensity groups (low, medium and high)\nvalue specifies names of the desired audience propensity groups\n\n\ntype of audience strategy (single- or multi- group) from which to create cohorts\n\n\n# 1. \nbest_run_id = \"742cf152d5094be0b686edf910c7d398\"\n\n# 2. label column\nlabel = \"made_purchase_on_future_visit\"\n\n# 3. categorical columns\ncategorical_features = [\n    'bounces',\n    'source',\n    'medium',\n    'channelGrouping',\n    'last_action',\n    'browser',\n    'os',\n    'deviceCategory',\n]\n\n# 4. numerical columns\nnumerical_features = [\n    'hits',\n    'pageviews',\n    'promos_displayed',\n    'product_views',\n    'product_clicks',\n    'time_on_site',\n]\n\n# 5. number of SHAP observations to be explained\nnum_shap_obs = 500\n\n# 6. number of top-performing SHAP features to be kept\nnum_features_shap = 10\n\n# 7. mapping dictionaries\naudience_groups_strategies = {\n    1: [\"High\", \"Medium\", \"Low\"], 2: [\"High\", \"High-Medium\", \"High-Medium-Low\"],\n}\n\n# 8. type of audience strategy to use when creating groups\naudience_strategy = 1"
  },
  {
    "objectID": "notebooks/03-explore/notebooks/08_explore_best_model.html#get-data",
    "href": "notebooks/03-explore/notebooks/08_explore_best_model.html#get-data",
    "title": "Explore Best Model Predictions",
    "section": "Get Data",
    "text": "Get Data\n\nFetch Latest Version of Best Deployment Candidate Model\nLoad best deployment candidate model object\n\npipe = joblib.load(model_filepath)\n\n\n\nGet Data Used in Development of Best Deployment Candidate Model\nFor the data used during development of the best deployment candidate model, get the\n\ntrain and test data splits\noverall (combined) data\n\n\ndf_train_val_eval_best_run, df_test_best_run = [\n    pd.read_parquet(\n        processed_data_filepath,\n        columns=list(dtypes_dict)+['split_type'],\n        filters=[('split_type', '=', split_type)]\n    )\n    for split_type in ['train_val', 'test']\n]\ndf_all = (\n    pd.concat(\n        [df_train_val_eval_best_run, df_test_best_run],\n        ignore_index=True,\n    )\n)\n\nGet features and label from\n\ntrain and test split\noverall data\n\n\nX_train, y_train = [\n    df_train_val_eval_best_run.drop(columns=[label, 'split_type']),\n    df_train_val_eval_best_run[label]\n]\nX_test, y_test = [\n    df_test_best_run.drop(columns=[label, 'split_type']), df_test_best_run[label]\n]\nX, y = [df_all.drop(columns=[label, 'split_type']), df_all[label]]\n\n\n\nGet Inference Data with Audience Assignments\nLoad inference data with audience and cohort assignments\n\ndf_infer_with_audience = (\n    pd.read_parquet(\n        audience_filepath,\n        columns=[\n            c\n            for c in list(dtypes_dict)\n            if c not in ['made_purchase_on_future_visit']\n        ] + ['maudience', 'audience_strategy'],\n        filters=[('audience_strategy', '=', audience_strategy)]\n    )\n)\n\nGet features from inference data\n\nX_infer_with_audience = (\n    df_infer_with_audience.drop(columns=['maudience', 'audience_strategy'])\n)"
  },
  {
    "objectID": "notebooks/03-explore/notebooks/08_explore_best_model.html#ml-evaluation-from-pipeline",
    "href": "notebooks/03-explore/notebooks/08_explore_best_model.html#ml-evaluation-from-pipeline",
    "title": "Explore Best Model Predictions",
    "section": "ML Evaluation From Pipeline",
    "text": "ML Evaluation From Pipeline\n\n\n\n\n\n\n\nsplit\nmetric\ntrain_val\ntest\n\n\n\n\n0\naccuracy\n0.919483\n0.940538\n\n\n1\nbalanced_accuracy\n0.501968\n0.499218\n\n\n2\nprecision\n0.047763\n0.022135\n\n\n3\nrecall\n0.501968\n0.499218\n\n\n4\nroc_auc\n0.501968\n0.499218\n\n\n5\nf1\n0.502007\n0.498453\n\n\n6\nfbeta05\n0.502060\n0.498946\n\n\n7\nfbeta2\n0.501977\n0.498594\n\n\n8\npr_auc\n0.043754\n0.023227\n\n\n9\navg_precision\n0.043898\n0.023027"
  },
  {
    "objectID": "notebooks/03-explore/notebooks/08_explore_best_model.html#ml-evaluation-from-pipeline-using-transformed-data",
    "href": "notebooks/03-explore/notebooks/08_explore_best_model.html#ml-evaluation-from-pipeline-using-transformed-data",
    "title": "Explore Best Model Predictions",
    "section": "ML Evaluation From Pipeline Using Transformed Data",
    "text": "ML Evaluation From Pipeline Using Transformed Data\n\nGet Transformed Data\nIn order to get the transformed data used by the end-to-end Pipeline, the following two-step procedure will be used\n\nExtract untrained pipeline consisting of the following steps\n\nunder-sampling\n\nunder-sampling\npre-processing\n\nover-sampling\n\npre-processing\nover-sampling\n\n\nUse untrained pipeline to get transformed training and validation data as follows\n\nunder-sampling\n\nunder-sample training data\ntrain pre-processor using under-sampled training data\npre-process all\n\ntraining data\nvalidation data\n\n\nover-sampling\n\ntrain pre-processor using all training data\npre-process all\n\ntraining data\nvalidatation data\n\nover-sample pre-processed training data\n\n\n\n\nif 'Under' in type(pipe.named_steps['resampler']).__name__:\n    resampling_approach = 'us'\n    pipe_resample_trans = meh.get_preprocessor_pipeline(pipe, 'us')\n    X_train_trans, X_test_trans = meh.get_preprocessed_resampled_data(\n        'us',\n        X_train,\n        y_train,\n        X_test,\n        categorical_features,\n        pipe_resample_trans=pipe_resample_trans,\n    )\nelse:\n    resampling_approach = 'os'\n    pipe_trans, pipe_resample = meh.get_preprocessor_pipeline(pipe, 'os')\n    (\n        X_train_trans,\n        X_test_trans,\n        X_train_trans_rs,\n        y_train_trans_rs,\n    ) = meh.get_preprocessed_resampled_data(\n        'os',\n        X_train,\n        y_train,\n        X_test,\n        categorical_features,\n        pipe_trans=pipe_trans,\n        pipe_resample=pipe_resample,\n    )\n\n\n\nML Evaluation Using Transformed Data\n\n\n\n\n\n\n\nsplit\nmetric\ntrain_val\ntest\n\n\n\n\n0\naccuracy\n0.919483\n0.940538\n\n\n1\nbalanced_accuracy\n0.501968\n0.499218\n\n\n2\nprecision\n0.047763\n0.022135\n\n\n3\nrecall\n0.501968\n0.499218\n\n\n4\nroc_auc\n0.501968\n0.499218\n\n\n5\nf1\n0.502007\n0.498453\n\n\n6\nfbeta05\n0.502060\n0.498946\n\n\n7\nfbeta2\n0.501977\n0.498594\n\n\n8\npr_auc\n0.043754\n0.023227\n\n\n9\navg_precision\n0.043898\n0.023027"
  },
  {
    "objectID": "notebooks/03-explore/notebooks/08_explore_best_model.html#ml-evaluation-comparison",
    "href": "notebooks/03-explore/notebooks/08_explore_best_model.html#ml-evaluation-comparison",
    "title": "Explore Best Model Predictions",
    "section": "ML Evaluation Comparison",
    "text": "ML Evaluation Comparison\nVerify that the metrics obtained using the end-to-end Pipeline and the manually transformed data are identical.\n\nTransformed Data\nVerify agreement between predictions of training data split\n\nassert y_train_pred.equals(y_train_pred_m)\nassert y_train_pred_proba.equals(y_train_pred_proba_m)\n\nVerify agreement between predictions of test data split\n\nassert y_test_pred.reset_index(drop=True).equals(y_test_pred_m)\nassert y_test_pred_proba.reset_index(drop=True).equals(y_test_pred_proba_m)\n\n\n\nEvaluation Metrics\nVerify agreement between ML model evaluation metrics using the test data split\n\nassert df_metrics.equals(df_metrics_m)\n\nThis section has verified that the manual approach to transforming features and using them to make and evaluate predictions of test data observations gives the same classification metrics as using the self-contained end-to-end Pipeline to make and evaluate predictions."
  },
  {
    "objectID": "notebooks/03-explore/notebooks/08_explore_best_model.html#explore-ml-model-using-shap",
    "href": "notebooks/03-explore/notebooks/08_explore_best_model.html#explore-ml-model-using-shap",
    "title": "Explore Best Model Predictions",
    "section": "Explore ML Model Using SHAP",
    "text": "Explore ML Model Using SHAP\nTransform features in the inference data using the same feature transformation approach as that developed above using the test data split during ML development. Use SHAP to generate explanations for a subset of these transformed observations.\n\nGet Transformed Inferece Data\nGet the transformed features for the\n\noverall data used during ML development (combination of train+val and test)\ninference data\n\n\nif 'Under' in type(pipe.named_steps['resampler']).__name__:\n    resampling_approach = 'us'\n    pipe_resample_trans = meh.get_preprocessor_pipeline(pipe, 'us')\n    X_trans, X_infer_trans = meh.get_preprocessed_resampled_data(\n        'us',\n        X,\n        y,\n        X_infer_with_audience,\n        categorical_features,\n        pipe_resample_trans=pipe_resample_trans,\n    )\nelse:\n    resampling_approach = 'os'\n    pipe_trans, pipe_resample = meh.get_preprocessor_pipeline(pipe, 'os')\n    (\n        X_trans,\n        X_infer_trans,\n        X_trans_rs,\n        y_trans_rs,\n    ) = meh.get_preprocessed_resampled_data(\n        'os',\n        X,\n        y,\n        X_infer_with_audience,\n        categorical_features,\n        pipe_trans=pipe_trans,\n        pipe_resample=pipe_resample,\n    )\n\nAppend the audience group name and strategy to the transformed inference data\n\ndf_infer_trans = (\n    X_infer_trans.assign(maudience=df_infer_with_audience['maudience'])\n    .assign(audience_strategy=df_infer_with_audience['audience_strategy'])\n)\n\n\n\nExplore Model\nFor each audience group, profile the audience group(s) using SHAP feature importances as follows\n\nget a random slice of the inference data within each audience group\nget SHAP values for the selected slice of inference data\nappend metadata about the selected slice of inference data\n\nThe above process is repeated for all audience groups. This is shown below\n\ndfs_shap_values = []\nfor maudience in audience_groups_strategies[audience_strategy]:\n    # 1. Get SHAP training data for audience group\n    df_infer_shap_train = df_infer_trans.sample(n=num_shap_obs)\n    print(\n        f\"Randomly sliced {num_shap_obs:,} observations \"\n        f\"from {maudience} audience group in inference data\"\n    )\n\n    # 2. Get SHAP values\n    start_time = datetime.now(pytz.timezone(\"US/Eastern\"))\n    start_time_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n    print(f\"Query execution start time = {start_time_str[:-3]}...\", end=\"\")\n    # shap_means = np.abs(shap_values).mean(axis=0)\n    shap_means = np.random.rand(num_features_shap, 1)\n    end_time = datetime.now(pytz.timezone(\"US/Eastern\"))\n    end_time_str = end_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n    duration = end_time - start_time\n    duration = duration.seconds + (duration.microseconds / 1_000_000)\n    print(f\"done at {end_time_str[:-3]} ({duration:.3f} seconds).\")\n    print(\n        f\"Retrieved SHAP values using {num_shap_obs:,} observations \"\n        f\"from {maudience} group in inference data\"\n    )\n\n    # 3. Log SHAP values and metadata\n    df_shap_values = (\n        pd.DataFrame.from_dict(\n            {\n                'feature': list(X_trans_rs)[:num_features_shap],\n                'mean_shap_value': shap_means.squeeze(),\n            },\n            orient='index'\n        ).transpose()\n        .sort_values(by='mean_shap_value', ascending=False)\n        .reset_index(drop=True)\n        .assign(infer_month=month_name[X_infer_with_audience['month'].squeeze()[0]])\n        .assign(maudience=maudience)\n        .assign(audience_strategy=audience_strategy)\n        .assign(num_shap_observations=num_shap_obs)\n    )\n    dfs_shap_values.append(df_shap_values)\n\nConvert the feature importances and metadata into a single DataFrame\n\n\n\n\n\n\n\n\n\nfeature\nmean_shap_value\ninfer_month\nmaudience\naudience_strategy\nnum_shap_observations\n\n\n\n\n0\nsource__Check out\n0.825161\nMarch\nHigh\n1\n500\n\n\n1\npromos_clicked\n0.737896\nMarch\nHigh\n1\n500\n\n\n2\ntime_on_site\n0.615848\nMarch\nHigh\n1\n500\n\n\n3\nsource__Remove product(s) from cart\n0.421039\nMarch\nHigh\n1\n500\n\n\n4\nbounces__1\n0.3781\nMarch\nHigh\n1\n500\n\n\n5\npromos_displayed\n0.24851\nMarch\nHigh\n1\n500\n\n\n6\nhits\n0.223202\nMarch\nHigh\n1\n500\n\n\n7\nsource__Product detail views\n0.134502\nMarch\nHigh\n1\n500\n\n\n8\nsource__Click through of product lists\n0.116958\nMarch\nHigh\n1\n500\n\n\n9\nsource__Completed purchase\n0.021289\nMarch\nHigh\n1\n500\n\n\n10\ntime_on_site\n0.885989\nMarch\nMedium\n1\n500\n\n\n11\nsource__Click through of product lists\n0.839497\nMarch\nMedium\n1\n500\n\n\n12\nhits\n0.827934\nMarch\nMedium\n1\n500\n\n\n13\nsource__Product detail views\n0.747796\nMarch\nMedium\n1\n500\n\n\n14\npromos_clicked\n0.546836\nMarch\nMedium\n1\n500\n\n\n15\nbounces__1\n0.545509\nMarch\nMedium\n1\n500\n\n\n16\nsource__Completed purchase\n0.458597\nMarch\nMedium\n1\n500\n\n\n17\nsource__Check out\n0.40554\nMarch\nMedium\n1\n500\n\n\n18\nsource__Remove product(s) from cart\n0.157091\nMarch\nMedium\n1\n500\n\n\n19\npromos_displayed\n0.131683\nMarch\nMedium\n1\n500\n\n\n20\nhits\n0.838479\nMarch\nLow\n1\n500\n\n\n21\nbounces__1\n0.717931\nMarch\nLow\n1\n500\n\n\n22\nsource__Product detail views\n0.685082\nMarch\nLow\n1\n500\n\n\n23\npromos_clicked\n0.628822\nMarch\nLow\n1\n500\n\n\n24\nsource__Remove product(s) from cart\n0.241925\nMarch\nLow\n1\n500\n\n\n25\npromos_displayed\n0.143101\nMarch\nLow\n1\n500\n\n\n26\ntime_on_site\n0.114057\nMarch\nLow\n1\n500\n\n\n27\nsource__Completed purchase\n0.077123\nMarch\nLow\n1\n500\n\n\n28\nsource__Check out\n0.070355\nMarch\nLow\n1\n500\n\n\n29\nsource__Click through of product lists\n0.002271\nMarch\nLow\n1\n500"
  },
  {
    "objectID": "notebooks/03-explore/notebooks/08_explore_best_model.html#export-to-disk",
    "href": "notebooks/03-explore/notebooks/08_explore_best_model.html#export-to-disk",
    "title": "Explore Best Model Predictions",
    "section": "Export To Disk",
    "text": "Export To Disk\nExport DataFrame with audience profiles, based on feature importances, to disk\n\ndf_audience_explored.to_parquet(\n    os.path.join(\n        processed_data_dir,\n        f\"audience_profiles_feature_importances__run_\"\n        f\"{best_run_id}__\"\n        f\"infer_month_{month_name[X_infer_with_audience['month'].squeeze()[0]]}__\"\n        f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet.gzip\",\n    ),\n    index=False,\n    engine='pyarrow',\n    compression='gzip',\n)"
  },
  {
    "objectID": "notebooks/03-explore/notebooks/08_explore_best_model.html#next-step",
    "href": "notebooks/03-explore/notebooks/08_explore_best_model.html#next-step",
    "title": "Explore Best Model Predictions",
    "section": "Next Step",
    "text": "Next Step\nThe next step will assess the performance of the marketing campaign, after the campaign has completed."
  },
  {
    "objectID": "notebooks/05-cleanup/notebooks/12_cleanup_bigquery.html",
    "href": "notebooks/05-cleanup/notebooks/12_cleanup_bigquery.html",
    "title": "Cleanup BigQuery Resources",
    "section": "",
    "text": "This step deletes all Google Bigquery resources that were created during this project. This includes BigQuery datasets and tables."
  },
  {
    "objectID": "notebooks/05-cleanup/notebooks/12_cleanup_bigquery.html#about",
    "href": "notebooks/05-cleanup/notebooks/12_cleanup_bigquery.html#about",
    "title": "Cleanup BigQuery Resources",
    "section": "",
    "text": "This step deletes all Google Bigquery resources that were created during this project. This includes BigQuery datasets and tables."
  },
  {
    "objectID": "notebooks/05-cleanup/notebooks/12_cleanup_bigquery.html#user-inputs",
    "href": "notebooks/05-cleanup/notebooks/12_cleanup_bigquery.html#user-inputs",
    "title": "Cleanup BigQuery Resources",
    "section": "User Inputs",
    "text": "User Inputs\nDefine the following GCP BigQuery resources\n\ndataset id\ntable id\n\n\n# 1. GCP resources\ngbq_dataset_id = 'mydemo2asdf'\ngbq_table_id = 'ecwa'"
  },
  {
    "objectID": "notebooks/05-cleanup/notebooks/12_cleanup_bigquery.html#cleanup-bigquery",
    "href": "notebooks/05-cleanup/notebooks/12_cleanup_bigquery.html#cleanup-bigquery",
    "title": "Cleanup BigQuery Resources",
    "section": "Cleanup BigQuery",
    "text": "Cleanup BigQuery\n\nDatasets\nDelete BigQuery dataset\n\nclient.delete_dataset(\n    gbq_dataset_id, delete_contents=False, not_found_ok=True\n)\nprint(f\"Deleted dataset with ID {gbq_dataset_id}\")\n\n\n\nTables\nDelete BigQuery table\n\nclient.delete_table(gbq_table_fully_resolved)\nprint(f\"Deleted table {gbq_table_fully_resolved}\")"
  }
]