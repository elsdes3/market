[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning for Propensity Modeling",
    "section": "",
    "text": "E-Commerce is important since it allows a business to reach customers across a larger footprint than a group of physical (brick-and-mortar) stores. Visitors to an e-commerce store can make a purchase at an time, in any location and in their choice of currency. The ability to attract such a diverse customer base is the main value of e-commerce to a business. While website traffic is a highly-tracked metric by e-commerce businesses, their hard work and efforts to attract visitors to their site should not go to waste. It is customers that every e-commerce site owner needs to sustain their business."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Machine Learning for Propensity Modeling",
    "section": "",
    "text": "E-Commerce is important since it allows a business to reach customers across a larger footprint than a group of physical (brick-and-mortar) stores. Visitors to an e-commerce store can make a purchase at an time, in any location and in their choice of currency. The ability to attract such a diverse customer base is the main value of e-commerce to a business. While website traffic is a highly-tracked metric by e-commerce businesses, their hard work and efforts to attract visitors to their site should not go to waste. It is customers that every e-commerce site owner needs to sustain their business."
  },
  {
    "objectID": "index.html#problem-with-visitors-to-an-e-commerce-store",
    "href": "index.html#problem-with-visitors-to-an-e-commerce-store",
    "title": "Machine Learning for Propensity Modeling",
    "section": "Problem with Visitors to an E-Commerce Store",
    "text": "Problem with Visitors to an E-Commerce Store\nThe majority of visitors to an e-commerce site leave without performing a transaction (making a purchase) on their first visit. In the case of the the Google merchandise store’s site, the fraction of such visitors leaving is greater than 95%. Industry research shows that the majority of purchases by a visitor to such a site don’t occur on the visitor’s first visit. If they do purchase, then more often than not they will return later and make a purchase."
  },
  {
    "objectID": "index.html#why-market-to-return-users",
    "href": "index.html#why-market-to-return-users",
    "title": "Machine Learning for Propensity Modeling",
    "section": "Why Market to Return Users?",
    "text": "Why Market to Return Users?\nBeing able to identify such high-value visitors ahead of time can be of tremendous help to a marketing team to develop campaigns to grow the number of first-time visitors who make a purchase (converters) or a return purchase (repeat customers).\nThe marketing team can design and deploy a campaign after visitors’ first visit to improve their chances of making a purchase on a future visit."
  },
  {
    "objectID": "index.html#why-machine-learning",
    "href": "index.html#why-machine-learning",
    "title": "Machine Learning for Propensity Modeling",
    "section": "Why Machine Learning?",
    "text": "Why Machine Learning?\nThrough the use of machine learning (ML), we can scale this approach to capture all first-time visitors to the store and also improve their likelihood (or propensity) to make a future purchase while they search on a competitor’s site for the same or a similar product. Doing so is one way to help grow the base of converters and repeat customers."
  },
  {
    "objectID": "index.html#who-is-the-business-client",
    "href": "index.html#who-is-the-business-client",
    "title": "Machine Learning for Propensity Modeling",
    "section": "Who is the Business Client?",
    "text": "Who is the Business Client?\nThis project would be directly useful to Robertson Marketing, who is responsible for management of the Google merchandise store."
  },
  {
    "objectID": "index.html#what-is-this-project-about",
    "href": "index.html#what-is-this-project-about",
    "title": "Machine Learning for Propensity Modeling",
    "section": "What is This Project About?",
    "text": "What is This Project About?\nIn this project, ML predictions are used to select a marketing audience with a low, medium and high propensity to make a purchase on a return visit. Within each group, we also develop and briefly profile test (or treatment) and control cohorts in order to help facilitate deployment of a marketing campaign."
  },
  {
    "objectID": "references/Scope.html",
    "href": "references/Scope.html",
    "title": "Project Scope",
    "section": "",
    "text": "The Google Merchandise store is an e-commerce store that sells Google-branded products. Less than 5% of visitors make a purchase from the Google merchandise store on the Google Marketplace. So, a large number of visitors are not making a purchase. They are either just visiting the store once (their first and only visit) and leaving, or visiting multiple times and but not making a purchase on any of those visits. It goes without saying that customers, and not visitors alone, ensure the sustainability of an e-commerce business.\nConsiderable effort has been made by the web design team to build the store’s website to attract site traffic and make a good impression on first-time visitors to the store. This effort should not go to waste. However, approximately 90% of purchases do not happen during an initial visit to an e-commerce website. Furthermore, repeat customers spend 33% more with a brand than new customers do. Only approximately 20% of existing customers account for approximately 80% of future profits. Getting visitors to return to a site is important, but is possibly of equal or greater importance to an e-commerce business to have these visitors make a purchase on a return visit. This underscores the importance of getting visitors to make a purchase on a subsequent visit to the store.\nBut, it is not just sufficient to ensure return visits occur. This is of no use to a business since they can’t grow customer revenue by relying visitors to return and make purchaes on their own volition. The visitor should be motivated between visits to return and make a purchase on one or more future visits, rather than just returning and browsing through the store as they did during a previous visit. Between August 1, 2016 and August 1, 2017, a little less than 2% of visitors made a purchase on a return visit to the Google merchandise store. One of the main reasons for visitors browsing an e-commerce store, rather than making a purchase, is because they are comparson-shopping across multiple such websites looking for the lowest price for the same or a very similar product.\nIn summary, it will help the Google Merchandise store grow their customer base and increase revenue if some of these first-time visitors who have\n\nnot made a purchase during their first visit\nmade a purchase during their first visit\n\nto the store will make a purchase during a subsequent visit. In other words, it is desirable that first-time visitors to the store become customers or repeat customers.\n\n\n\nIf the business can find ways to reach out to (interact with) visitors to the store after their first visit, and provide promotions, shipping offers, etc., then this could be one way to motivate these visitors to make a purchase on a return visit to the store. If the Google merchandise store management company can reach out to these first-time visitors based on the characteristics of their first visit to the store and get them to make a purchase on a return visit then this can not only grow the business’ customer base but also reduce the loss of customer revenue to a competitor.\nAlphabet is the parent company of Google, but Robertson Marketing is the company that manages the Google Merchandise store. The management company (Robertson Marketing) is impacted by the problem of low conversions or repeat customers among the pool of first-time visitors to the merchandise store and they would be interested in ways to turn such first-time visitors into one of the following\n\nfuture customers (converting visitors into customers, or conversions)\nrepeat-customers (customer retention)\n\n\n\n\nThe business (store management company, Robertson Marketing) has tasked its marketing team with growing\n\nnew customers (conversion)\nrepeat customers (getting customers to become repeat customers)\n\nfrom the pool of first-time visitors to the store.\nFor obvious reasons, not all first-time visitors to the merchandise store are alike and reaching out to visitors is a costly process. With this mind, the marketing team would like to design an appropriate marketing campaign to help achive the business’ objectives. With a strong preference to spend marketing funds (budget) wisely, the marketing team wants to interact with such first-time visitors through focused and relevant recommendations, reminders and other types of marketing promotions after their first visit to the store.\nA logical approach to developing targeted promotions to grow customers or repeat customers is to offer a promotion based on a visitor’s likelihood of making a purchase during a future visit to the merchandise store. Knowing this likelihood is useful in knowing which visitors the marketing team should be focused on and, subsequently, how much funding can be allocated to communicating with those visitors. Accordingly, the business can determine the type of promotion that should be offered. For example, a minimal discount could be offered to visitors with a high likelihood of making a future purchase. Similarly, more loyalty points, coupon giveaways or free shipping could be offered to visitors who are deemed less likely to make a future purchase from the store. These promotions can be offered after a visitor’s first visit to the store with the aim of persuading them to make a purchase during a future visit.\n\n\n\nWithout knowing visitors’ likelihood of making a purchase on a return visit, it is not possible to segment visitors into audience groups (eg. visitors with a high, medium or low likelihood of making a purchase on a return visit) after their first visit. If these groups and cohorts were known, the marketing team could test how they respond to marketing strategies. Naive random guesses at visitor likelihood groupings are costly and unlikely to get buy-in for funding requests from business management. Furthermore, a test and control cohort is needed within each group in order to quantitatively determine how each group responds to the chosen marketing campaign (i.e. these cohorts are needed to evaluate the performance of the campaign).\nWe will assume that the marketing team has not yet designed any approaches to address this problem. With this in mind, both the size of the audience groups and the size of these cohorts are currently completely unknown."
  },
  {
    "objectID": "references/Scope.html#understanding-the-problem",
    "href": "references/Scope.html#understanding-the-problem",
    "title": "Project Scope",
    "section": "",
    "text": "The Google Merchandise store is an e-commerce store that sells Google-branded products. Less than 5% of visitors make a purchase from the Google merchandise store on the Google Marketplace. So, a large number of visitors are not making a purchase. They are either just visiting the store once (their first and only visit) and leaving, or visiting multiple times and but not making a purchase on any of those visits. It goes without saying that customers, and not visitors alone, ensure the sustainability of an e-commerce business.\nConsiderable effort has been made by the web design team to build the store’s website to attract site traffic and make a good impression on first-time visitors to the store. This effort should not go to waste. However, approximately 90% of purchases do not happen during an initial visit to an e-commerce website. Furthermore, repeat customers spend 33% more with a brand than new customers do. Only approximately 20% of existing customers account for approximately 80% of future profits. Getting visitors to return to a site is important, but is possibly of equal or greater importance to an e-commerce business to have these visitors make a purchase on a return visit. This underscores the importance of getting visitors to make a purchase on a subsequent visit to the store.\nBut, it is not just sufficient to ensure return visits occur. This is of no use to a business since they can’t grow customer revenue by relying visitors to return and make purchaes on their own volition. The visitor should be motivated between visits to return and make a purchase on one or more future visits, rather than just returning and browsing through the store as they did during a previous visit. Between August 1, 2016 and August 1, 2017, a little less than 2% of visitors made a purchase on a return visit to the Google merchandise store. One of the main reasons for visitors browsing an e-commerce store, rather than making a purchase, is because they are comparson-shopping across multiple such websites looking for the lowest price for the same or a very similar product.\nIn summary, it will help the Google Merchandise store grow their customer base and increase revenue if some of these first-time visitors who have\n\nnot made a purchase during their first visit\nmade a purchase during their first visit\n\nto the store will make a purchase during a subsequent visit. In other words, it is desirable that first-time visitors to the store become customers or repeat customers.\n\n\n\nIf the business can find ways to reach out to (interact with) visitors to the store after their first visit, and provide promotions, shipping offers, etc., then this could be one way to motivate these visitors to make a purchase on a return visit to the store. If the Google merchandise store management company can reach out to these first-time visitors based on the characteristics of their first visit to the store and get them to make a purchase on a return visit then this can not only grow the business’ customer base but also reduce the loss of customer revenue to a competitor.\nAlphabet is the parent company of Google, but Robertson Marketing is the company that manages the Google Merchandise store. The management company (Robertson Marketing) is impacted by the problem of low conversions or repeat customers among the pool of first-time visitors to the merchandise store and they would be interested in ways to turn such first-time visitors into one of the following\n\nfuture customers (converting visitors into customers, or conversions)\nrepeat-customers (customer retention)\n\n\n\n\nThe business (store management company, Robertson Marketing) has tasked its marketing team with growing\n\nnew customers (conversion)\nrepeat customers (getting customers to become repeat customers)\n\nfrom the pool of first-time visitors to the store.\nFor obvious reasons, not all first-time visitors to the merchandise store are alike and reaching out to visitors is a costly process. With this mind, the marketing team would like to design an appropriate marketing campaign to help achive the business’ objectives. With a strong preference to spend marketing funds (budget) wisely, the marketing team wants to interact with such first-time visitors through focused and relevant recommendations, reminders and other types of marketing promotions after their first visit to the store.\nA logical approach to developing targeted promotions to grow customers or repeat customers is to offer a promotion based on a visitor’s likelihood of making a purchase during a future visit to the merchandise store. Knowing this likelihood is useful in knowing which visitors the marketing team should be focused on and, subsequently, how much funding can be allocated to communicating with those visitors. Accordingly, the business can determine the type of promotion that should be offered. For example, a minimal discount could be offered to visitors with a high likelihood of making a future purchase. Similarly, more loyalty points, coupon giveaways or free shipping could be offered to visitors who are deemed less likely to make a future purchase from the store. These promotions can be offered after a visitor’s first visit to the store with the aim of persuading them to make a purchase during a future visit.\n\n\n\nWithout knowing visitors’ likelihood of making a purchase on a return visit, it is not possible to segment visitors into audience groups (eg. visitors with a high, medium or low likelihood of making a purchase on a return visit) after their first visit. If these groups and cohorts were known, the marketing team could test how they respond to marketing strategies. Naive random guesses at visitor likelihood groupings are costly and unlikely to get buy-in for funding requests from business management. Furthermore, a test and control cohort is needed within each group in order to quantitatively determine how each group responds to the chosen marketing campaign (i.e. these cohorts are needed to evaluate the performance of the campaign).\nWe will assume that the marketing team has not yet designed any approaches to address this problem. With this in mind, both the size of the audience groups and the size of these cohorts are currently completely unknown."
  },
  {
    "objectID": "references/Scope.html#project-client-and-definition-of-objective",
    "href": "references/Scope.html#project-client-and-definition-of-objective",
    "title": "Project Scope",
    "section": "Project Client and Definition of Objective",
    "text": "Project Client and Definition of Objective\n\nBusiness Client\nThe client for this project is a marketing team responsible for managing marketing campaigns related to the Google merchandise store.\n\n\nProject Goal\nThis project exists to help the marketing team (client) interact with first-time visitors to the merchandise store, with the hopes of increasing the likelihood that these visitors will make a purchase (convert) or repeat-purchase during a future visit to the store. If this can be done, then it will help the team address the business’ objectives of growing new and repeat customers as mentioned above.\nThe objective of this project is to increase the number of first-time visitors to the merchandise store who are converted into new or repeat customers."
  },
  {
    "objectID": "references/Scope.html#actions-that-need-to-be-taken",
    "href": "references/Scope.html#actions-that-need-to-be-taken",
    "title": "Project Scope",
    "section": "Actions that Need to be Taken",
    "text": "Actions that Need to be Taken\nThis project will facilitate develpoment of a proactive and targeted marketing strategy (eg. promotions) to grow new and repeat customers."
  },
  {
    "objectID": "references/Scope.html#analysis",
    "href": "references/Scope.html#analysis",
    "title": "Project Scope",
    "section": "Analysis",
    "text": "Analysis\n\nType of Analysis\nWe need to answer the important question: Which visitors should we prioritize through proactive marketing promotions. In orther words, we want to identify the visitors with a low, medium and high likelihood of making a purchase during a future (or return) visit to the store.\nSince we want to intervene before a visitor’s next visit to the store, we would predict the likelihood of every first-time visitor making a purchase during a subsequent visit. These predictions will used to create audience groups based on the likelihood of making a future purchase and prioritize and focus the marketing strategy per group.\nThe analysis to be performed here is a prediction task. We need to predict the likelihood (or propensity) of a purchase during a future visit.\n\n\nFormat of Data\nThe analysis will be performed using machine learning (ML). A ML model will be trained using attributes (features) of the visitors’ first visit and it will predict visitors’ propensity to make a purchase on a return visit to the store. The best-performing trained ML model will be the one that can make this prediction with the highest accuracy or some other evaluation metric (this will be discussed later in the Evaluation Metric sub-section below). This application of ML is called propensity modeling. The outcome to be predicted is binary (there are only two possible outcomes)\n\nthe visitor will make a purchase on a return visit\nthe visitor will not make a purchase on a return visit\n\nIn a ML context, this is a supervised learning problem. Attributes about the first visit made by visitors to the store site are retrieved from Google Analytics (GA) tracking data accumulated for visitors to the merchandise store. GA tracking code has been embeded in the store’s website in order to anonymously track visitor interactions on the site. These attributes, or characteristics, of visitors’ first visit are the independent variables or features in ML.\nFor the same visitors, the outcome (or label in ML) of return visits (whether a purchase on a return visit was made or not) is retrieved to determine if a purchase on a future visit was made by this visitor. This label is a forward-looking label since it references events from the future. By comparison, the features are from the past (historical data) since they reference attributes of the visitor’s first visit to the store. Both features and label refer to the same visitor.\n\n\nAnalysis Workflow Overview\nWith such a dataset of Google Analytics tracking data available for all visitors to the store between August 1, 2016 and August 1, 2017, a ML model will be trained to predict whether first-time visitors will make a purchase during a future (return) visit. The trained model then predicts probabilities (which are interpreted as likelihoods or propensities) for new visitors to make a purchase on a return (or future) visit to the store. These new visitors were not part of the model’s training data. The predicted probabilities are then used to generate marketing audience groups (low, medium and high propensity) and test (or treatment) and control cohorts within each group as described above.\nIn summary the steps of such a workflow are\n\ntrain ML model using historical data for first visit of visitors\n\nthis is the training data\n\nuse trained model to predict probabilities for first visit of visitors that are not part of the training data\n\nthis is the unseen data\n\nuse predicted probabilities to assign audience cohorts (test or control) to all visitors in the unseen data\nbuild a brief profile of the visitors in the test cohort in unseen data\n\nwhen building a marketing strategy, we are not allowed to look at the control cohort and so the profile will be required for the test cohort only\n\nprovide audience test cohorts and their associated profile summaries to the marketing team\n\n\n\nTimeframes for Study\n\nIn order to avoid data leakage (or lookahead bias), the data splits are created in chronological order\n\ntraining data\n\nSeptember 1, 2016 to December 31, 2016\n\nvalidation data\n\nJanuary 1 - 31, 2017\n\ntest data\n\nFebruary 1 - 28, 2017\n\nunseen data\n\nMarch 1 - 31, 2017\n\n\nWe will assume that\n\nthe current date is March 1, 2017\nML model development can be performed between March 1 - 31, 2017\n\nThe marketing team is interested in growing new and repeat customers from visitors who made their first visit to the store during March 1 - 31, 2017 (unseen data). With this in mind, they want to\n\nbuild their campaign around these visitors\nlaunch their campaign on April 10, 2017\n\nThere are two constraints facing the client (marketing team)\n\nthe first visit data covering this period (unseen data) is only available on March 31, 2017 and this is close to the proposed campaign start date of April 10, 2017\ndesigning a typical marketing campaign takes 1 - 12 weeks\ncampaign launch windows occur every month\n\ncampaigns can be launched on April 10, 2017, May 10, 2017, June 10, 2017, etc.\n\n\nWith this in mind, the marketing team wants to start designing their campaign today (March 1, 2017). They do not want to wait until March 31, 2017 to receive recommended audience cohorts from the data science team and begin their campaign design. So, instead of waiting until March 31, 2017, the marketing team will start their campaign design using the audience cohorts recommended by the data science team using the test data split, which covers February 1 - 28, 2017.\nOn March 31, 2017, the marketing team will receive the audience cohorts from the data science team for the visits who made their first visit to the store during March 1 - 31, 2017. Between April 1, 2017 and April 9, 2017, the marketing team will start making adjustments to the campaign strategy by using the audience cohorts recommended by the data science team using the unseen data period (covering March 1 - 31, 2017). This will allow the marketing team to meet the proposed campaign start date of April 10, 2017.\nIf the datascience team is unable to generate a sufficiently accurate ML model to meet the April 10, 2017 campaign launch date then they will need to improve their analysis in order to try to meet the next available launch date (May 10, 2017).\n\n\n\nNotes\n\nRegarding ML labels (y)\n\nas mentioned earlier, these are forward-looking labels\n\nthe ML features (X) are attributes of a visitor’s first visit to the store\nthe ML labels (y) are the outcome (whether a purchase occurred or not) of that same visitor’s future visits to the store\n\na purchase is allowed to occur during any future visit to the store, not just the next visit\n\n\nin the period covering train, validation and test data splits, if a visitor has\n\nmade at least one purchase of a product during their return visit, then the label is set to True (or 1)\nnot made at least one purchase of a product during their return visit, then the label is set to False (or 0)\n\n\nThe data science team’s recommended audience cohorts (test and control) of visitors will be accepted by the marketing team if the ML model’s performance during evaluation (using the test data split) is better than that of a random model."
  },
  {
    "objectID": "references/Scope.html#how-do-actions-follow-from-the-analysis",
    "href": "references/Scope.html#how-do-actions-follow-from-the-analysis",
    "title": "Project Scope",
    "section": "How do Actions Follow From the Analysis",
    "text": "How do Actions Follow From the Analysis\nBased on visitors’ predicted likelihood of making a purchase on a future visit, marketing audience test and control groups (cohorts) will be created. Each group will contain a visitor ID as well as all the attributes of the visitor’s first visit that were used to predict the likelihood of a purchase during a return visit to the store.\nThese groups can be used by the marketing team to\n\ndesign appropriate strategies that can be implemented during activation\nestimate campaign costs"
  },
  {
    "objectID": "references/Scope.html#validation",
    "href": "references/Scope.html#validation",
    "title": "Project Scope",
    "section": "Validation",
    "text": "Validation\n\nDuring Development\nDevelopment covers September 1, 2016 - February 28, 2017.\nSince the current date is assumed to be March 1, 2017 and the training, validation and test data splits end no later than February 28, 2017, ML model predictions during validation (using validation split) and evaluation (using test split) can be scored before March 31, 2017.\nScoring is performed using evalaution metrics discussed in the ML Evaluation Metric sub-section below.\n\n\nDuring Production\nProduction covers March 1 - 31, 2017.\nThe model’s predictions will be scored against the outcome (whether the visitor makes a purchase on their return/future visit to the store) at the end of the marketing campaign.\n\n\nDifferences between Development and Production\n\nDuring production, the predictions are used to inform a marketing audience cohorts (test and control). By definition, the marketing strategy will be applied to the test cohort. It will not be applied to the control group. With this in mind, during the production period (March 1 - 31, 2017), we can only evaluate the predictions of the trained ML model that are associated with first-time visitors to the store during this period if those visitors are placed in the control cohort.\nAs mentioned earlier, the marketing team will only accept the data science team’s recommended audiente cohorts if the ML model outperforms a random model. At the same time, the data science team should also be checking for drift between attributes (features) of the first visit using the test split (development) and using the unseen data (production). If drift in features is observed outside a pre-defined threshold, then the data science team will need to repeat\n\nML model training using more training data (earlier start date than September 1, 2016)\nevaluation using the test split\nevaluation using the unseen data\n\nuntil drift is no longer observed. Feature drift checking will need to be done on April 1, 2017, before marketing audience visitor cohorts are given to the marketing team.\n\n\n\nWorkflow in Production\n\nThe trained model will make predictions of probability (propensity or likelihood) for all first-time visitors to the store during March 1 - 31, 2017. Predictions are used to identify marketing audience test (or treatment) and control cohorts.\nA marketing strategy is applied to all first-time visitors in the test group\nLength of marketing promotion campaign is to be determined by marketing team\nAt the end of the marketing campaign\n\nwe will know which visitors who were predicted to make a purchase on a return visit did actually make a purchase\nwe can evaluate the predictions made by the trained ML model on first visits that occurred during March 1 - 31, 2017\nwe can calculate a suitable KPI for this project\n\nKPI = number of purchases made by visitors in the test cohort - number of purchases made by visitors in the control cohort\n\nif this KPI is larger than zero, then we have successfully grown our customer base, which was the objective of the task that the business has given to the marketing team\n\nadditional KPIs can also be considered\n\n\n\nWe mentioned that scoring predictions of first visits that occurred during the unseen data period (production) of March 1 - 31, 2017 cannot be performed until the end of the marketing campaign. This was also mentioned in the during production sub-section above. It is worth emphasizing that until the end of the marketing campaign, we are unable to evalute the ML model’s predictions of data during the unseen data period (production). For this reason, it is improtant to check for drift in ML features between the unseen data (March 1 - 31, 2017) and test data (February 1 - 28, 2017) before the predictions are made and the audience cohorts are generated.\nGenerally, marketing campaigns run for approximately three months but this depends on numerous factors including\n\nmessage\ncall to action (CTA)\nfunds available (marketing budget)\nexpectations (desired uplift, etc.)\n\nThe duration, design and structure of the campaign will be determined by the marketing team starting on March 1, 2017 (today) and it will be finalized between April 1 - 9, 2019. On April 9, 2017, if it is determined that it is not feasible to design a campaign based on the audience cohorts recommended by the data science team (eg. cohorts are too large, etc.) then\n\nthe next available campaign launch window (May 10, 2017) will have to be targeted\nthe new unseen data (production) period will cover April 1 - 30, 2017\nthe datascience team will have to improve ML model performance between April 10 - 30, 2017\n\n\n\nML Evaluation Metric\nFalse negatives (tweets that should have been responded to but were predicted to not need a response) and false positives (tweets that did not need review by a team member but were predicted as requiring a review) are the most important types of errors. So the candidate metrics to be used to assess ML model performance are\n\nF1-score (if false negatives and false positives are equally important)\nF2-score (if false negatives are more important)\nF0.5-score (if false positives are more important)\n\nFor the current predicton task, there are two possible outcomes indicate whether a visitor did or did not make a purchase on a return visit to the store and these are\n\nactual\n\nis the true outcome\nthis is known after a visitor’s first visit to the store\nthis indicates that action that the marketing team should have taken\n\npredicted\n\nis the predicted outcome\nthis is predicted after the visitor’s first visit to the store\nthis indicates that action that the marketing team was predicted to have taken\n\n\nThe four possible ML prediction scenarios are listed below for the prediction of the outcome [whether a first-time visitor will, or will not, make a purchase on a return (future) visit to the merchandise store]\n\nTP: actual = makes purchase on return visit, predicted = makes purchase on return visit\n\npredicted marketing strategy matches what should be the actual marketing strategy\n\nTN: actual = does not make purchase on return visit, predicted = does not make purchase on return visit\n\npredicted marketing strategy matches what should be the actual marketing strategy\n\nFN: actual = makes purchase on return visit, predicted = does not make purchase on return visit\n\npredicted marketing strategy\n\npredicted to offer minimal promotion\n\nactual marketing strategy\n\nactually should have offered a stronger promotion\n\neg. more loyalty points, more frequent free/shipping, etc.\n\n\nthese errors in prediction lead to missed opportunities to correctly target first-time visitors since the predicted promotion offered is an underestimate of the true promotion that the team should have offered to these visitors\nthese errors lead to underspending on promotions to first-time visitors who are likely to benefit from them\n\nFP: actual = does not make purchase on return visit, predicted = makes purchase on return visit\n\npredicted marketing strategy\n\npredicted to offer a stronger promotion\n\nactual marketing strategy\n\nactually should have offered minimal promotion\n\nthese errors lead to overspending on promotions to first-time visitors who are not likely to benefit from them\nthis is the most expensive type of prediction error\nthis scenario must be avoided\n\n\nSince FP (false positives) are more costly than FN (false negatives), the scoring metric chosen to evaluate predictions made using the ML model is F0.5-score."
  },
  {
    "objectID": "references/Scope.html#data",
    "href": "references/Scope.html#data",
    "title": "Project Scope",
    "section": "Data",
    "text": "Data\nAn important factor that is driving propensity modeling in marketing is the need to do more with first-party customer data. This is data that comes directly from the customer and not from third-party sources. For marketing use-cases, effective propensity models use customer attributes from online and offline first-party data sources, including site analytics (online) and CRM (offline) data.\nHere, we have access to online data only Google Analytics tracking data (see the dataset and its documentation). This will be used to build a ML model to predict visitors’ propensity to make a purchase during a future visit to the store.\nVisit data for the merchandise store is available for the period of August 1, 2016 to August 1, 2017. This data provides information such as\n\nvisitor ID\nvisit date\nvist datetime\nactions performed during visit\n\nadd to cart\nremove from cart\nmake purchase\nview product details\netc.\n\ntotal time spent viewing pages during each visit\netc."
  },
  {
    "objectID": "references/Scope.html#analysis-notebooks",
    "href": "references/Scope.html#analysis-notebooks",
    "title": "Project Scope",
    "section": "Analysis Notebooks",
    "text": "Analysis Notebooks\n\nGet data and EDA Part 1\n\nconnect to raw visit data generated by Google Analytics tracking embedded in the merchandise store’s site\n\ndata is stored as Google BigQuery public dataset\nuse Python client to connect to dataset\nget overview of the columns in the raw visit data\n\nunderstand underlying patterns and stats about the visit-level data\nEDA part 1/2\n01_get_data.ipynb\n\nEDA Part 2\n\nEDA part 2/2\n02_eda.ipynb\n\nTransform data\n\n03_transform.ipynb\nextract the first visit per visitor (features, or X) and align with whether they made a purchase on a return (future) visit to the store (labels, or y)\n\nBaseline model development\n\ndevelop a baseline model to predict probability of purchase during future visit\n\nthis will be fast to train and will demonstrate the end-to-end project workflow, but will likely be over-simplified and so will underperform relative to a ML-based approach\n\n04_development.ipynb\n\nML model development\n\nrepeat baseline model development, but use a ML model instead\n05_dev_v2.ipynb"
  },
  {
    "objectID": "references/Scope.html#limitations",
    "href": "references/Scope.html#limitations",
    "title": "Project Scope",
    "section": "Limitations",
    "text": "Limitations\n\nBusiness Use Case\nThe analysis implemented here is only possible if Google Analytics tracking is embedded into an e-commerce website. Guides for embedding GA tracking code are documented below\n\nchartio blog post\nGoogle Support documentation\n\nFor the current use-case, this was already done for the Google Merchandise store’s website and so valuable tracking data could be collected and used. However, if such a solution is to be adopted for other digital marketplaces, then the Google Analytics tracking code must be embedded into those websites.\n\n\nData\n\nThe analytics dataset used in this project is based on a version of Google Analytics (GA360) that is deprecated as of July 1 2023 or 2024.\n\n\n\nOthers\nFor other limitations, please see the Limitations section in each notebook."
  },
  {
    "objectID": "references/Scope.html#assumptions",
    "href": "references/Scope.html#assumptions",
    "title": "Project Scope",
    "section": "Assumptions",
    "text": "Assumptions\n\nBusiness Use Case\n\nFor visitors who made a purchase on a return visit, we will include those who could have bought on their first as well. These are repeat customers, who we have assumed are one of the two types of visitors that we want to grow. For this reason, we will include their visits in the data.\nThe marketing team has does not have a preliminary idea as to the size of the audience groups (low, medium, high likelihood or propensity to make a purchase on a return visit) or cohorts and the strategy they will deploy as part of a campaign. As such, they have not yet designed any approaches to address this problem.\nDeployment-related assumptions (see point 4. in Timeframes for Study)\n\nwe have assumed that the current date is March 1, 2017\nwe have assumed that a trained ML model will make predictions for all first-time visitors to the store between March 1 - 31, 2017\n\n\nFor other assumptions, please see the Assumptions section in each notebook."
  },
  {
    "objectID": "notebooks/01-get-data/notebooks/01_get_data.html",
    "href": "notebooks/01-get-data/notebooks/01_get_data.html",
    "title": "Get Data",
    "section": "",
    "text": "Import Python modules\nCode\nimport os\nfrom datetime import date, datetime\nfrom glob import glob\n\nimport pandas as pd\nimport pytz\nfrom google.oauth2 import service_account"
  },
  {
    "objectID": "notebooks/01-get-data/notebooks/01_get_data.html#about",
    "href": "notebooks/01-get-data/notebooks/01_get_data.html#about",
    "title": "Get Data",
    "section": "About",
    "text": "About\n\nObjective\nTo start analysis for this project, we will first retrieve visitor session data from a small sample of the Google Merchandise Store between the summer of August 1, 2016 and August 1, 2017.\n\n\nData\nThe dataset is provided by Google BigQuery (Google’s data warehouse) and is available here. It consists of a single table of visitor session data. The documentation for this dataset shows the column names and description."
  },
  {
    "objectID": "notebooks/01-get-data/notebooks/01_get_data.html#user-inputs",
    "href": "notebooks/01-get-data/notebooks/01_get_data.html#user-inputs",
    "title": "Get Data",
    "section": "User Inputs",
    "text": "User Inputs\nGet relative path to project root directory\n\nPROJ_ROOT_DIR = os.path.join(os.pardir)\n\nRetrieve credentials for bigquery client\n\n# Google Cloud PROJECT ID\ngcp_project_id = os.environ[\"GCP_PROJECT_ID\"]\n\nGet filepath to Google Cloud Service Account JSON key\n\nraw_data_dir = os.path.join(PROJ_ROOT_DIR, \"data\", \"raw\")\ngcp_creds_fpath = glob(os.path.join(raw_data_dir, \"*.json\"))[0]\n\nAuthenticate bigquery client and get dictionary with credentials\n\ngcp_credentials = service_account.Credentials.from_service_account_file(gcp_creds_fpath)\ngcp_auth_dict = dict(gcp_project_id=gcp_project_id, gcp_creds=gcp_credentials)\n\nCreate a mapping between action type integer and label, in order to get meaningful names from the action_type column\n\nmapper = {\n    1: \"Click through of product lists\",\n    2: \"Product detail views\",\n    3: \"Add product(s) to cart\",\n    4: \"Remove product(s) from cart\",\n    5: \"Check out\",\n    6: \"Completed purchase\",\n    7: \"Refund of purchase\",\n    8: \"Checkout options\",\n    0: \"Unknown\",\n}\n\nDefine a Python helper function to execute a SQL query using Google BigQuery\n\ndef run_sql_query(\n    query: str,\n    gcp_project_id: str,\n    gcp_creds: os.PathLike,\n    show_dtypes: bool = False,\n    show_info: bool = False,\n    show_df: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Run query on BigQuery and return results as pandas.DataFrame.\"\"\"\n    start_time = datetime.now(pytz.timezone(\"US/Eastern\"))\n    start_time_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n    print(f\"Query execution start time = {start_time_str[:-3]}...\", end=\"\")\n    df = pd.read_gbq(\n        query,\n        project_id=gcp_project_id,\n        credentials=gcp_creds,\n        dialect=\"standard\",\n        # configuration is optional, since default for query caching is True\n        configuration={\"query\": {\"useQueryCache\": True}},\n        # use_bqstorage_api=True,\n    )\n    end_time = datetime.now(pytz.timezone(\"US/Eastern\"))\n    end_time_str = end_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n    duration = end_time - start_time\n    duration = duration.seconds + (duration.microseconds / 1_000_000)\n    print(f\"done at {end_time_str[:-3]} ({duration:.3f} seconds).\")\n    print(f\"Query returned {len(df):,} rows\")\n    if show_df:\n        with pd.option_context(\"display.max_columns\", None):\n            display(df)\n    if show_dtypes:\n        display(df.dtypes.rename(\"dtype\").to_frame().transpose())\n    if show_info:\n        df.info()\n    return df"
  },
  {
    "objectID": "notebooks/01-get-data/notebooks/01_get_data.html#get-data",
    "href": "notebooks/01-get-data/notebooks/01_get_data.html#get-data",
    "title": "Get Data",
    "section": "Get Data",
    "text": "Get Data\nShow the column properties (schema) of the sessions by users on the Google Marketplace\n\n\nCode\nquery = \"\"\"\n        SELECT table_name,\n               column_name,\n               data_type,\n               is_nullable,\n               ordinal_position\n        FROM `data-to-insights`.ecommerce.INFORMATION_SCHEMA.COLUMNS\n        WHERE table_name = 'web_analytics'\n        \"\"\"\n_ = run_sql_query(query, **gcp_auth_dict, show_df=True)\n\n\nQuery execution start time = 2023-04-13 16:50:18.755...done at 2023-04-13 16:50:20.428 (1.673 seconds).\nQuery returned 15 rows\n\n\n\n\n\n\n\n\n\ntable_name\ncolumn_name\ndata_type\nis_nullable\nordinal_position\n\n\n\n\n0\nweb_analytics\nvisitorId\nINT64\nYES\n1\n\n\n1\nweb_analytics\nvisitNumber\nINT64\nYES\n2\n\n\n2\nweb_analytics\nvisitId\nINT64\nYES\n3\n\n\n3\nweb_analytics\nvisitStartTime\nINT64\nYES\n4\n\n\n4\nweb_analytics\ndate\nSTRING\nYES\n5\n\n\n5\nweb_analytics\ntotals\nSTRUCT&lt;visits INT64, hits INT64, pageviews INT...\nYES\n6\n\n\n6\nweb_analytics\ntrafficSource\nSTRUCT&lt;referralPath STRING, campaign STRING, s...\nYES\n7\n\n\n7\nweb_analytics\ndevice\nSTRUCT&lt;browser STRING, browserVersion STRING, ...\nYES\n8\n\n\n8\nweb_analytics\ngeoNetwork\nSTRUCT&lt;continent STRING, subContinent STRING, ...\nYES\n9\n\n\n9\nweb_analytics\ncustomDimensions\nARRAY&lt;STRUCT&lt;index INT64, value STRING&gt;&gt;\nNO\n10\n\n\n10\nweb_analytics\nhits\nARRAY&lt;STRUCT&lt;hitNumber INT64, time INT64, hour...\nNO\n11\n\n\n11\nweb_analytics\nfullVisitorId\nSTRING\nYES\n12\n\n\n12\nweb_analytics\nuserId\nSTRING\nYES\n13\n\n\n13\nweb_analytics\nchannelGrouping\nSTRING\nYES\n14\n\n\n14\nweb_analytics\nsocialEngagementType\nSTRING\nYES\n15\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThere are eight flattened columns and six nested columns in the web_analytics table.\nThe flattened columns are briefly explained below\n\ngeoNetwork provides information about the geography of the users who accessed the store website\ncustomDimensions are user-configured combinations of values of specific metrics to be tracked\n\nthese won’t be used for the current project\n\ndevice provides information about the user’s electronic device that was used to access the store website\ntotals provides aggregated stats per visit\ntrafficSource contains metadata about the traffic source that resulted in a user’s visit\nhits tracks execution of Google analytics tracking code embedded in the store’s website (this happends when a user performs an interaction with the store webpage)\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nWhile features for machine learning can be extracted from all the nested and flattened columns, the usefulness of these features is based on our understanding of the data. Only the features that can be\n\nwell understood\nextracted without data leakage/lookahead bias\n\nshould be extracted for use in ML. The following nested features should be intuitive and so will likely be easier to understand in the context of e-commerce transactions that the other nested columns\n\ngeoNetwork\ndevice\ntotals\n\n\n\n\nA simple SELECT is used to show a subset of the columns for the first 15 visits between November 1, 2016 and November 2, 2016\n\nquery = \"\"\"\n        SELECT fullvisitorid,\n               -- get date of transaction\n               date,\n               -- convert date to datetime in year-month-date format\n               PARSE_DATE('%Y%m%d', DATE) AS datetime_ymd,\n               -- visit start time and number\n               DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific') AS visitStartTime,\n               visitNumber,\n               -- source of the traffic from which the session was initiated\n               trafficSource.source,\n               -- medium of the traffic from which the session was initiated\n               trafficSource.medium,\n               -- referring channel connected to session\n               channelGrouping,\n               -- user's type of device\n               device.deviceCategory AS device_category,\n               -- user's operating system\n               device.operatingSystem AS os,\n               -- referring page for visits that ended in a transaction\n               CAST(h.eCommerceAction.action_type AS INT64) AS action_type,\n               -- transactions\n               totals.transactions,\n               -- return visits to the merchandise store\n               totals.newVisits AS newVisits\n        FROM `data-to-insights.ecommerce.web_analytics`,\n        UNNEST(hits) AS h\n        WHERE date BETWEEN '20161101' AND '20161102'\n        LIMIT 15\n        \"\"\"\ndf = run_sql_query(query, **gcp_auth_dict, show_df=False)\ndf[\"action_type\"] = df[\"action_type\"].map(mapper).astype(pd.StringDtype())\nprint(\n    f\"Datetimes: {df['visitStartTime'].min().strftime('%Y-%m-%d %H:%M:%S')} - \"\n    f\"{df['visitStartTime'].max().strftime('%Y-%m-%d %H:%M:%S')}\"\n)\ndisplay(df)\ndf.info()\n\nQuery execution start time = 2023-04-13 16:50:20.458...done at 2023-04-13 16:50:21.646 (1.188 seconds).\nQuery returned 15 rows\nDatetimes: 2016-11-01 16:26:08 - 2016-11-02 12:58:15\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 15 entries, 0 to 14\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype         \n---  ------           --------------  -----         \n 0   fullvisitorid    15 non-null     object        \n 1   date             15 non-null     object        \n 2   datetime_ymd     15 non-null     dbdate        \n 3   visitStartTime   15 non-null     datetime64[ns]\n 4   visitNumber      15 non-null     Int64         \n 5   source           15 non-null     object        \n 6   medium           15 non-null     object        \n 7   channelGrouping  15 non-null     object        \n 8   device_category  15 non-null     object        \n 9   os               15 non-null     object        \n 10  action_type      15 non-null     string        \n 11  transactions     0 non-null      Int64         \n 12  newVisits        10 non-null     Int64         \ndtypes: Int64(3), datetime64[ns](1), dbdate(1), object(7), string(1)\nmemory usage: 1.7+ KB\n\n\n\n\n\n\n\n\n\nfullvisitorid\ndate\ndatetime_ymd\nvisitStartTime\nvisitNumber\nsource\nmedium\nchannelGrouping\ndevice_category\nos\naction_type\ntransactions\nnewVisits\n\n\n\n\n0\n7563857575439343066\n20161102\n2016-11-02\n2016-11-02 12:58:15\n1\ngoogle\norganic\nOrganic Search\ndesktop\nWindows\nUnknown\n&lt;NA&gt;\n1\n\n\n1\n7563857575439343066\n20161102\n2016-11-02\n2016-11-02 12:58:15\n1\ngoogle\norganic\nOrganic Search\ndesktop\nWindows\nUnknown\n&lt;NA&gt;\n1\n\n\n2\n7563857575439343066\n20161102\n2016-11-02\n2016-11-02 12:58:15\n1\ngoogle\norganic\nOrganic Search\ndesktop\nWindows\nUnknown\n&lt;NA&gt;\n1\n\n\n3\n7563857575439343066\n20161102\n2016-11-02\n2016-11-02 12:58:15\n1\ngoogle\norganic\nOrganic Search\ndesktop\nWindows\nUnknown\n&lt;NA&gt;\n1\n\n\n4\n7563857575439343066\n20161102\n2016-11-02\n2016-11-02 12:58:15\n1\ngoogle\norganic\nOrganic Search\ndesktop\nWindows\nUnknown\n&lt;NA&gt;\n1\n\n\n5\n7563857575439343066\n20161102\n2016-11-02\n2016-11-02 12:58:15\n1\ngoogle\norganic\nOrganic Search\ndesktop\nWindows\nUnknown\n&lt;NA&gt;\n1\n\n\n6\n7563857575439343066\n20161102\n2016-11-02\n2016-11-02 12:58:15\n1\ngoogle\norganic\nOrganic Search\ndesktop\nWindows\nUnknown\n&lt;NA&gt;\n1\n\n\n7\n0719274696629877759\n20161101\n2016-11-01\n2016-11-01 16:26:08\n2\ngoogle\norganic\nOrganic Search\ndesktop\nMacintosh\nUnknown\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n8\n0719274696629877759\n20161101\n2016-11-01\n2016-11-01 16:26:08\n2\ngoogle\norganic\nOrganic Search\ndesktop\nMacintosh\nUnknown\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n9\n0719274696629877759\n20161101\n2016-11-01\n2016-11-01 16:26:08\n2\ngoogle\norganic\nOrganic Search\ndesktop\nMacintosh\nUnknown\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n10\n0719274696629877759\n20161101\n2016-11-01\n2016-11-01 16:26:08\n2\ngoogle\norganic\nOrganic Search\ndesktop\nMacintosh\nUnknown\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n11\n0719274696629877759\n20161101\n2016-11-01\n2016-11-01 16:26:08\n2\ngoogle\norganic\nOrganic Search\ndesktop\nMacintosh\nUnknown\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n12\n4480038936311138124\n20161102\n2016-11-02\n2016-11-02 12:45:33\n1\ngoogle\norganic\nOrganic Search\ndesktop\nWindows\nUnknown\n&lt;NA&gt;\n1\n\n\n13\n4480038936311138124\n20161102\n2016-11-02\n2016-11-02 12:45:33\n1\ngoogle\norganic\nOrganic Search\ndesktop\nWindows\nUnknown\n&lt;NA&gt;\n1\n\n\n14\n4480038936311138124\n20161102\n2016-11-02\n2016-11-02 12:45:33\n1\ngoogle\norganic\nOrganic Search\ndesktop\nWindows\nUnknown\n&lt;NA&gt;\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nFor the datetime_ymd column, dbdate is the expected BigQuery SQL datatype.\nThe columns shown here are a subset of all the columns available in the table.\nUNNEST has to be used to flatten arrays (nested columns, such as hits) (explode them into separate rows), which is similar to pandas.DataFrame.explode().\nBigQuery uses query output caching, so subsequent runs of the same query SQL will return results in a shorter period of time than the previous run. See limitations here.\nThe timezone in the DATETIME() function was arbitrarily set to be US/Pacific. This means we are assuming our local timezone is US/Pacific. This will be kept consistent throughout this project."
  },
  {
    "objectID": "notebooks/01-get-data/notebooks/01_get_data.html#next-step",
    "href": "notebooks/01-get-data/notebooks/01_get_data.html#next-step",
    "title": "Get Data",
    "section": "Next Step",
    "text": "Next Step\nThe next step in the analysis workflow will involve exploring the visits data."
  },
  {
    "objectID": "notebooks/02-prepare/notebooks/02_prepare.html",
    "href": "notebooks/02-prepare/notebooks/02_prepare.html",
    "title": "Data Preparation",
    "section": "",
    "text": "Import Python modules\nCode\nimport os\nfrom datetime import datetime\nfrom glob import glob\nfrom typing import Dict, List\n\nimport pandas as pd\nimport pytz\nfrom google.oauth2 import service_account"
  },
  {
    "objectID": "notebooks/02-prepare/notebooks/02_prepare.html#about",
    "href": "notebooks/02-prepare/notebooks/02_prepare.html#about",
    "title": "Data Preparation",
    "section": "About",
    "text": "About\nThis step of the analysis will prepare data for use in exploratory and quantitative data analysis.\n\nDiscussion of Study Period for This Project\nWe will need to discuss the data that can be used to prepare data without suffering from lookahead bias/data leakage.\nData splits are created in a later step. ML model development will be performed using a training data split, validation (feature selection, hyperparameter tuning, etc.) is performed using the validation data split and, finally, the best ML model is evaluated using the test data split.\nSo, the validation and test data splits should be treated as unseen data. In order to avoid lookahead bias/data leakage during data preparation, this means that data preparation steps should be determined using the training data and simply applied to the validation and test data splits.\n\n\nData Selection for Data Preparation\nWith the above in mind, data preparation will cover the training data."
  },
  {
    "objectID": "notebooks/02-prepare/notebooks/02_prepare.html#user-inputs",
    "href": "notebooks/02-prepare/notebooks/02_prepare.html#user-inputs",
    "title": "Data Preparation",
    "section": "User Inputs",
    "text": "User Inputs\nGet relative path to project root directory\n\nPROJ_ROOT_DIR = os.path.join(os.pardir)\n\nDefine the following\n\ntrain data start date\ntrain data end date\ntest data end date\n\n\ntrain_start_date = \"20160901\"\ntrain_end_date = \"20161231\"\ntest_end_date = \"20170228\"\n\nRetrieve credentials for bigquery client\n\n# Google Cloud PROJECT ID\ngcp_project_id = os.environ[\"GCP_PROJECT_ID\"]\n\nGet filepath to Google Cloud Service Account JSON key\n\nraw_data_dir = os.path.join(PROJ_ROOT_DIR, \"data\", \"raw\")\ngcp_creds_fpath = glob(os.path.join(raw_data_dir, \"*.json\"))[0]\n\nAuthenticate bigquery client and get dictionary with credentials\n\ngcp_credentials = service_account.Credentials.from_service_account_file(gcp_creds_fpath)\ngcp_auth_dict = dict(gcp_project_id=gcp_project_id, gcp_creds=gcp_credentials)\n\nCreate a mapping between action type integer and label, in order to get meaningful names from the action_type column\n\nmapper = {\n    1: \"Click through of product lists\",\n    2: \"Product detail views\",\n    3: \"Add product(s) to cart\",\n    4: \"Remove product(s) from cart\",\n    5: \"Check out\",\n    6: \"Completed purchase\",\n    7: \"Refund of purchase\",\n    8: \"Checkout options\",\n    0: \"Unknown\",\n}\n\nDefine a Python helper function to execute a SQL query using Google BigQuery\n\ndef run_sql_query(\n    query: str,\n    gcp_project_id: str,\n    gcp_creds: os.PathLike,\n    show_dtypes: bool = False,\n    show_info: bool = False,\n    show_df: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Run query on BigQuery and return results as pandas.DataFrame.\"\"\"\n    start_time = datetime.now(pytz.timezone(\"US/Eastern\"))\n    start_time_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n    print(f\"Query execution start time = {start_time_str[:-3]}...\", end=\"\")\n    df = pd.read_gbq(\n        query,\n        project_id=gcp_project_id,\n        credentials=gcp_creds,\n        dialect=\"standard\",\n        # configuration is optional, since default for query caching is True\n        configuration={\"query\": {\"useQueryCache\": True}},\n        # use_bqstorage_api=True,\n    )\n    end_time = datetime.now(pytz.timezone(\"US/Eastern\"))\n    end_time_str = end_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n    duration = end_time - start_time\n    duration = duration.seconds + (duration.microseconds / 1_000_000)\n    print(f\"done at {end_time_str[:-3]} ({duration:.3f} seconds).\")\n    print(f\"Query returned {len(df):,} rows\")\n    if show_df:\n        with pd.option_context(\"display.max_columns\", None):\n            display(df)\n    if show_dtypes:\n        display(df.dtypes.rename(\"dtype\").to_frame().transpose())\n    if show_info:\n        df.info()\n    return df"
  },
  {
    "objectID": "notebooks/02-prepare/notebooks/02_prepare.html#data-preparation",
    "href": "notebooks/02-prepare/notebooks/02_prepare.html#data-preparation",
    "title": "Data Preparation",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nVisitors Who Made a Purchase\nQuestion 1. Get visitors with a purchase on a future visit to the Marketplace.\nTo get these visitors, a similar approach to that from the get-data step will be used. In that step, two criteria were used to identify a purchase on a future visit, namely total.transactions &gt; 0 and totals.newVisits IS NULL. Those will be used here as well.\nA query with these filters is executed below\n\nquery = f\"\"\"\n        SELECT fullvisitorid,\n               IF(COUNTIF(totals.transactions &gt; 0 AND totals.newVisits IS NULL) &gt; 0, True, False) AS made_purchase_on_future_visit,\n               IF(\n                   SUM(CASE WHEN totals.transactions &gt; 0 AND totals.newVisits IS NULL THEN 1 ELSE 0 END) &gt; 0, True, False\n               ) AS made_purchase_on_future_visit_v2\n        FROM `data-to-insights.ecommerce.web_analytics`\n        WHERE date BETWEEN '{train_start_date}' AND '{test_end_date}'\n        AND geoNetwork.country = 'United States'\n        GROUP BY fullvisitorid\n        \"\"\"\ndf = run_sql_query(query, **gcp_auth_dict, show_df=False)\ndisplay(\n    # breakdown of returning purchasers using COUNTIF()\n    df[\"made_purchase_on_future_visit\"]\n    .value_counts()\n    .rename(\"num_return_purchasers_using_countif\")\n    .to_frame()\n    .merge(\n        # breakdown of returning purchasers using CASE WHEN()\n        df[\"made_purchase_on_future_visit_v2\"]\n        .value_counts()\n        .rename(\"num_return_purchasers_using_if_sum_casewhen\")\n        .to_frame(),\n        left_index=True,\n        right_index=True,\n    )\n    .merge(\n        (\n            100\n            * df[\"made_purchase_on_future_visit\"]\n            .value_counts(normalize=True)\n            .rename(\"frac_made_purchase_on_future_visit\")\n        ).to_frame(),\n        left_index=True,\n        right_index=True,\n    )\n    .reset_index()\n    .rename(columns={\"index\": \"made_return_purchase\"})\n)\n\nQuery execution start time = 2023-04-13 17:03:04.444...done at 2023-04-13 17:03:17.048 (12.604 seconds).\nQuery returned 137,727 rows\n\n\n\n\n\n\n\n\n\nmade_purchase_on_future_visit\nnum_return_purchasers_using_countif\nnum_return_purchasers_using_if_sum_casewhen\nfrac_made_purchase_on_future_visit\n\n\n\n\n0\nFalse\n131734\n131734\n95.648638\n\n\n1\nTrue\n5993\n5993\n4.351362\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nCOUNTIF() is a BigQuery SQL function (1) but it gives the same output as a standard SQL-based approach using IF(SUM(CASE WHEN...)). For the rest of this step, COUNTIF() will be used\nOver the selected months, the class imbalance is close to 96% to 4% (or 96:4)\n\nthis comes from the made_purchase_on_future_visit column\nthese are the class labels for ML experiments\n\n\n\n\n\n\nFirst Visit Attributes\nQuestion 2. Extract attributes from the first visit by visitors that made a purchase on a future visit.\nThe following are the attributes extracted from the first visit (for the above visitors only) and the high-level categories that they belong to\n\ngeospatial and temporal\n\ncountry\ndatetime attributes (day of month, hour of day, etc.)\n\nmetadata of each visit and visitor\n\nthese are id (or equivalent) columns\n\ntraffic sources and channels\n\ntraffic sources\n\nthese are search engines, social media networks, and other sources that result in visitors reaching the merchandise store’s website (link)\n\nchannels\n\nthese are groups of traffic sources (link)\n\n\nvisitor activity on site\n\nhits\nbounces\npage views\ntime spent on site\nnumber of add-to-cart actions performed\n\nvisitor’s device used to access site\n\nbrowser\ndevice category\noperating system\n\nlabel for machine learning\n\nmade_purchase_on_future_visit\n\nsame as in the above query\nindicates whether a visitor makes a purchase during their next visit\n\n\nProduct\n\nproducts viewed\nproducts clicked\n\nPromotion\n\npromotions viewed (impressions)\npromotions clicked\n\n\n\nquery = f\"\"\"\n        WITH\n        -- Step 1. get visitors with a purchase on a future visit\n        next_visit_purchasers AS (\n             SELECT fullvisitorid,\n                    IF(COUNTIF(totals.transactions &gt; 0 AND totals.newVisits IS NULL) &gt; 0, True, False) AS made_purchase_on_future_visit\n             FROM `data-to-insights.ecommerce.web_analytics`\n             WHERE date BETWEEN '{train_start_date}' AND '{test_end_date}'\n             AND geoNetwork.country = 'United States'\n             GROUP BY fullvisitorid\n        ),\n        -- Steps 2. and 3. get attributes of the first visit\n        first_visit_attributes AS (\n            SELECT -- =========== GEOSPATIAL AND TEMPORAL ATTRIBUTES OF VISIT ===========\n                   geoNetwork.country,\n                   EXTRACT(QUARTER FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS quarter,\n                   EXTRACT(MONTH FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS month,\n                   EXTRACT(DAY FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS day_of_month,\n                   EXTRACT(DAYOFWEEK FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS day_of_week,\n                   EXTRACT(HOUR FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS hour,\n                   EXTRACT(MINUTE FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS minute,\n                   EXTRACT(SECOND FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS second,\n                   -- =========== VISIT AND VISITOR METADATA ===========\n                   fullvisitorid,\n                   visitId,\n                   visitNumber,\n                   DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific') AS visitStartTime,\n                   -- =========== SOURCE OF SITE TRAFFIC ===========\n                   -- source of the traffic from which the visit was initiated\n                   trafficSource.source,\n                   -- medium of the traffic from which the visit was initiated\n                   trafficSource.medium,\n                   -- referring channel connected to visit\n                   channelGrouping,\n                   -- =========== VISITOR ACTIVITY ===========\n                   -- total number of hits\n                   (CASE WHEN totals.hits &gt; 0 THEN totals.hits ELSE 0 END) AS hits,\n                   -- number of bounces\n                   (CASE WHEN totals.bounces &gt; 0 THEN totals.bounces ELSE 0 END) AS bounces,\n                   -- action performed during first visit\n                   CAST(h.eCommerceAction.action_type AS INT64) AS action_type,\n                   -- page views\n                   IFNULL(totals.pageviews, 0) AS pageviews,\n                   -- total revenue\n                   totals.totalTransactionRevenue / 1000000 AS transact_revenue,\n                   -- time on the website\n                   IFNULL(totals.timeOnSite, 0) AS time_on_site,\n                   -- whether add-to-cart was performed during visit\n                   (CASE WHEN CAST(h.eCommerceAction.action_type AS INT64) = 3 THEN 1 ELSE 0 END) AS added_to_cart,\n                   (CASE WHEN CAST(h.eCommerceAction.action_type AS INT64) = 2 THEN 1 ELSE 0 END) AS product_details_viewed,\n                   -- =========== VISITOR DEVICES ===========\n                   -- user's browser\n                   device.browser,\n                   -- user's operating system\n                   device.operatingSystem AS os,\n                   -- user's type of device\n                   device.deviceCategory,\n                   -- =========== PROMOTION ===========\n                   h.promotion,\n                   h.promotionActionInfo AS pa_info,\n                   -- =========== PRODUCT ===========\n                   h.product,\n                   -- =========== ML LABEL (DEPENDENT VARIABLE) ===========\n                   made_purchase_on_future_visit\n            FROM `data-to-insights.ecommerce.web_analytics`,\n            UNNEST(hits) AS h\n            INNER JOIN next_visit_purchasers USING (fullvisitorid)\n            WHERE date BETWEEN '{train_start_date}' AND '{train_end_date}'\n            AND geoNetwork.country = 'United States'\n            AND totals.newVisits = 1\n        ),\n        -- Step 4. get aggregated features (attributes) per visit\n        visit_attributes AS (\n            SELECT fullvisitorid,\n                   visitId,\n                   visitNumber,\n                   visitStartTime,\n                   country,\n                   quarter,\n                   month,\n                   day_of_month,\n                   day_of_week,\n                   hour,\n                   minute,\n                   second,\n                   source,\n                   medium,\n                   channelGrouping,\n                   hits,\n                   bounces,\n                   -- get the last action performed during the first visit\n                   -- (this indicates where the visitor left off at the end of their visit)\n                   MAX(action_type) AS last_action,\n                   -- get number of products whose details were viewed\n                   SUM(product_details_viewed) AS product_detail_views,\n                   -- get number of promotions displayed and clicked during the first visit\n                   COUNT(CASE WHEN pa_info IS NOT NULL THEN pa_info.promoIsView ELSE NULL END) AS promos_displayed,\n                   COUNT(CASE WHEN pa_info IS NOT NULL THEN pa_info.promoIsClick ELSE NULL END) AS promos_clicked,\n                   -- get number of products displayed and clicked during the first visit\n                   COUNT(CASE WHEN pu.isImpression IS NULL THEN NULL ELSE 1 END) AS product_views,\n                   COUNT(CASE WHEN pu.isClick IS NULL THEN NULL ELSE 1 END) AS product_clicks,\n                   pageviews,\n                   transact_revenue,\n                   time_on_site,\n                   browser,\n                   os,\n                   deviceCategory,\n                   SUM(added_to_cart) AS added_to_cart,\n                   made_purchase_on_future_visit,\n            FROM first_visit_attributes\n            LEFT JOIN UNNEST(promotion) as p\n            LEFT JOIN UNNEST(product) as pu\n            GROUP BY fullvisitorid,\n                     visitId,\n                     visitNumber,\n                     visitStartTime,\n                     country,\n                     quarter,\n                     month,\n                     day_of_month,\n                     day_of_week,\n                     hour,\n                     minute,\n                     second,\n                     source,\n                     medium,\n                     channelGrouping,\n                     hits,\n                     bounces,\n                     pageviews,\n                     transact_revenue,\n                     time_on_site,\n                     browser,\n                     os,\n                     deviceCategory,\n                     made_purchase_on_future_visit\n        )\n        SELECT *\n        FROM visit_attributes\n        \"\"\"\ndf = run_sql_query(query, **gcp_auth_dict, show_df=False)\nwith pd.option_context(\"display.max_columns\", None):\n    display(df.head())\n    display(df.tail())\n\nQuery execution start time = 2023-04-13 17:03:44.045...done at 2023-04-13 17:04:06.815 (22.770 seconds).\nQuery returned 92,859 rows\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nsource\nmedium\nchannelGrouping\nhits\nbounces\nlast_action\nproduct_detail_views\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntransact_revenue\ntime_on_site\nbrowser\nos\ndeviceCategory\nadded_to_cart\nmade_purchase_on_future_visit\n\n\n\n\n0\n8163735676529750721\n1481176805\n1\n2016-12-07 22:00:05\nUnited States\n4\n12\n7\n4\n22\n0\n5\ngoogle\ncpc\nPaid Search\n5\n0\n0\n0\n0\n0\n0\n0\n5\nNaN\n109\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n1\n0897634596694862660\n1475199828\n1\n2016-09-29 18:43:48\nUnited States\n3\n9\n29\n5\n18\n43\n48\ngoogle\norganic\nOrganic Search\n21\n0\n2\n2\n9\n0\n90\n2\n19\nNaN\n467\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n2\n3891893273235584028\n1478844153\n1\n2016-11-10 22:02:33\nUnited States\n4\n11\n10\n5\n22\n2\n33\nyoutube.com\nreferral\nSocial\n11\n0\n2\n1\n18\n0\n36\n1\n10\nNaN\n1003\nSafari (in-app)\niOS\ntablet\n0\nFalse\n\n\n3\n6952706734234495703\n1480581075\n1\n2016-12-01 00:31:15\nUnited States\n4\n12\n1\n5\n0\n31\n15\ngoogle\norganic\nOrganic Search\n9\n0\n0\n0\n27\n1\n24\n0\n8\nNaN\n141\nChrome\nAndroid\nmobile\n0\nFalse\n\n\n4\n3008949133172215443\n1475213353\n1\n2016-09-29 22:29:13\nUnited States\n3\n9\n29\n5\n22\n29\n13\ngoogle\norganic\nOrganic Search\n14\n0\n2\n3\n9\n0\n24\n3\n11\nNaN\n229\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nsource\nmedium\nchannelGrouping\nhits\nbounces\nlast_action\nproduct_detail_views\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntransact_revenue\ntime_on_site\nbrowser\nos\ndeviceCategory\nadded_to_cart\nmade_purchase_on_future_visit\n\n\n\n\n92854\n1446285229092173706\n1478190491\n1\n2016-11-03 09:28:11\nUnited States\n4\n11\n3\n5\n9\n28\n11\n(direct)\n(none)\nDirect\n3\n0\n0\n0\n0\n0\n29\n0\n3\nNaN\n224\nChrome\nChrome OS\ndesktop\n0\nTrue\n\n\n92855\n927804845306410814\n1482181118\n1\n2016-12-19 12:58:38\nUnited States\n4\n12\n19\n2\n12\n58\n38\ngoogle\norganic\nOrganic Search\n3\n0\n0\n0\n0\n0\n32\n0\n3\nNaN\n83\nChrome\nLinux\ndesktop\n0\nFalse\n\n\n92856\n8969674851394509099\n1482941116\n1\n2016-12-28 08:05:16\nUnited States\n4\n12\n28\n4\n8\n5\n16\nyoutube.com\nreferral\nSocial\n3\n0\n0\n0\n0\n0\n28\n0\n3\nNaN\n85\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n92857\n0486118040507415508\n1477164641\n1\n2016-10-22 12:30:41\nUnited States\n4\n10\n22\n7\n12\n30\n41\nt.co\nreferral\nSocial\n3\n0\n0\n0\n0\n0\n28\n0\n3\nNaN\n78\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n92858\n9088496055778533168\n1482668980\n1\n2016-12-25 04:29:40\nUnited States\n4\n12\n25\n1\n4\n29\n40\nyoutube.com\nreferral\nSocial\n3\n0\n0\n0\n0\n0\n36\n0\n3\nNaN\n69\nChrome\nLinux\ndesktop\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nBelow is a brief overview of the CTEs used here\n\nnext_visit_purchasers\n\ngets visitors who made a purchase on a return visit to the merchandise store on the Google Marketplace\n\nfirst_visit_attributes\n\ngets attributes of first visit\nthe statement INNER JOIN next_visit_purchasers USING (fullvisitorid) is used to only select the visitors that made a purchase on a return visit to the store (these fullvisitorids are stored in the next_visit_purchasers CTE)\n\nvisit_attributes\n\naggregates values from nested columns to get views and clicks for promotions and products\n\n\nThe BigQuery SQL function UNNEST was used to flatten nested columns.\nThe start and end dates of the ML training, validation and test data splits were defined. The training dates have been used to filter the first_visit_attributes CTE in order since EDA in this step will only be performed using the training data in order to avoid data leakage (or lookahead bias).\nThe SQL required to extract most of these columns was determined from (a) the documentation for the dataset and (b) examining the first few rows of the dataset in these columns. For brevity, we won’t discuss these columns in further detail. These column categories are listed below\n\ngeospatial and temporal\nmetadata of each visit and visitor\ntraffic sources and channels\nvisitor activity on merchandise store website\nvisitor’s device\nlabel for machine learning (discussed in Data Preparation Question 2. above)\n\nThese columns were extracted based on intuition about the attributes of each visit that will help to predict the probability of a visitor making a purchase on a future (return) visit to the merchandise store.\n\n\n\n\n\nFraction of Visitors with Add-to-Cart on First Visit\nQuestion 3. What fraction of visitors added one or more items to their shopping cart during their first visit?\n\nfor c in [\"added_to_cart\"]:\n    display(\n        (100 * df[c].value_counts(dropna=False, normalize=True))\n        .rename(\"number\")\n        .reset_index()\n        .rename(columns={\"index\": \"added_to_cart\"})\n    )\n\n\n\n\n\n\n\n\nadded_to_cart\nnumber\n\n\n\n\n0\n0\n89.336521\n\n\n1\n1\n6.335412\n\n\n2\n2\n2.073035\n\n\n3\n3\n0.915366\n\n\n4\n4\n0.479221\n\n\n5\n5\n0.273533\n\n\n6\n6\n0.176612\n\n\n7\n7\n0.102306\n\n\n8\n8\n0.078614\n\n\n9\n9\n0.041999\n\n\n10\n10\n0.034461\n\n\n11\n12\n0.033384\n\n\n12\n11\n0.029076\n\n\n13\n14\n0.014\n\n\n14\n13\n0.012923\n\n\n15\n15\n0.010769\n\n\n16\n17\n0.009692\n\n\n17\n16\n0.006461\n\n\n18\n18\n0.004308\n\n\n19\n20\n0.004308\n\n\n20\n19\n0.004308\n\n\n21\n21\n0.003231\n\n\n22\n25\n0.003231\n\n\n23\n22\n0.002154\n\n\n24\n34\n0.002154\n\n\n25\n40\n0.002154\n\n\n26\n36\n0.001077\n\n\n27\n26\n0.001077\n\n\n28\n28\n0.001077\n\n\n29\n32\n0.001077\n\n\n30\n24\n0.001077\n\n\n31\n113\n0.001077\n\n\n32\n45\n0.001077\n\n\n33\n39\n0.001077\n\n\n34\n27\n0.001077\n\n\n35\n30\n0.001077\n\n\n36\n&lt;NA&gt;\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nDuring the months covered by the training data, nearly 90% of visitors did not add an item to their shopping cart during their first visit to the merchandise store (added_to_cart = 0). Only 10% of such visitors added an item to their shopping cart during this time.\n\n\n\n\n\nReasons For and Handling of Duplicates\nQuestion 4. Comment on duplicates present in the data prepared above. What are some possible reasons for the presence of duplicates in the above prepared data? How should these be handled?\nBelow we show that there are duplicates within the fullvisitorid column\n\n\nCode\nprint(\n    f\"Number of rows = {len(df):,}\\nNumber of unique visitor IDs = \"\n    f\"{df['fullvisitorid'].nunique():,}\\n\"\n    f\"Largest visitNumber = {df['visitNumber'].max()}\"\n)\n\n\nNumber of rows = 92,859\nNumber of unique visitor IDs = 92,551\nLargest visitNumber = 1\n\n\nThese duplicates are retrieved below, showing that multiple visitIds are present for the same visitorid\n\ndf_num_dups = (\n    df.groupby([\"fullvisitorid\"])\n    .agg({\"visitId\": \"count\", \"visitNumber\": \"max\"})\n    .reset_index()\n    .rename(columns={\"visitId\": \"num_visitIds\"})\n    .query(\"num_visitIds &gt; 1\")\n)\ndisplay(df_num_dups.head())\ndisplay(df_num_dups.tail())\n\n\n\n\n\n\n\n\nfullvisitorid\nnum_visitIds\nvisitNumber\n\n\n\n\n124\n0014997413479849928\n2\n1\n\n\n1186\n012569301201854368\n2\n1\n\n\n1400\n0153393931967124172\n2\n1\n\n\n1831\n0196238382136996118\n2\n1\n\n\n2163\n0233922069260074966\n2\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfullvisitorid\nnum_visitIds\nvisitNumber\n\n\n\n\n91561\n9897914422695841426\n2\n1\n\n\n91653\n9906208132011345120\n2\n1\n\n\n91749\n9915457192772678365\n2\n1\n\n\n92054\n9949751653823311987\n2\n1\n\n\n92079\n9952616174324085427\n2\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nDuplicates can occur by\n\nfullvisitorid\nvisitId\n\nso, we should explore both cases separately.\n\n\n\nDuplicated visitIds are shown below\n\ndup_visit_ids = df[df.duplicated(subset=[\"visitId\"], keep=False)]\nnum_dups = len(df[df.duplicated(subset=[\"visitId\"], keep=\"first\")])\nprint(\n    f\"Found {num_dups:,} duplicated visitIds out of \"\n    f\"{len(df):,} ({100*num_dups/len(df):.3f}%)\"\n)\nwith pd.option_context(\"display.max_columns\", None):\n    display(dup_visit_ids.sort_values(by=[\"visitId\"]).head(25))\n\nFound 742 duplicated visitIds out of 92,859 (0.799%)\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nsource\nmedium\nchannelGrouping\nhits\nbounces\nlast_action\nproduct_detail_views\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntransact_revenue\ntime_on_site\nbrowser\nos\ndeviceCategory\nadded_to_cart\nmade_purchase_on_future_visit\n\n\n\n\n28174\n7673928089571501866\n1472751431\n1\n2016-09-01 10:37:11\nUnited States\n3\n9\n1\n5\n10\n37\n11\ngoogle\norganic\nOrganic Search\n27\n0\n2\n6\n0\n0\n293\n8\n19\nNaN\n845\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n47204\n6028120763017470092\n1472751431\n1\n2016-09-01 10:37:11\nUnited States\n3\n9\n1\n5\n10\n37\n11\ndfa\ncpm\nDisplay\n7\n0\n0\n0\n9\n0\n147\n0\n7\nNaN\n153\nChrome\nAndroid\nmobile\n0\nFalse\n\n\n66489\n4654710059786315542\n1472752926\n1\n2016-09-01 11:02:06\nUnited States\n3\n9\n1\n5\n11\n2\n6\nyoutube.com\nreferral\nSocial\n3\n0\n0\n0\n18\n0\n0\n0\n3\nNaN\n50\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n14790\n0622219713224273207\n1472752926\n1\n2016-09-01 11:02:06\nUnited States\n3\n9\n1\n5\n11\n2\n6\nmall.googleplex.com\nreferral\nReferral\n7\n0\n0\n0\n9\n0\n167\n0\n7\nNaN\n178\nChrome\nLinux\ndesktop\n0\nFalse\n\n\n79228\n0983194581450463928\n1472771118\n1\n2016-09-01 16:05:18\nUnited States\n3\n9\n1\n5\n16\n5\n18\nreddit.com\nreferral\nSocial\n2\n0\n0\n0\n9\n0\n34\n0\n2\nNaN\n15\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n64049\n8679833862264857329\n1472771118\n1\n2016-09-01 16:05:18\nUnited States\n3\n9\n1\n5\n16\n5\n18\ngoogle\norganic\nOrganic Search\n1\n1\n0\n0\n0\n0\n0\n0\n1\nNaN\n0\nSafari\nMacintosh\ndesktop\n0\nFalse\n\n\n62058\n5501176514964856126\n1472799309\n1\n2016-09-01 23:55:09\nUnited States\n3\n9\n1\n5\n23\n55\n9\ngoogle\norganic\nOrganic Search\n12\n0\n2\n4\n0\n0\n108\n4\n8\nNaN\n288\nChrome\nChrome OS\ndesktop\n0\nFalse\n\n\n83797\n5501176514964856126\n1472799309\n1\n2016-09-02 00:00:01\nUnited States\n3\n9\n2\n6\n0\n0\n1\ngoogle\norganic\nOrganic Search\n5\n0\n0\n0\n0\n0\n143\n0\n5\nNaN\n75\nChrome\nChrome OS\ndesktop\n0\nFalse\n\n\n5052\n3718403740052363161\n1472826491\n1\n2016-09-02 07:28:11\nUnited States\n3\n9\n2\n6\n7\n28\n11\ngoogle\norganic\nOrganic Search\n27\n0\n2\n9\n36\n2\n70\n9\n16\nNaN\n196\nSafari\niOS\ntablet\n0\nFalse\n\n\n25190\n0841953213802800334\n1472826491\n1\n2016-09-02 07:28:11\nUnited States\n3\n9\n2\n6\n7\n28\n11\n(direct)\n(none)\nDirect\n83\n0\n3\n15\n45\n0\n361\n20\n59\nNaN\n873\nChrome\nMacintosh\ndesktop\n4\nFalse\n\n\n41174\n8072421534192464916\n1472827968\n1\n2016-09-02 07:52:48\nUnited States\n3\n9\n2\n6\n7\n52\n48\nyoutube.com\nreferral\nSocial\n1\n1\n0\n0\n0\n0\n0\n0\n1\nNaN\n0\nEdge\nWindows\ndesktop\n0\nFalse\n\n\n17780\n3379865943266894087\n1472827968\n1\n2016-09-02 07:52:48\nUnited States\n3\n9\n2\n6\n7\n52\n48\n(direct)\n(none)\nDirect\n1\n1\n0\n0\n0\n0\n0\n0\n1\nNaN\n0\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n78548\n1755792970575692332\n1472830743\n1\n2016-09-02 08:39:03\nUnited States\n3\n9\n2\n6\n8\n39\n3\n(direct)\n(none)\nDirect\n3\n0\n0\n0\n9\n1\n34\n0\n2\nNaN\n23\nChrome\nAndroid\nmobile\n0\nFalse\n\n\n71863\n7256457379544446924\n1472830743\n1\n2016-09-02 08:39:03\nUnited States\n3\n9\n2\n6\n8\n39\n3\n(direct)\n(none)\nDirect\n4\n0\n0\n0\n9\n1\n17\n0\n3\nNaN\n31\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n8965\n0632675186889921612\n1472837248\n1\n2016-09-02 10:27:28\nUnited States\n3\n9\n2\n6\n10\n27\n28\ngoogle\norganic\nOrganic Search\n3\n0\n0\n0\n9\n1\n26\n0\n2\nNaN\n8\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n28879\n3083609601584075244\n1472837248\n1\n2016-09-02 10:27:28\nUnited States\n3\n9\n2\n6\n10\n27\n28\ndfa\ncpm\nDisplay\n1\n1\n0\n0\n0\n0\n0\n0\n1\nNaN\n0\nSafari\niOS\nmobile\n0\nFalse\n\n\n4095\n921700488150606409\n1472840651\n1\n2016-09-02 11:24:11\nUnited States\n3\n9\n2\n6\n11\n24\n11\nanalytics.google.com\nreferral\nReferral\n12\n0\n3\n1\n9\n1\n62\n1\n9\nNaN\n71\nChrome\nWindows\ndesktop\n1\nFalse\n\n\n70680\n0519598949480129245\n1472840651\n1\n2016-09-02 11:24:11\nUnited States\n3\n9\n2\n6\n11\n24\n11\ngoogle\norganic\nOrganic Search\n9\n0\n0\n0\n18\n0\n108\n0\n9\nNaN\n1906\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n10460\n7005010567253535201\n1472845470\n1\n2016-09-02 12:44:30\nUnited States\n3\n9\n2\n6\n12\n44\n30\ngoogle\norganic\nOrganic Search\n1\n1\n0\n0\n0\n0\n10\n0\n1\nNaN\n0\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n18462\n837289177119980105\n1472845470\n1\n2016-09-02 12:44:30\nUnited States\n3\n9\n2\n6\n12\n44\n30\n(direct)\n(none)\nDirect\n1\n1\n0\n0\n9\n0\n0\n0\n1\nNaN\n0\nChrome\nAndroid\nmobile\n0\nFalse\n\n\n48661\n0634421561835257232\n1472930237\n1\n2016-09-03 12:17:17\nUnited States\n3\n9\n3\n7\n12\n17\n17\nyoutube.com\nreferral\nSocial\n4\n0\n2\n1\n9\n0\n31\n1\n3\nNaN\n119\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n88669\n8889451242449489132\n1472930237\n1\n2016-09-03 12:17:17\nUnited States\n3\n9\n3\n7\n12\n17\n17\ngoogle\norganic\nOrganic Search\n1\n1\n0\n0\n9\n0\n0\n0\n1\nNaN\n0\nSafari\niOS\nmobile\n0\nFalse\n\n\n38741\n1935680082667534577\n1472972244\n1\n2016-09-04 00:00:42\nUnited States\n3\n9\n4\n1\n0\n0\n42\ngoogle\norganic\nOrganic Search\n12\n0\n2\n3\n0\n0\n115\n3\n9\nNaN\n203\nChrome\niOS\nmobile\n0\nFalse\n\n\n12240\n1935680082667534577\n1472972244\n1\n2016-09-03 23:57:24\nUnited States\n3\n9\n3\n7\n23\n57\n24\ngoogle\norganic\nOrganic Search\n6\n0\n0\n0\n9\n0\n106\n0\n6\nNaN\n144\nChrome\niOS\nmobile\n0\nFalse\n\n\n9641\n1007310562563797720\n1473058799\n1\n2016-09-05 00:00:01\nUnited States\n3\n9\n5\n2\n0\n0\n1\n(direct)\n(none)\nDirect\n3\n0\n0\n0\n9\n1\n34\n0\n2\nNaN\n52\nSafari (in-app)\niOS\nmobile\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nFor the same visitId, different traffic sources (source, medium, channelGrouping) bring the same or different visitors (fullvisitorid) to the website at the same datetime (visitStartTime). Google Analytics assigns the same visitId to such visitors. There are two types of nested duplicates here\n\nthe same visitor accessing the merchandise store from\n\nmultiple devices at the same time\n\nthis cross-device tracking appears to be allowed by Google Analytics\n\nthe same device and same browser (using separate browser windows after clearing cookies) at the same time\n\nthis is also allowed by Google Analytics\n\n\ndifferent visitors are accessing the merchandise store from multiple devices at the same time\n\nthis is not a duplicated occurrence\nmost likely this corresponds to two distinct visitors who happened to navigate to the site at the same time\n\n\nThere are a negligible number of such duplicates in the dataset.\n\n\n\nDuplicated fullvisitorIds are shown below\n\ndup_visitor_ids = df[df.duplicated(subset=[\"fullvisitorid\"], keep=False)]\nnum_dups = len(df[df.duplicated(subset=[\"fullvisitorid\"], keep=\"first\")])\nprint(\n    f\"Found {num_dups:,} duplicated fullvisitorid out of \"\n    f\"{len(df):,} ({100*num_dups/len(df):.3f}%)\"\n)\nwith pd.option_context(\"display.max_columns\", None):\n    display(dup_visitor_ids.sort_values(by=[\"fullvisitorid\"]).head(25))\n\nFound 308 duplicated fullvisitorid out of 92,859 (0.332%)\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nsource\nmedium\nchannelGrouping\nhits\nbounces\nlast_action\nproduct_detail_views\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntransact_revenue\ntime_on_site\nbrowser\nos\ndeviceCategory\nadded_to_cart\nmade_purchase_on_future_visit\n\n\n\n\n50597\n0014997413479849928\n1477513747\n1\n2016-10-26 13:29:07\nUnited States\n4\n10\n26\n4\n13\n29\n7\n(direct)\n(none)\nDirect\n14\n0\n2\n1\n18\n0\n38\n4\n10\nNaN\n77\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n62473\n0014997413479849928\n1474324872\n1\n2016-09-19 15:41:12\nUnited States\n3\n9\n19\n2\n15\n41\n12\nmall.googleplex.com\nreferral\nReferral\n17\n0\n2\n3\n9\n0\n168\n5\n12\nNaN\n349\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n6720\n012569301201854368\n1477496472\n1\n2016-10-26 08:41:12\nUnited States\n4\n10\n26\n4\n8\n41\n12\nanalytics.google.com\nreferral\nReferral\n1\n1\n0\n0\n9\n0\n0\n0\n1\nNaN\n0\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n32410\n012569301201854368\n1477339547\n1\n2016-10-24 13:05:52\nUnited States\n4\n10\n24\n2\n13\n5\n52\n(direct)\n(none)\nDirect\n2\n0\n0\n0\n9\n0\n10\n0\n2\nNaN\n12\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n11900\n0153393931967124172\n1481528488\n1\n2016-12-11 23:41:28\nUnited States\n4\n12\n11\n1\n23\n41\n28\ngoogle\norganic\nOrganic Search\n5\n0\n0\n0\n9\n0\n15\n0\n5\nNaN\n1105\nSafari\nMacintosh\ndesktop\n0\nFalse\n\n\n62110\n0153393931967124172\n1481528488\n1\n2016-12-12 00:00:03\nUnited States\n4\n12\n12\n2\n0\n0\n3\ngoogle\norganic\nOrganic Search\n13\n0\n2\n1\n9\n0\n92\n2\n11\nNaN\n614\nSafari\nMacintosh\ndesktop\n0\nFalse\n\n\n11590\n0196238382136996118\n1482220536\n1\n2016-12-20 00:00:19\nUnited States\n4\n12\n20\n3\n0\n0\n19\n(direct)\n(none)\nDirect\n4\n0\n6\n0\n0\n0\n0\n0\n4\n165.0\n61\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n15516\n0196238382136996118\n1482220536\n1\n2016-12-19 23:55:36\nUnited States\n4\n12\n19\n2\n23\n55\n36\n(direct)\n(none)\nDirect\n11\n0\n5\n1\n0\n0\n7\n1\n9\nNaN\n99\nChrome\nMacintosh\ndesktop\n1\nFalse\n\n\n72238\n0233922069260074966\n1474926165\n1\n2016-09-26 14:42:45\nUnited States\n3\n9\n26\n2\n14\n42\n45\n(direct)\n(none)\nDirect\n5\n0\n0\n0\n0\n0\n18\n0\n5\nNaN\n869\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n91048\n0233922069260074966\n1477516633\n1\n2016-10-26 14:17:13\nUnited States\n4\n10\n26\n4\n14\n17\n13\nmall.googleplex.com\nreferral\nReferral\n3\n0\n0\n0\n9\n0\n14\n0\n3\nNaN\n46\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n79391\n023880134273808115\n1473836398\n1\n2016-09-14 00:01:08\nUnited States\n3\n9\n14\n4\n0\n1\n8\nmall.googleplex.com\nreferral\nReferral\n3\n0\n0\n0\n0\n1\n68\n0\n2\nNaN\n9\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n29785\n023880134273808115\n1473836398\n1\n2016-09-13 23:59:58\nUnited States\n3\n9\n13\n3\n23\n59\n58\nmall.googleplex.com\nreferral\nReferral\n1\n1\n0\n0\n9\n0\n0\n0\n1\nNaN\n0\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n71200\n0253256708649119169\n1475866302\n1\n2016-10-07 11:51:42\nUnited States\n4\n10\n7\n6\n11\n51\n42\ngoogle\norganic\nOrganic Search\n17\n0\n4\n2\n9\n0\n60\n2\n12\nNaN\n185\nChrome\nMacintosh\ndesktop\n2\nFalse\n\n\n29788\n0253256708649119169\n1477495482\n1\n2016-10-26 08:24:42\nUnited States\n4\n10\n26\n4\n8\n24\n42\nmall.googleplex.com\nreferral\nReferral\n1\n1\n0\n0\n9\n0\n0\n0\n1\nNaN\n0\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n14576\n0273626426804732878\n1477523531\n1\n2016-10-26 16:12:11\nUnited States\n4\n10\n26\n4\n16\n12\n11\nmall.googleplex.com\nreferral\nReferral\n6\n0\n0\n0\n9\n0\n32\n0\n6\nNaN\n71\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n27661\n0273626426804732878\n1475625049\n1\n2016-10-04 16:50:49\nUnited States\n4\n10\n4\n3\n16\n50\n49\nmall.googleplex.com\nreferral\nReferral\n17\n0\n0\n0\n27\n1\n101\n0\n16\nNaN\n279\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n68152\n0495389417785084319\n1477460221\n1\n2016-10-25 22:37:01\nUnited States\n4\n10\n25\n3\n22\n37\n1\n(direct)\n(none)\nDirect\n1\n1\n0\n0\n0\n0\n12\n0\n1\nNaN\n0\nChrome\nAndroid\nmobile\n0\nFalse\n\n\n16918\n0495389417785084319\n1476424324\n1\n2016-10-13 22:52:04\nUnited States\n4\n10\n13\n5\n22\n52\n4\ngoogle\norganic\nOrganic Search\n45\n0\n3\n2\n54\n4\n108\n3\n34\nNaN\n2644\nChrome\nAndroid\nmobile\n4\nFalse\n\n\n44564\n0510616407042133197\n1477499309\n1\n2016-10-26 09:28:29\nUnited States\n4\n10\n26\n4\n9\n28\n29\nsites.google.com\nreferral\nReferral\n3\n0\n0\n0\n9\n0\n15\n0\n3\nNaN\n20\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n16656\n0510616407042133197\n1473965199\n1\n2016-09-15 11:46:39\nUnited States\n3\n9\n15\n5\n11\n46\n39\nmall.googleplex.com\nreferral\nReferral\n28\n0\n3\n3\n36\n0\n36\n3\n23\nNaN\n1183\nChrome\nMacintosh\ndesktop\n2\nFalse\n\n\n84961\n0567348342496807208\n1480060521\n1\n2016-11-25 00:08:08\nUnited States\n4\n11\n25\n6\n0\n8\n8\ngoogle\norganic\nOrganic Search\n9\n0\n2\n3\n0\n0\n21\n3\n6\nNaN\n103\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n92691\n0567348342496807208\n1480060521\n1\n2016-11-24 23:55:21\nUnited States\n4\n11\n24\n5\n23\n55\n21\ngoogle\norganic\nOrganic Search\n2\n0\n0\n0\n0\n0\n24\n0\n2\nNaN\n66\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n32159\n0611144195339874223\n1477810729\n1\n2016-10-30 00:00:13\nUnited States\n4\n10\n30\n1\n0\n0\n13\nmall.googleplex.com\nreferral\nReferral\n3\n0\n0\n0\n18\n0\n10\n0\n3\nNaN\n65\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n83274\n0611144195339874223\n1477810729\n1\n2016-10-29 23:58:49\nUnited States\n4\n10\n29\n7\n23\n58\n49\nmall.googleplex.com\nreferral\nReferral\n3\n0\n0\n0\n18\n0\n10\n0\n3\nNaN\n33\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n33391\n0707913578847667979\n1480233588\n1\n2016-11-26 23:59:48\nUnited States\n4\n11\n26\n7\n23\n59\n48\n(direct)\n(none)\nDirect\n1\n1\n0\n0\n0\n0\n12\n0\n1\nNaN\n0\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nFor the same fullvisitorid, different traffic sources (source, medium, channelGrouping) bring the same visitor (fullvisitorid) to the website at the different datetimes (visitStartTimes) from the same device (browser, os, deviceCategory). There are two types of nested duplicates here\n\nthe same visit by the same visitor with a &lt;30 minute period of inactivity between duplicates\n\n(by default) Google Analytics allows up to 30 minutes of inactivity before starting a new visit, so it is not clear why such a short period of inactivity should start a new instance of the same visit instead of just accumulating stats into the same instance\n\na different visit by the same visitor with a &gt;30 minute period of inactivity between duplicates\n\nit is also not clear why these duplicates are present in the data\n\n\nSince\n\nthe use-case for this project requires attributes of only the first visit (per visitor) to be used to predict the probability of a purchase during a future visit by the same visitor\nit is not clear why this type of duplicated record is present in the prepared data\n\nwe will want to drop this type of duplicate from the training, validation and test splits of the prepared data (we’re assuming that the same type of problem can occur throughout the dataset and not just in the training data).\nThere are a negligible number of such duplicates in the dataset.\n\n\n\nWith this in mind, columns with duplicates in the fullvisitorid column are dropped. This will be done using Python\n\ndf = df.drop_duplicates(subset=[\"fullvisitorid\"], keep=\"first\")\n\n\n\nNested Promotion Column\nQuestion 5. Show and comment on unique values in the nested promotion column.\nThe number of promotions and products displayed (impressions) and clicked are shown below\n\nfor c in [\n    \"promos_displayed\",\n    \"promos_clicked\",\n    \"product_views\",\n    \"product_clicks\",\n    \"product_detail_views\",\n]:\n    df_num_visitor_counts = (\n        df[c]\n        .value_counts(dropna=False)\n        .rename(\"num_visitors\")\n        .reset_index()\n        .rename(columns={\"index\": f\"num_{c}\"})\n    )\n    assert (\n        type(df_num_visitor_counts.query(\"num_visitors == 0\")[c].squeeze()).__name__\n        == \"NAType\"\n    )\n    display(df_num_visitor_counts.query(\"num_visitors &gt; 0\"))\n\n\n\n\n\n\n\n\npromos_displayed\nnum_visitors\n\n\n\n\n0\n9\n42840\n\n\n1\n0\n28655\n\n\n2\n18\n12938\n\n\n3\n27\n4079\n\n\n4\n36\n1729\n\n\n5\n45\n875\n\n\n6\n54\n426\n\n\n7\n13\n251\n\n\n8\n63\n242\n\n\n9\n72\n144\n\n\n10\n81\n74\n\n\n11\n90\n58\n\n\n12\n26\n54\n\n\n13\n99\n30\n\n\n14\n117\n28\n\n\n15\n108\n22\n\n\n16\n39\n17\n\n\n17\n135\n14\n\n\n18\n126\n13\n\n\n19\n153\n9\n\n\n20\n198\n8\n\n\n21\n65\n6\n\n\n22\n52\n5\n\n\n23\n144\n4\n\n\n24\n171\n3\n\n\n25\n180\n3\n\n\n26\n162\n2\n\n\n27\n252\n2\n\n\n28\n342\n1\n\n\n29\n216\n1\n\n\n30\n765\n1\n\n\n31\n432\n1\n\n\n32\n12\n1\n\n\n33\n711\n1\n\n\n34\n31\n1\n\n\n35\n143\n1\n\n\n36\n189\n1\n\n\n37\n50\n1\n\n\n38\n130\n1\n\n\n39\n279\n1\n\n\n40\n22\n1\n\n\n41\n225\n1\n\n\n42\n207\n1\n\n\n43\n297\n1\n\n\n44\n234\n1\n\n\n45\n522\n1\n\n\n46\n315\n1\n\n\n47\n49\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npromos_clicked\nnum_visitors\n\n\n\n\n0\n0\n78328\n\n\n1\n1\n10802\n\n\n2\n2\n2019\n\n\n3\n3\n752\n\n\n4\n4\n318\n\n\n5\n5\n145\n\n\n6\n6\n76\n\n\n7\n7\n47\n\n\n8\n8\n20\n\n\n9\n9\n13\n\n\n10\n10\n8\n\n\n11\n11\n6\n\n\n12\n14\n3\n\n\n13\n12\n2\n\n\n14\n16\n2\n\n\n15\n17\n2\n\n\n16\n34\n2\n\n\n17\n30\n1\n\n\n18\n21\n1\n\n\n19\n23\n1\n\n\n20\n19\n1\n\n\n21\n13\n1\n\n\n22\n15\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_views\nnum_visitors\n\n\n\n\n0\n0\n26848\n\n\n1\n12\n14273\n\n\n2\n24\n4663\n\n\n3\n36\n2343\n\n\n4\n7\n2271\n\n\n...\n...\n...\n\n\n602\n366\n1\n\n\n603\n685\n1\n\n\n604\n647\n1\n\n\n605\n719\n1\n\n\n606\n577\n1\n\n\n\n\n607 rows × 2 columns\n\n\n\n\n\n\n\n\n\n\nproduct_clicks\nnum_visitors\n\n\n\n\n0\n0\n68175\n\n\n1\n1\n8680\n\n\n2\n2\n5455\n\n\n3\n3\n3056\n\n\n4\n4\n2036\n\n\n...\n...\n...\n\n\n62\n42\n1\n\n\n63\n125\n1\n\n\n64\n71\n1\n\n\n65\n48\n1\n\n\n66\n61\n1\n\n\n\n\n67 rows × 2 columns\n\n\n\n\n\n\n\n\n\n\nproduct_detail_views\nnum_visitors\n\n\n\n\n0\n0\n68220\n\n\n1\n1\n10547\n\n\n2\n2\n5197\n\n\n3\n3\n2928\n\n\n4\n4\n1734\n\n\n...\n...\n...\n\n\n56\n41\n1\n\n\n57\n82\n1\n\n\n58\n31\n1\n\n\n59\n122\n1\n\n\n60\n71\n1\n\n\n\n\n61 rows × 2 columns\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nProduct views are the number of times a product was seen while in a list of other products (eg. on a product listing page or in a product category page).\nProduct clicks are the number of times a product was clicked on after being viewed.\nProduct detail views are the number of times a visitor has visited a product’s page (not just viewed its details as part of a product listing).\n\na visitor might have viewed product details page for products that are\n\npart of a product listing\nnot part of a product listing\n\n\nSimilar logic applies to promotions (eg. banners) views and clicks.\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nProducts and promotions on the merchandise store’s website on the Google Marketplace are not being\n\nviewed (as part of a listing or in detail)\nclicked\n\noften.\n\n\n\nPromotion-related columns are flattened and shown (see promotionActionInfo) for a single visit\n\nquery = f\"\"\"\n        WITH visit_promotion_attrs AS (\n            SELECT fullvisitorid,\n                   visitId,\n                   visitNumber,\n                   DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific') AS visitStartTime,\n                   CAST(h.ecommerceaction.action_type AS INT64) AS action_type,\n                   h.promotion,\n                   h.promotionActionInfo AS pa_info,\n                   trafficSource.source,\n                   trafficSource.medium,\n                   channelGrouping,\n                   device.browser,\n                   device.operatingSystem,\n                   device.deviceCategory\n            FROM `data-to-insights.ecommerce.web_analytics`,\n            UNNEST(hits) AS h\n            WHERE visitId = 1476880065  -- 1476880065, 1478579523, 1474972357, 1478844153\n            AND geoNetwork.country = 'United States'\n        )\n        SELECT * EXCEPT(promotion, promoId, pa_info, visitStartTime),\n               pa_info,\n               CASE WHEN pa_info IS NOT NULL THEN pa_info.promoIsView ELSE NULL END AS view_promo,\n               CASE WHEN pa_info IS NOT NULL THEN pa_info.promoIsClick ELSE NULL END AS click_promo\n        FROM visit_promotion_attrs\n        LEFT JOIN UNNEST(promotion) as p\n        \"\"\"\ndf_raw = run_sql_query(query, **gcp_auth_dict, show_df=False)\ndf_raw[\"action_type\"] = df_raw[\"action_type\"].map(mapper)\nwith pd.option_context(\"display.max_colwidth\", None, \"display.max_rows\", None):\n    display(df_raw.head(10))\n\nQuery execution start time = 2023-04-13 17:07:00.571...done at 2023-04-13 17:07:02.254 (1.683 seconds).\nQuery returned 42 rows\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\naction_type\nsource\nmedium\nchannelGrouping\nbrowser\noperatingSystem\ndeviceCategory\npromoName\npromoCreative\npromoPosition\npa_info\nview_promo\nclick_promo\n\n\n\n\n0\n3072592563711482446\n1476880065\n1\nUnknown\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\nNone\nNone\nNone\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n1\n3072592563711482446\n1476880065\n1\nUnknown\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\nNone\nNone\nNone\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n2\n3072592563711482446\n1476880065\n1\nUnknown\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\nNone\nNone\nNone\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n3\n3072592563711482446\n1476880065\n1\nUnknown\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\nNone\nNone\nNone\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n4\n3072592563711482446\n1476880065\n1\nUnknown\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\nApparel\nhome_main_link_apparel.jpg\nRow 1\n{'promoIsView': True, 'promoIsClick': None}\nTrue\n&lt;NA&gt;\n\n\n5\n3072592563711482446\n1476880065\n1\nUnknown\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\nBackpacks\nhome_bags_google_2.jpg\nRow 2 Combo\n{'promoIsView': True, 'promoIsClick': None}\nTrue\n&lt;NA&gt;\n\n\n6\n3072592563711482446\n1476880065\n1\nUnknown\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\nMens T-Shirts\nmens-tshirts.jpg\nRow 3-1\n{'promoIsView': True, 'promoIsClick': None}\nTrue\n&lt;NA&gt;\n\n\n7\n3072592563711482446\n1476880065\n1\nUnknown\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\nWomens T-Shirts\nwomens-tshirts.jpg\nRow 3-2\n{'promoIsView': True, 'promoIsClick': None}\nTrue\n&lt;NA&gt;\n\n\n8\n3072592563711482446\n1476880065\n1\nUnknown\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\nOffice\ngreen_row_link_to_office.jpg\nRow 5 Color Combo\n{'promoIsView': True, 'promoIsClick': None}\nTrue\n&lt;NA&gt;\n\n\n9\n3072592563711482446\n1476880065\n1\nUnknown\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\nDrinkware\nred_row_hydrate.jpg\nRow 4 Color Combo\n{'promoIsView': True, 'promoIsClick': None}\nTrue\n&lt;NA&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\npromotionActionInfo contains information about visitor views and clicks.\nThe CASE WHEN was constructed for both view_promo and click_promo columns based on the nested promotionActionInfo (mapped to pa_info) column.\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nWhen view_promo = True, a visitor has viewed a promotion. If it is not viewed, then it is NULL.\nWhen a visitor clicks a promotion after viewing it\n\nclick_promo = True\nview_promo is NULL\n\nthis prevents double-counting a promotion that is both viewed and clicked\n\n\n\n\n\n\n\nNested Product Column\nQuestion 6. Show and comment on unique values in the nested product column.\nProduct-related columns are flattened and shown for a single visit\n\nquery = f\"\"\"\n        WITH visit_product_attrs AS (\n            SELECT fullvisitorid,\n               visitId,\n               visitNumber,\n               DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific') AS visitStartTime,\n               CAST(h.ecommerceaction.action_type AS INT64) AS action_type,\n               h.product,\n               (CASE WHEN ARRAY_LENGTH(h.product) = 0 THEN 0 ELSE ARRAY_LENGTH(h.product) END) AS product_count,\n               (CASE WHEN CAST(h.eCommerceAction.action_type AS INT64) = 2 THEN 1 ELSE 0 END) AS product_details_viewed,\n               trafficSource.source,\n               trafficSource.medium,\n               channelGrouping,\n               device.browser,\n               device.operatingSystem,\n               device.deviceCategory\n            FROM `data-to-insights.ecommerce.web_analytics`,\n            UNNEST(hits) AS h\n            WHERE visitId = 1478579523  -- 1478579523, 1474972357\n            AND geoNetwork.country = 'United States'\n        )\n        SELECT *,\n               p.isImpression AS viewed_product,\n               p.isClick AS clicked_product\n        FROM visit_product_attrs\n        LEFT JOIN UNNEST(product) as p\n        \"\"\"\ndf_raw = run_sql_query(query, **gcp_auth_dict, show_df=False)\ndf_raw[\"action_type\"] = df_raw[\"action_type\"].map(mapper)\nwith pd.option_context(\"display.max_columns\", None):\n    display(df_raw.head())\n    display(df_raw.tail())\n\nQuery execution start time = 2023-04-13 17:07:08.966...done at 2023-04-13 17:07:11.872 (2.906 seconds).\nQuery returned 266 rows\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\naction_type\nproduct\nproduct_count\nproduct_details_viewed\nsource\nmedium\nchannelGrouping\nbrowser\noperatingSystem\ndeviceCategory\nproductSKU\nv2ProductName\nv2ProductCategory\nproductVariant\nproductBrand\nproductRevenue\nlocalProductRevenue\nproductPrice\nlocalProductPrice\nproductQuantity\nproductRefundAmount\nlocalProductRefundAmount\nisImpression\nisClick\ncustomDimensions\ncustomMetrics\nproductListName\nproductListPosition\nviewed_product\nclicked_product\n\n\n\n\n0\n7270403007208566857\n1478579523\n1\n2016-11-07 20:32:03\nUnknown\n[]\n0\n0\nsiliconvalley.about.com\nreferral\nReferral\nChrome\nChrome OS\ndesktop\nNone\nNone\nNone\nNone\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n[]\n[]\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n1\n7270403007208566857\n1478579523\n1\n2016-11-07 20:32:03\nUnknown\n[]\n0\n0\nsiliconvalley.about.com\nreferral\nReferral\nChrome\nChrome OS\ndesktop\nNone\nNone\nNone\nNone\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n[]\n[]\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n2\n7270403007208566857\n1478579523\n1\n2016-11-07 20:32:03\nUnknown\n[]\n0\n0\nsiliconvalley.about.com\nreferral\nReferral\nChrome\nChrome OS\ndesktop\nNone\nNone\nNone\nNone\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n[]\n[]\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n3\n7270403007208566857\n1478579523\n1\n2016-11-07 20:32:03\nUnknown\n[]\n0\n0\nsiliconvalley.about.com\nreferral\nReferral\nChrome\nChrome OS\ndesktop\nNone\nNone\nNone\nNone\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n[]\n[]\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n4\n7270403007208566857\n1478579523\n1\n2016-11-07 20:32:03\nUnknown\n[]\n0\n0\nsiliconvalley.about.com\nreferral\nReferral\nChrome\nChrome OS\ndesktop\nNone\nNone\nNone\nNone\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n[]\n[]\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\naction_type\nproduct\nproduct_count\nproduct_details_viewed\nsource\nmedium\nchannelGrouping\nbrowser\noperatingSystem\ndeviceCategory\nproductSKU\nv2ProductName\nv2ProductCategory\nproductVariant\nproductBrand\nproductRevenue\nlocalProductRevenue\nproductPrice\nlocalProductPrice\nproductQuantity\nproductRefundAmount\nlocalProductRefundAmount\nisImpression\nisClick\ncustomDimensions\ncustomMetrics\nproductListName\nproductListPosition\nviewed_product\nclicked_product\n\n\n\n\n261\n7270403007208566857\n1478579523\n1\n2016-11-07 20:32:03\nUnknown\n[{'productSKU': 'GGOEGAAX0318', 'v2ProductName...\n12\n0\nsiliconvalley.about.com\nreferral\nReferral\nChrome\nChrome OS\ndesktop\nGGOEGAAX0686\nYouTube Youth Short Sleeve Tee Red\nHome/Shop by Brand/YouTube/\n(not set)\n(not set)\n&lt;NA&gt;\n&lt;NA&gt;\n18990000\n18990000\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\n[]\n[]\nCategory\n9\nTrue\n&lt;NA&gt;\n\n\n262\n7270403007208566857\n1478579523\n1\n2016-11-07 20:32:03\nUnknown\n[{'productSKU': 'GGOEGAAX0318', 'v2ProductName...\n12\n0\nsiliconvalley.about.com\nreferral\nReferral\nChrome\nChrome OS\ndesktop\nGGOEYHPA003510\nYouTube Trucker Hat\nHome/Shop by Brand/YouTube/\n(not set)\n(not set)\n&lt;NA&gt;\n&lt;NA&gt;\n21990000\n21990000\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\n[]\n[]\nCategory\n10\nTrue\n&lt;NA&gt;\n\n\n263\n7270403007208566857\n1478579523\n1\n2016-11-07 20:32:03\nUnknown\n[{'productSKU': 'GGOEGAAX0318', 'v2ProductName...\n12\n0\nsiliconvalley.about.com\nreferral\nReferral\nChrome\nChrome OS\ndesktop\nGGOEGAAX0330\nYouTube Men's Skater Tee Charcoal\nHome/Shop by Brand/YouTube/\n(not set)\n(not set)\n&lt;NA&gt;\n&lt;NA&gt;\n19990000\n19990000\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\n[]\n[]\nCategory\n11\nTrue\n&lt;NA&gt;\n\n\n264\n7270403007208566857\n1478579523\n1\n2016-11-07 20:32:03\nUnknown\n[{'productSKU': 'GGOEGAAX0318', 'v2ProductName...\n12\n0\nsiliconvalley.about.com\nreferral\nReferral\nChrome\nChrome OS\ndesktop\nGGOEYDHJ056099\n22 oz YouTube Bottle Infuser\nHome/Shop by Brand/YouTube/\n(not set)\n(not set)\n&lt;NA&gt;\n&lt;NA&gt;\n4990000\n4990000\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\n[]\n[]\nCategory\n12\nTrue\n&lt;NA&gt;\n\n\n265\n7270403007208566857\n1478579523\n1\n2016-11-07 20:32:03\nUnknown\n[]\n0\n0\nsiliconvalley.about.com\nreferral\nReferral\nChrome\nChrome OS\ndesktop\nNone\nNone\nNone\nNone\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n[]\n[]\nNone\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThe product column is a nested column with a multi-element array. Each element (dictionary) of the array corresponds to a different product in a listing or category of products. With the BigQuery UNNEST() function, this column is exploded into the following standalone columns\n\nproductSKU\nv2ProductName\nv2ProductCategory\nproductVariant\nproductBrand\nproductRevenue\nlocalProductRevenue\nproductPrice\nlocalProductPrice\nproductQuantity\nproductRefundAmount\nlocalProductRefundAmount\nisImpression\nisClick\ncustomDimensions\ncustomMetrics\nproductListName\nproductListPosition\nviewed_product\nclicked_product\n\nviewed_product is True for every product in the product listing that was viewed.\n\n\n\nIf a product is viewed in a listing (product_count &gt; 0) during a visit, then there are only two possible values for clicked_product and viewed_product, as shown below\n\nfor c in [\"viewed_product\", \"clicked_product\"]:\n    # show unique values\n    display(\n        df_raw.query(\"product_count &gt; 0\")[c].value_counts(dropna=False).reset_index()\n    )\n\n    # verify that False is not a unique value\n    assert df_raw.query(\"product_count &gt; 0\").query(f\"{c} == False\").empty\n\n\n\n\n\n\n\n\nviewed_product\ncount\n\n\n\n\n0\nTrue\n189\n\n\n1\n&lt;NA&gt;\n41\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclicked_product\ncount\n\n\n\n\n0\n&lt;NA&gt;\n213\n\n\n1\nTrue\n17\n\n\n\n\n\n\n\nIf a product is not viewed in a listing, then the only value in these same two columns is NULL since they come from a nested column product which contains an empty array [] if a product is such a scenario.\nThis is shown below\n\nfor c in [\"viewed_product\", \"clicked_product\"]:\n    display(\n        df_raw.query(\"product_count == 0\")[c].value_counts(dropna=False).reset_index()\n    )\n\n\n\n\n\n\n\n\nviewed_product\ncount\n\n\n\n\n0\n&lt;NA&gt;\n36\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclicked_product\ncount\n\n\n\n\n0\n&lt;NA&gt;\n36\n\n\n\n\n\n\n\nFor every product in the product listing that was viewed and clicked\n\nclicked_product is True\nviewed_product is NULL\n\nwhich prevents double-counting products that are both viewed and clicked (similar to for promotions), as shown below\n\n\nCode\ndisplay(df_raw.query(\"clicked_product == True\")[[\"viewed_product\", \"clicked_product\"]])\n\n\n\n\n\n\n\n\n\nviewed_product\nclicked_product\n\n\n\n\n20\n&lt;NA&gt;\nTrue\n\n\n23\n&lt;NA&gt;\nTrue\n\n\n42\n&lt;NA&gt;\nTrue\n\n\n44\n&lt;NA&gt;\nTrue\n\n\n75\n&lt;NA&gt;\nTrue\n\n\n140\n&lt;NA&gt;\nTrue\n\n\n149\n&lt;NA&gt;\nTrue\n\n\n152\n&lt;NA&gt;\nTrue\n\n\n155\n&lt;NA&gt;\nTrue\n\n\n158\n&lt;NA&gt;\nTrue\n\n\n161\n&lt;NA&gt;\nTrue\n\n\n173\n&lt;NA&gt;\nTrue\n\n\n176\n&lt;NA&gt;\nTrue\n\n\n187\n&lt;NA&gt;\nTrue\n\n\n190\n&lt;NA&gt;\nTrue\n\n\n223\n&lt;NA&gt;\nTrue\n\n\n224\n&lt;NA&gt;\nTrue\n\n\n\n\n\n\n\nProduct detail views and clicking of products that were viewed in a product or product category listing can also be retrieved from the action_type column, which tracks each action performed by a visitor during a visit. Its unique values are shown below\n\n\nCode\ndf_raw[\"action_type\"].value_counts(dropna=False).reset_index()\n\n\n\n\n\n\n\n\n\naction_type\ncount\n\n\n\n\n0\nUnknown\n225\n\n\n1\nProduct detail views\n22\n\n\n2\nClick through of product lists\n17\n\n\n3\nAdd product(s) to cart\n2\n\n\n\n\n\n\n\nBelow, we verify that the products that were clicked can be equivalently determined using separated nested columns\n\nclicked_product (extracted from nested column product)\n\nclicked_product == True\n\naction_type (extracted from nested column hits)\n\naction_type == 'Click through of product lists'\n\n\n\nvisit_prodict_view_click_cols = [\n    \"fullvisitorid\",\n    \"visitStartTime\",\n    \"action_type\",\n    \"product_details_viewed\",\n    \"isImpression\",\n    \"isClick\",\n    \"viewed_product\",\n    \"clicked_product\",\n]\nassert df_raw.query(\"clicked_product == True\")[visit_prodict_view_click_cols].equals(\n    df_raw.query(\"action_type == 'Click through of product lists'\")[\n        visit_prodict_view_click_cols\n    ]\n)\n\nWhen a product is viewed in a listing (viewed_product), during a visit, the product count for those visits is greater than zero\n\nassert df_raw.query(\"viewed_product == True\")[\"product_count\"].min() &gt; 0\ndisplay(df_raw.query(\"viewed_product == True\")[\"product_count\"].describe().to_frame())\n\n\n\n\n\n\n\n\nproduct_count\n\n\n\n\ncount\n189.0\n\n\nmean\n9.719577\n\n\nstd\n3.355004\n\n\nmin\n2.0\n\n\n25%\n6.0\n\n\n50%\n12.0\n\n\n75%\n12.0\n\n\nmax\n12.0\n\n\n\n\n\n\n\nFor informational purposes, the raw dataset without unnesting the products and promotions columns is shown below for a small number of visits\n\nvisit_ids_dict = {\n    1478844153: \"papayawhip\",\n    1476880065: \"mistyrose\",\n    1478579523: \"lavender\",\n    1474972357: \"lightcyan\",\n}\nvisit_ids_str = \"(\" + \", \".join([str(v) for v in list(visit_ids_dict)]) + \")\"\n\nAttributes for these visits are retrieved below without unnesting product and promotion\n\nquery = f\"\"\"\n        WITH visit_promotion_attrs AS (\n            SELECT fullvisitorid,\n                   visitId,\n                   visitNumber,\n                   DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific') AS visitStartTime,\n                   CAST(h.ecommerceaction.action_type AS INT64) AS action_type,\n                   (CASE WHEN CAST(h.eCommerceAction.action_type AS INT64) = 2 THEN 1 ELSE 0 END) AS product_details_viewed,\n                   trafficSource.source,\n                   trafficSource.medium,\n                   channelGrouping,\n                   device.browser,\n                   device.operatingSystem,\n                   device.deviceCategory,\n                   -- visit\n                   totals.timeOnSite,\n                   totals.timeOnScreen,\n                   totals.visits,\n                   totals.totalTransactionRevenue / 1000000 AS transact_revenue,\n                   -- nested columns\n                   h.product,\n                   h.promotion,\n                   -- experimental columns that were not used\n                   h.isInteraction,\n                   trafficSource.campaign,\n                   trafficSource.isTrueDirect,\n            FROM `data-to-insights.ecommerce.web_analytics`,\n            UNNEST(hits) AS h\n            WHERE visitId IN {visit_ids_str}\n        )\n        SELECT * EXCEPT(visitStartTime)\n        FROM visit_promotion_attrs\n        \"\"\"\ndf_raw = run_sql_query(query, **gcp_auth_dict, show_df=False)\ndf_raw[\"action_type\"] = df_raw[\"action_type\"].map(mapper)\n\nQuery execution start time = 2023-04-13 17:07:43.363...done at 2023-04-13 17:07:46.085 (2.722 seconds).\nQuery returned 143 rows\n\n\nThese attributes can be shown per visitId using\nfor visit_id in list(visit_ids_dict):\n    with pd.option_context(\"display.max_columns\", None, \"display.max_rows\", None):\n        display(df_raw.query(f\"visitId == {visit_id}\"))\n\n\nChange Data Types in Prepared Data\n\ndtypes_dict = {\n    \"fullvisitorid\": pd.StringDtype(),\n    \"visitId\": pd.StringDtype(),\n    \"visitNumber\": pd.Int8Dtype(),\n    \"country\": pd.StringDtype(),\n    \"quarter\": pd.Int8Dtype(),\n    \"month\": pd.Int8Dtype(),\n    \"day_of_month\": pd.Int8Dtype(),\n    \"day_of_week\": pd.Int8Dtype(),\n    \"hour\": pd.Int8Dtype(),\n    \"minute\": pd.Int8Dtype(),\n    \"second\": pd.Int8Dtype(),\n    \"source\": pd.StringDtype(),\n    \"medium\": pd.StringDtype(),\n    \"channelGrouping\": pd.StringDtype(),\n    \"hits\": pd.Int16Dtype(),\n    \"bounces\": pd.Int16Dtype(),\n    \"last_action\": pd.Int8Dtype(),\n    \"product_detail_views\": pd.Int16Dtype(),\n    \"promos_displayed\": pd.Int16Dtype(),\n    \"promos_clicked\": pd.Int16Dtype(),\n    \"product_views\": pd.Int16Dtype(),\n    \"product_clicks\": pd.Int16Dtype(),\n    \"pageviews\": pd.Int16Dtype(),\n    \"transact_revenue\": pd.Float32Dtype(),\n    \"time_on_site\": pd.Int16Dtype(),\n    \"browser\": pd.StringDtype(),\n    \"os\": pd.StringDtype(),\n    \"added_to_cart\": pd.Int16Dtype(),\n    \"deviceCategory\": pd.StringDtype(),\n}\n\n\ndf = df.astype(dtypes_dict)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 92551 entries, 0 to 92858\nData columns (total 31 columns):\n #   Column                         Non-Null Count  Dtype         \n---  ------                         --------------  -----         \n 0   fullvisitorid                  92551 non-null  string        \n 1   visitId                        92551 non-null  string        \n 2   visitNumber                    92551 non-null  Int8          \n 3   visitStartTime                 92551 non-null  datetime64[ns]\n 4   country                        92551 non-null  string        \n 5   quarter                        92551 non-null  Int8          \n 6   month                          92551 non-null  Int8          \n 7   day_of_month                   92551 non-null  Int8          \n 8   day_of_week                    92551 non-null  Int8          \n 9   hour                           92551 non-null  Int8          \n 10  minute                         92551 non-null  Int8          \n 11  second                         92551 non-null  Int8          \n 12  source                         92551 non-null  string        \n 13  medium                         92551 non-null  string        \n 14  channelGrouping                92551 non-null  string        \n 15  hits                           92551 non-null  Int16         \n 16  bounces                        92551 non-null  Int16         \n 17  last_action                    92551 non-null  Int8          \n 18  product_detail_views           92551 non-null  Int16         \n 19  promos_displayed               92551 non-null  Int16         \n 20  promos_clicked                 92551 non-null  Int16         \n 21  product_views                  92551 non-null  Int16         \n 22  product_clicks                 92551 non-null  Int16         \n 23  pageviews                      92551 non-null  Int16         \n 24  transact_revenue               3094 non-null   Float32       \n 25  time_on_site                   92551 non-null  Int16         \n 26  browser                        92551 non-null  string        \n 27  os                             92551 non-null  string        \n 28  deviceCategory                 92551 non-null  string        \n 29  added_to_cart                  92551 non-null  Int16         \n 30  made_purchase_on_future_visit  92551 non-null  boolean       \ndtypes: Float32(1), Int16(10), Int8(9), boolean(1), datetime64[ns](1), string(9)\nmemory usage: 12.6 MB\n\n\n\n\nSeparate Columns by Type\nThe first three rows of the prepared data are shown below\n\n\nCode\nwith pd.option_context(\n    \"display.max_colwidth\", None, \"display.max_rows\", None, \"display.max_columns\", None\n):\n    display(df.head(3))\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nsource\nmedium\nchannelGrouping\nhits\nbounces\nlast_action\nproduct_detail_views\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntransact_revenue\ntime_on_site\nbrowser\nos\ndeviceCategory\nadded_to_cart\nmade_purchase_on_future_visit\n\n\n\n\n0\n8163735676529750721\n1481176805\n1\n2016-12-07 22:00:05\nUnited States\n4\n12\n7\n4\n22\n0\n5\ngoogle\ncpc\nPaid Search\n5\n0\n0\n0\n0\n0\n0\n0\n5\n&lt;NA&gt;\n109\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n1\n0897634596694862660\n1475199828\n1\n2016-09-29 18:43:48\nUnited States\n3\n9\n29\n5\n18\n43\n48\ngoogle\norganic\nOrganic Search\n21\n0\n2\n2\n9\n0\n90\n2\n19\n&lt;NA&gt;\n467\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n2\n3891893273235584028\n1478844153\n1\n2016-11-10 22:02:33\nUnited States\n4\n11\n10\n5\n22\n2\n33\nyoutube.com\nreferral\nSocial\n11\n0\n2\n1\n18\n0\n36\n1\n10\n&lt;NA&gt;\n1003\nSafari (in-app)\niOS\ntablet\n0\nFalse\n\n\n\n\n\n\n\nCreate lists of columns based on their type. Three such lists are shown below\n\ndatetime\ncategorical\nnumerical\n\n\ndatetime_columns = [\n    \"quarter\",\n    \"month\",\n    \"day_of_month\",\n    \"day_of_week\",\n    \"hour\",\n]\ncategorical_columns = [\n    \"bounces\",\n    \"last_action\",\n    \"source\",\n    \"medium\",\n    \"channelGrouping\",\n    \"browser\",\n    \"os\",\n    \"deviceCategory\",\n]\nnumerical_columns = [\n    \"hits\",\n    \"product_detail_views\",\n    \"promos_displayed\",\n    \"promos_clicked\",\n    \"product_views\",\n    \"product_clicks\",\n    \"pageviews\",\n    \"time_on_site\",\n    \"added_to_cart\",\n]\n\n\n\nHandling Categorical Columns\nHigh-cardinality categorical features are a problem for machine learning models as they create a large number of dummy variables (after dummy encoding), or a sparse matrix (1, 2) that slows ML model training. So, it is frequently necessary to reduce this cardinality before training a ML model.\nTwo of the well-known apprroaches to reduce dimensionality of such features are (1, 2)\n\nfrequency endoding\n\nonly keep the N most common values for each feature and replace all the other (infrequently occurring) values with a placeholder value such as other\nthis will be the approach used for the current project\n\nclass label or target encoding\n\ngroup categorical features by the class labels (the dependent variable, or y)\n\n\nReducing the cardinality of such features is performed during the data transformation step of a ML workflow. Here, we will demonstrate this before exploratory data analysis (this step) and then apply it during data transformation (next step).\nShow the number of unique values in all categorical columns\n\ndf_nunique = pd.DataFrame.from_records(\n    [{\"column\": c, \"num_unique_values\": df[c].nunique()} for c in categorical_columns]\n)\ndf_nunique\n\n\n\n\n\n\n\n\ncolumn\nnum_unique_values\n\n\n\n\n0\nbounces\n2\n\n\n1\nlast_action\n7\n\n\n2\nsource\n116\n\n\n3\nmedium\n7\n\n\n4\nchannelGrouping\n8\n\n\n5\nbrowser\n26\n\n\n6\nos\n15\n\n\n7\ndeviceCategory\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nHigh-cardinality categorical columns are present in the training data.\nThe following categorical columns are high-cardinality columns with the largest number of unique values\n\nsource (source of visitor traffic reaching the merchandise store’s website)\nbrowser\nos (visitor’s operating system used to access merchandise store’s website)\n\nand will likely need to be binned or grouped\nInfrequently occurring values in the following medium-cardinality columns related the source of website visitor traffic will also be grouped\n\nchannelGrouping\nmedium\n\nlast_action will be left unchanged\n\nit is likely that the last action performed by a visitor during their first visit to the merchandise store will have some influence on their probability (propensity) to make a purchase during a future visit\n\n\n\n\nThe category distributions (frequencies) after grouping are shown below for all categorical columns (including those that were grouped)\n\ndfs_cats_groups = []\nfor c in categorical_columns:\n    # get fraction of unique values\n    df_frequencies = (\n        df[c]\n        .value_counts()\n        .rename(\"number_of_visitors\")\n        .to_frame()\n        .merge(\n            (\n                df[c].value_counts(normalize=True).rename(\"fraction_of_visitors\") * 100\n            ).to_frame(),\n            left_index=True,\n            right_index=True,\n        )\n    )\n\n    # map unique values for last_action and bounces to get meaningful names\n    if c == \"last_action\":\n        df_frequencies.index = df_frequencies.index.map(mapper)\n    if c == \"bounces\":\n        df_frequencies.index = df_frequencies.index.map({0: False, 1: True})\n\n    # get running total of fraction (cumulative sum)\n    df_frequencies = (\n        df_frequencies.sort_values(by=[\"fraction_of_visitors\"])\n        .assign(\n            cumulative_fraction_of_visitors=lambda df: df[\n                \"fraction_of_visitors\"\n            ].cumsum(),\n            column_name=c,\n        )\n        .sort_values(by=[\"fraction_of_visitors\"], ascending=False)\n    )\n\n    # rename columns\n    df_frequencies = df_frequencies.reset_index().rename(columns={c: \"column_value\"})\n    dfs_cats_groups.append(df_frequencies)\ndf_frequencies_raw = pd.concat(dfs_cats_groups, ignore_index=True)\ncol = df_frequencies_raw.pop(\"column_name\")\ndf_frequencies_raw.insert(0, col.name, col)\nwith pd.option_context(\"display.max_rows\", None):\n    display(df_frequencies_raw)\n\n\n\n\n\n\n\n\ncolumn_name\ncolumn_value\nnumber_of_visitors\nfraction_of_visitors\ncumulative_fraction_of_visitors\n\n\n\n\n0\nbounces\nFalse\n65505\n70.777193\n100.0\n\n\n1\nbounces\nTrue\n27046\n29.222807\n29.222807\n\n\n2\nlast_action\nUnknown\n67511\n72.944647\n100.0\n\n\n3\nlast_action\nProduct detail views\n14731\n15.91663\n27.055353\n\n\n4\nlast_action\nAdd product(s) to cart\n4655\n5.029659\n11.138724\n\n\n5\nlast_action\nCompleted purchase\n3097\n3.346263\n6.109064\n\n\n6\nlast_action\nCheck out\n1635\n1.766594\n2.762801\n\n\n7\nlast_action\nRemove product(s) from cart\n881\n0.951908\n0.996207\n\n\n8\nlast_action\nClick through of product lists\n41\n0.0443\n0.0443\n\n\n9\nsource\ngoogle\n43484\n46.983825\n100.0\n\n\n10\nsource\n(direct)\n21532\n23.265011\n53.016175\n\n\n11\nsource\nmall.googleplex.com\n11777\n12.724876\n29.751164\n\n\n12\nsource\nyoutube.com\n8021\n8.666573\n17.026288\n\n\n13\nsource\nsites.google.com\n1098\n1.186373\n8.359715\n\n\n14\nsource\nmoma.corp.google.com\n1040\n1.123705\n7.173342\n\n\n15\nsource\nPartners\n899\n0.971356\n6.049637\n\n\n16\nsource\ndfa\n841\n0.908688\n5.078281\n\n\n17\nsource\nsiliconvalley.about.com\n593\n0.640728\n4.169593\n\n\n18\nsource\nanalytics.google.com\n486\n0.525116\n3.528865\n\n\n19\nsource\ngoogle.com\n413\n0.44624\n3.003749\n\n\n20\nsource\nbaidu\n314\n0.339272\n2.557509\n\n\n21\nsource\nm.facebook.com\n190\n0.205292\n2.218236\n\n\n22\nsource\nyahoo\n178\n0.192326\n2.012944\n\n\n23\nsource\nbing\n161\n0.173958\n1.820618\n\n\n24\nsource\nreddit.com\n131\n0.141544\n1.64666\n\n\n25\nsource\nl.facebook.com\n127\n0.137222\n1.505116\n\n\n26\nsource\ngroups.google.com\n120\n0.129658\n1.367894\n\n\n27\nsource\nfacebook.com\n112\n0.121014\n1.238236\n\n\n28\nsource\nmail.google.com\n110\n0.118853\n1.117222\n\n\n29\nsource\ngoogleux.perksplus.com\n95\n0.102646\n0.998368\n\n\n30\nsource\nquora.com\n86\n0.092922\n0.895722\n\n\n31\nsource\nblog.golang.org\n82\n0.0886\n0.802801\n\n\n32\nsource\nt.co\n80\n0.086439\n0.714201\n\n\n33\nsource\ndealspotr.com\n75\n0.081036\n0.627762\n\n\n34\nsource\nask\n48\n0.051863\n0.546726\n\n\n35\nsource\ndocs.google.com\n39\n0.042139\n0.494862\n\n\n36\nsource\nconnect.googleforwork.com\n33\n0.035656\n0.452723\n\n\n37\nsource\nplus.google.com\n21\n0.02269\n0.417067\n\n\n38\nsource\noutlook.live.com\n20\n0.02161\n0.394377\n\n\n39\nsource\ntpc.googlesyndication.com\n19\n0.020529\n0.372767\n\n\n40\nsource\nm.reddit.com\n18\n0.019449\n0.352238\n\n\n41\nsource\nphandroid.com\n17\n0.018368\n0.332789\n\n\n42\nsource\nsearch.xfinity.com\n17\n0.018368\n0.314421\n\n\n43\nsource\nlearn.colorado.edu\n13\n0.014046\n0.296053\n\n\n44\nsource\ngoogleads.g.doubleclick.net\n12\n0.012966\n0.282007\n\n\n45\nsource\nm.baidu.com\n12\n0.012966\n0.269041\n\n\n46\nsource\nlunametrics.com\n11\n0.011885\n0.256075\n\n\n47\nsource\nproductforums.google.com\n11\n0.011885\n0.24419\n\n\n48\nsource\npinterest.com\n10\n0.010805\n0.210695\n\n\n49\nsource\narstechnica.com\n10\n0.010805\n0.221499\n\n\n50\nsource\nduckduckgo.com\n10\n0.010805\n0.232304\n\n\n51\nsource\nsearch.tb.ask.com\n9\n0.009724\n0.19989\n\n\n52\nsource\nus.search.yahoo.com\n9\n0.009724\n0.190165\n\n\n53\nsource\nplus.sandbox.google.com\n8\n0.008644\n0.180441\n\n\n54\nsource\nuweoconnect.extn.washington.edu\n7\n0.007563\n0.171797\n\n\n55\nsource\nluyaochen.mtv.corp.google.com:8080\n6\n0.006483\n0.164234\n\n\n56\nsource\ncourse.fso.fullsail.edu\n6\n0.006483\n0.157751\n\n\n57\nsource\nmg.mail.yahoo.com\n6\n0.006483\n0.151268\n\n\n58\nsource\ngophergala.com\n5\n0.005402\n0.144785\n\n\n59\nsource\nwap.sogou.com\n5\n0.005402\n0.139383\n\n\n60\nsource\nplus.url.google.com\n5\n0.005402\n0.13398\n\n\n61\nsource\ntrainup.withgoogle.com\n5\n0.005402\n0.128578\n\n\n62\nsource\nfeedly.com\n4\n0.004322\n0.118853\n\n\n63\nsource\namazon.com\n4\n0.004322\n0.11021\n\n\n64\nsource\ndrawnames.com\n4\n0.004322\n0.123175\n\n\n65\nsource\nkeep.google.com\n4\n0.004322\n0.114531\n\n\n66\nsource\nsearch.earthlink.net\n4\n0.004322\n0.105888\n\n\n67\nsource\ngithub.com\n4\n0.004322\n0.097244\n\n\n68\nsource\nhangouts.google.com\n4\n0.004322\n0.092922\n\n\n69\nsource\ncsfirst.withgoogle.com\n4\n0.004322\n0.0886\n\n\n70\nsource\nm.sogou.com\n4\n0.004322\n0.084278\n\n\n71\nsource\ndynamite.sandbox.google.com\n4\n0.004322\n0.079956\n\n\n72\nsource\naol\n4\n0.004322\n0.101566\n\n\n73\nsource\nqiita.com\n3\n0.003241\n0.056185\n\n\n74\nsource\nics-devel-west.qa.adz.google.com\n3\n0.003241\n0.062668\n\n\n75\nsource\nus-mg5.mail.yahoo.com\n3\n0.003241\n0.059427\n\n\n76\nsource\nmessenger.com\n3\n0.003241\n0.06591\n\n\n77\nsource\nweb.mail.comcast.net\n3\n0.003241\n0.069151\n\n\n78\nsource\noptimize.google.com\n3\n0.003241\n0.072393\n\n\n79\nsource\ns0.2mdn.net\n3\n0.003241\n0.075634\n\n\n80\nsource\n(not set)\n2\n0.002161\n0.052944\n\n\n81\nsource\nso.com\n2\n0.002161\n0.050783\n\n\n82\nsource\nawics.corp.google.com\n2\n0.002161\n0.048622\n\n\n83\nsource\nm.mg.mail.yahoo.com\n2\n0.002161\n0.046461\n\n\n84\nsource\nsearchlock.com\n1\n0.00108\n0.017288\n\n\n85\nsource\n0.shared.bow.cat2.ads-bow.lf.borg.google.com:9860\n1\n0.00108\n0.015127\n\n\n86\nsource\n0.shared.bow.cat2.ads-bow.qk.borg.google.com:9848\n1\n0.00108\n0.016207\n\n\n87\nsource\ng3doc.corp.google.com\n1\n0.00108\n0.002161\n\n\n88\nsource\nbusinessinsider.com\n1\n0.00108\n0.018368\n\n\n89\nsource\nwheretoget.it\n1\n0.00108\n0.019449\n\n\n90\nsource\nblackboard.neu.edu\n1\n0.00108\n0.020529\n\n\n91\nsource\ngoto.google.com\n1\n0.00108\n0.012966\n\n\n92\nsource\nmail.aol.com\n1\n0.00108\n0.014046\n\n\n93\nsource\n0.shared.bow.cat2.ads-bow.yw.borg.google.com:9885\n1\n0.00108\n0.003241\n\n\n94\nsource\nprod.facebook.com\n1\n0.00108\n0.011885\n\n\n95\nsource\nlogin.corp.google.com\n1\n0.00108\n0.010805\n\n\n96\nsource\nm.sp.sm.cn\n1\n0.00108\n0.009724\n\n\n97\nsource\ngetpocket.com\n1\n0.00108\n0.008644\n\n\n98\nsource\nus.reddit.com\n1\n0.00108\n0.007563\n\n\n99\nsource\npinpoint.corp.google.com\n1\n0.00108\n0.006483\n\n\n100\nsource\nwanelo.com\n1\n0.00108\n0.005402\n\n\n101\nsource\nmail.verizon.com\n1\n0.00108\n0.004322\n\n\n102\nsource\ninbox.google.com\n1\n0.00108\n0.02269\n\n\n103\nsource\n0.shared.bow.cat2.ads-bow.qk.borg.google.com:9830\n1\n0.00108\n0.02161\n\n\n104\nsource\n0.shared.bow.cat2.ads-bow.yw.borg.google.com:9849\n1\n0.00108\n0.032415\n\n\n105\nsource\n9to5google.com\n1\n0.00108\n0.023771\n\n\n106\nsource\nadwords.google.com\n1\n0.00108\n0.024851\n\n\n107\nsource\ncl-cards.googleplex.com\n1\n0.00108\n0.0443\n\n\n108\nsource\nnewclasses.nyu.edu\n1\n0.00108\n0.043219\n\n\n109\nsource\nmyactivity.google.com\n1\n0.00108\n0.042139\n\n\n110\nsource\nseroundtable.com\n1\n0.00108\n0.041058\n\n\n111\nsource\n0.shared.bow.cat2.ads-bow.yw.borg.google.com:9850\n1\n0.00108\n0.039978\n\n\n112\nsource\nlm.facebook.com\n1\n0.00108\n0.038897\n\n\n113\nsource\ncases.corp.google.com\n1\n0.00108\n0.037817\n\n\n114\nsource\ngoogle.co.jp\n1\n0.00108\n0.036737\n\n\n115\nsource\nadwords-displayads.googleusercontent.com\n1\n0.00108\n0.035656\n\n\n116\nsource\nxbidprodmirror.corp.google.com\n1\n0.00108\n0.034576\n\n\n117\nsource\nevernote.com\n1\n0.00108\n0.033495\n\n\n118\nsource\nonline-metrics.com\n1\n0.00108\n0.031334\n\n\n119\nsource\n0.shared.bow.cat2.ads-bow.yw.borg.google.com:9813\n1\n0.00108\n0.030254\n\n\n120\nsource\nkik.com\n1\n0.00108\n0.029173\n\n\n121\nsource\nbaidu.com\n1\n0.00108\n0.028093\n\n\n122\nsource\ndailydot.com\n1\n0.00108\n0.027012\n\n\n123\nsource\ndigg.com\n1\n0.00108\n0.025932\n\n\n124\nsource\nspaces.google.com\n1\n0.00108\n0.00108\n\n\n125\nmedium\norganic\n38584\n41.689447\n100.0\n\n\n126\nmedium\nreferral\n25086\n27.105056\n58.310553\n\n\n127\nmedium\n(none)\n21532\n23.265011\n31.205498\n\n\n128\nmedium\ncpc\n5607\n6.058281\n7.940487\n\n\n129\nmedium\naffiliate\n899\n0.971356\n1.882205\n\n\n130\nmedium\ncpm\n841\n0.908688\n0.910849\n\n\n131\nmedium\n(not set)\n2\n0.002161\n0.002161\n\n\n132\nchannelGrouping\nOrganic Search\n38584\n41.689447\n100.0\n\n\n133\nchannelGrouping\nDirect\n21532\n23.265011\n58.310553\n\n\n134\nchannelGrouping\nReferral\n16160\n17.460643\n35.045542\n\n\n135\nchannelGrouping\nSocial\n8926\n9.644412\n17.584899\n\n\n136\nchannelGrouping\nPaid Search\n5607\n6.058281\n7.940487\n\n\n137\nchannelGrouping\nAffiliates\n899\n0.971356\n1.882205\n\n\n138\nchannelGrouping\nDisplay\n841\n0.908688\n0.910849\n\n\n139\nchannelGrouping\n(Other)\n2\n0.002161\n0.002161\n\n\n140\nbrowser\nChrome\n69002\n74.55565\n100.0\n\n\n141\nbrowser\nSafari\n15294\n16.524943\n25.44435\n\n\n142\nbrowser\nFirefox\n2594\n2.802779\n8.919407\n\n\n143\nbrowser\nInternet Explorer\n2214\n2.392195\n6.116628\n\n\n144\nbrowser\nOpera\n1119\n1.209063\n3.724433\n\n\n145\nbrowser\nEdge\n989\n1.0686\n2.51537\n\n\n146\nbrowser\nSafari (in-app)\n799\n0.863308\n1.44677\n\n\n147\nbrowser\nAndroid Webview\n165\n0.17828\n0.583462\n\n\n148\nbrowser\nYaBrowser\n107\n0.115612\n0.405182\n\n\n149\nbrowser\nAmazon Silk\n93\n0.100485\n0.28957\n\n\n150\nbrowser\nMozilla Compatible Agent\n31\n0.033495\n0.189085\n\n\n151\nbrowser\nUC Browser\n28\n0.030254\n0.15559\n\n\n152\nbrowser\nOpera Mini\n19\n0.020529\n0.125336\n\n\n153\nbrowser\nAndroid Browser\n16\n0.017288\n0.104807\n\n\n154\nbrowser\nIron\n15\n0.016207\n0.087519\n\n\n155\nbrowser\nCoc Coc\n13\n0.014046\n0.071312\n\n\n156\nbrowser\nNintendo Browser\n13\n0.014046\n0.057266\n\n\n157\nbrowser\nBlackBerry\n10\n0.010805\n0.043219\n\n\n158\nbrowser\nMaxthon\n10\n0.010805\n0.032415\n\n\n159\nbrowser\nMRCHROME\n7\n0.007563\n0.02161\n\n\n160\nbrowser\nNichrome\n5\n0.005402\n0.014046\n\n\n161\nbrowser\nApple-iPhone7C2\n3\n0.003241\n0.008644\n\n\n162\nbrowser\nMozilla\n2\n0.002161\n0.005402\n\n\n163\nbrowser\nNokiaE52-1\n1\n0.00108\n0.002161\n\n\n164\nbrowser\nno-ua\n1\n0.00108\n0.003241\n\n\n165\nbrowser\nSeaMonkey\n1\n0.00108\n0.00108\n\n\n166\nos\nMacintosh\n30610\n33.073657\n100.0\n\n\n167\nos\nWindows\n25588\n27.647459\n66.926343\n\n\n168\nos\niOS\n13500\n14.586552\n39.278884\n\n\n169\nos\nAndroid\n11486\n12.410455\n24.692332\n\n\n170\nos\nLinux\n6500\n7.023155\n12.281877\n\n\n171\nos\nChrome OS\n4741\n5.122581\n5.258722\n\n\n172\nos\n(not set)\n53\n0.057266\n0.136141\n\n\n173\nos\nWindows Phone\n39\n0.042139\n0.078875\n\n\n174\nos\nNintendo Wii\n11\n0.011885\n0.036737\n\n\n175\nos\nBlackBerry\n10\n0.010805\n0.024851\n\n\n176\nos\nXbox\n9\n0.009724\n0.014046\n\n\n177\nos\nSamsung\n1\n0.00108\n0.00108\n\n\n178\nos\nFreeBSD\n1\n0.00108\n0.002161\n\n\n179\nos\nNokia\n1\n0.00108\n0.003241\n\n\n180\nos\nSunOS\n1\n0.00108\n0.004322\n\n\n181\ndeviceCategory\ndesktop\n67355\n72.776091\n100.0\n\n\n182\ndeviceCategory\nmobile\n21790\n23.543776\n27.223909\n\n\n183\ndeviceCategory\ntablet\n3406\n3.680133\n3.680133\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nWe’ll create frequency groupings as follows\n\nsource and browser\n\nall categories which occur with a frequency of less than 5% will be grouped into a single value other\n\nos, channelGrouping and medium\n\nall categories which occur with a frequency of less than 10% will be grouped into a single value other\n\n\nThese thresholds were determined by examining the output of df_frequencies_raw, which shows the freqencies of all categories for all categorical columns.\n\n\n\nBelow are lists of categorical columns to be grouped based on this threshold (5% or 10%)\n\ncols_to_group_5_pct = [\"source\", \"browser\"]\ncols_to_group_10_pct = [\"os\", \"channelGrouping\", \"medium\"]\n\nWe’ll get names for the columns after grouping, by adding a _grouped suffix\n\ngrouped_cols_5_pct = [f\"{c}_grouped\" for c in cols_to_group_5_pct]\ngrouped_cols_10_pct = [f\"{c}_grouped\" for c in cols_to_group_10_pct]\n\nNext, create lists of categorical columns that will and will not be grouped and then combine them into a single list\n\ncategorical_columns_mapped = (\n    # columns that will not be grouped\n    list(\n        set(categorical_columns) - set(cols_to_group_5_pct) - set(cols_to_group_10_pct)\n    )\n    # columns that will be grouped\n    + grouped_cols_5_pct\n    + grouped_cols_10_pct\n)\n\nCreate a duplicate of the columns that will be grouped and add a _grouped suffix to their column name\n\nfor c in cols_to_group_5_pct + cols_to_group_10_pct:\n    df[f\"{c}_grouped\"] = df[c]\n\nFinally, perform the grouping using\n\npandas.value_counts(normalize=True) &lt; 0.05 (5% threshold)\npandas.value_counts(normalize=True) &lt; 0.10 (10% threshold)\n\nwhere all infrequently occurring values that satisfy these filters will have their values replaced by other\n\ndf[grouped_cols_5_pct] = df[grouped_cols_5_pct].apply(\n    lambda x: x.mask(x.map(x.value_counts(normalize=True)) &lt; 0.05, \"other\"), axis=0\n)\ndf[grouped_cols_10_pct] = df[grouped_cols_10_pct].apply(\n    lambda x: x.mask(x.map(x.value_counts(normalize=True)) &lt; 0.10, \"other\"), axis=0\n)\n\nThe cardinality of the columns before and after grouping is shown below\n\ndf_nunique.merge(\n    pd.DataFrame.from_records(\n        [\n            {\n                \"column\": c.replace(\"_grouped\", \"\"),\n                \"column_grouped\": c,\n                \"num_unique_values_after_grouping\": df[c].nunique(),\n            }\n            for c in categorical_columns_mapped\n        ]\n    ).assign(column_grouped=lambda df: df[\"column_grouped\"] != df[\"column\"]),\n    on=[\"column\"],\n    how=\"left\",\n)\n\n\n\n\n\n\n\n\ncolumn\nnum_unique_values\ncolumn_grouped\nnum_unique_values_after_grouping\n\n\n\n\n0\nbounces\n2\nFalse\n2\n\n\n1\nlast_action\n7\nFalse\n7\n\n\n2\nsource\n116\nTrue\n5\n\n\n3\nmedium\n7\nTrue\n4\n\n\n4\nchannelGrouping\n8\nTrue\n4\n\n\n5\nbrowser\n26\nTrue\n3\n\n\n6\nos\n15\nTrue\n5\n\n\n7\ndeviceCategory\n3\nFalse\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThe cardinality has been significantly reduced for the columns where the infrequently occurring values were grouped (column_grouped == True).\nThe cardinality is unchanged for the columns where the infrequently occurring values were not grouped (column_grouped == False).\n\n\n\nThe category distributions (frequencies) after grouping are shown below for all categorical columns (including those that were grouped)\n\ndfs_cats_groups = []\nfor c in categorical_columns_mapped:\n    # get fraction of unique values\n    df_frequencies = (\n        df[c]\n        .value_counts()\n        .rename(\"number_of_visitors\")\n        .to_frame()\n        .merge(\n            (\n                df[c].value_counts(normalize=True).rename(\"fraction_of_visitors\") * 100\n            ).to_frame(),\n            left_index=True,\n            right_index=True,\n        )\n    )\n\n    # map unique values for last_action and bounces to get meaningful names\n    if c == \"last_action\":\n        df_frequencies.index = df_frequencies.index.map(mapper)\n    if c == \"bounces\":\n        df_frequencies.index = df_frequencies.index.map({0: False, 1: True})\n\n    # get running total of fraction (cumulative sum)\n    df_frequencies = (\n        df_frequencies.sort_values(by=[\"fraction_of_visitors\"])\n        .assign(\n            cumulative_fraction_of_visitors=lambda df: df[\n                \"fraction_of_visitors\"\n            ].cumsum(),\n            column_name=c,\n        )\n        .sort_values(by=[\"fraction_of_visitors\"], ascending=False)\n    )\n\n    # rename columns\n    df_frequencies = df_frequencies.reset_index().rename(columns={c: \"column_value\"})\n    dfs_cats_groups.append(df_frequencies)\ndf_frequencies_grouped = pd.concat(dfs_cats_groups, ignore_index=True)\ncol = df_frequencies_grouped.pop(\"column_name\")\ndf_frequencies_grouped.insert(0, col.name, col)\nwith pd.option_context(\"display.max_rows\", None):\n    display(df_frequencies_grouped)\n\n\n\n\n\n\n\n\ncolumn_name\ncolumn_value\nnumber_of_visitors\nfraction_of_visitors\ncumulative_fraction_of_visitors\n\n\n\n\n0\ndeviceCategory\ndesktop\n67355\n72.776091\n100.0\n\n\n1\ndeviceCategory\nmobile\n21790\n23.543776\n27.223909\n\n\n2\ndeviceCategory\ntablet\n3406\n3.680133\n3.680133\n\n\n3\nbounces\nFalse\n65505\n70.777193\n100.0\n\n\n4\nbounces\nTrue\n27046\n29.222807\n29.222807\n\n\n5\nlast_action\nUnknown\n67511\n72.944647\n100.0\n\n\n6\nlast_action\nProduct detail views\n14731\n15.91663\n27.055353\n\n\n7\nlast_action\nAdd product(s) to cart\n4655\n5.029659\n11.138724\n\n\n8\nlast_action\nCompleted purchase\n3097\n3.346263\n6.109064\n\n\n9\nlast_action\nCheck out\n1635\n1.766594\n2.762801\n\n\n10\nlast_action\nRemove product(s) from cart\n881\n0.951908\n0.996207\n\n\n11\nlast_action\nClick through of product lists\n41\n0.0443\n0.0443\n\n\n12\nsource_grouped\ngoogle\n43484\n46.983825\n100.0\n\n\n13\nsource_grouped\n(direct)\n21532\n23.265011\n53.016175\n\n\n14\nsource_grouped\nmall.googleplex.com\n11777\n12.724876\n29.751164\n\n\n15\nsource_grouped\nyoutube.com\n8021\n8.666573\n17.026288\n\n\n16\nsource_grouped\nother\n7737\n8.359715\n8.359715\n\n\n17\nbrowser_grouped\nChrome\n69002\n74.55565\n100.0\n\n\n18\nbrowser_grouped\nSafari\n15294\n16.524943\n25.44435\n\n\n19\nbrowser_grouped\nother\n8255\n8.919407\n8.919407\n\n\n20\nos_grouped\nMacintosh\n30610\n33.073657\n100.0\n\n\n21\nos_grouped\nWindows\n25588\n27.647459\n66.926343\n\n\n22\nos_grouped\niOS\n13500\n14.586552\n39.278884\n\n\n23\nos_grouped\nAndroid\n11486\n12.410455\n24.692332\n\n\n24\nos_grouped\nother\n11367\n12.281877\n12.281877\n\n\n25\nchannelGrouping_grouped\nOrganic Search\n38584\n41.689447\n100.0\n\n\n26\nchannelGrouping_grouped\nDirect\n21532\n23.265011\n58.310553\n\n\n27\nchannelGrouping_grouped\nother\n16275\n17.584899\n35.045542\n\n\n28\nchannelGrouping_grouped\nReferral\n16160\n17.460643\n17.460643\n\n\n29\nmedium_grouped\norganic\n38584\n41.689447\n100.0\n\n\n30\nmedium_grouped\nreferral\n25086\n27.105056\n58.310553\n\n\n31\nmedium_grouped\n(none)\n21532\n23.265011\n31.205498\n\n\n32\nmedium_grouped\nother\n7349\n7.940487\n7.940487\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThese distributions are shown here after frequency encoding (grouping) the high-cardinality columns in order to determine the thresholds (5% and 10%) for replacing infrequently occurring values in these columns. Earlier, the same was shown in the raw categorical columns. In that DataFrame, there were 184 unique categories across all categorical columns (length of df_frequencies_raw). After dummy encoding (where we will drop duplicate categories in each raw categorical column - 1), there would be 184 - &lt;number-of-categorical-columns&gt; = 184 - 8 = 176 features.\nAfter frequency grouping (where we will drop duplicate categories in each grouped categorical column), there are 33 unique categories. After dummy encoding, the number of dummy variables will be 33 - &lt;number-of-categorical-columns&gt; = 33 - 8 = 26 features. This frequency encoding approach has reduced the cardinality by (176 - 26) / 176 = 0.86 (or 86%).\n\n\n\nThe reduction in cardinality of the categorical feaures, after frequency grouping, is calculated below\n\nfrac_reduction_in_cats_cardinality = (\n    100\n    * (\n        (len(df_frequencies_raw) - len(categorical_columns))\n        - (len(df_frequencies_grouped) - len(categorical_columns))\n    )\n    / (len(df_frequencies_raw) - len(categorical_columns))\n)\nprint(\n    \"Frequency encoding (grouping) has reduced cardinality of categorical features by \"\n    f\"{frac_reduction_in_cats_cardinality:,.3f}%\"\n)\n\nFrequency encoding (grouping) has reduced cardinality of categorical features by 85.795%\n\n\nThe groupings above have been learnt from the training data. We now need to create a lookup table for columns that were grouped so that we can apply the same groupings to unseen data (validation and test data splits). This means when we encounter the same infrequently occurring values in the validation and test data splits, they will be replaced by other.\nThis lookup table is defined below\n\ndf_groupings = pd.DataFrame.from_dict(\n    {\n        c: df[c]\n        .value_counts(normalize=True)\n        .rename(\"fraction\")\n        .to_frame()\n        .query(f\"fraction &lt; {threshold}\")\n        .index.tolist()\n        for cols, threshold in zip(\n            [cols_to_group_5_pct, cols_to_group_10_pct], [0.05, 0.10]\n        )\n        for c in cols\n    },\n    orient=\"index\",\n).transpose()\ndisplay(df_groupings.head())\ndisplay(df_groupings.tail())\n\n\n\n\n\n\n\n\nsource\nbrowser\nos\nchannelGrouping\nmedium\n\n\n\n\n0\nsites.google.com\nFirefox\nLinux\nSocial\ncpc\n\n\n1\nmoma.corp.google.com\nInternet Explorer\nChrome OS\nPaid Search\naffiliate\n\n\n2\nPartners\nOpera\n(not set)\nAffiliates\ncpm\n\n\n3\ndfa\nEdge\nWindows Phone\nDisplay\n(not set)\n\n\n4\nsiliconvalley.about.com\nSafari (in-app)\nNintendo Wii\n(Other)\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\nbrowser\nos\nchannelGrouping\nmedium\n\n\n\n\n107\ndailydot.com\nNone\nNone\nNone\nNone\n\n\n108\ndigg.com\nNone\nNone\nNone\nNone\n\n\n109\nxbidprodmirror.corp.google.com\nNone\nNone\nNone\nNone\n\n\n110\nseroundtable.com\nNone\nNone\nNone\nNone\n\n\n111\nspaces.google.com\nNone\nNone\nNone\nNone\n\n\n\n\n\n\n\nWe’ll also create a lookup table of unique values in the categorical columns that were not grouped. When we encounter these values in the validation or test data splits, they will remain unchanged.\nThis lookup table is defined below\n\ndf_ungrouped = pd.DataFrame.from_dict(\n    {\n        c: df[c]\n        .value_counts(normalize=True)\n        .rename(\"fraction\")\n        .to_frame()\n        .query(f\"fraction &gt;= {threshold}\")\n        .index.tolist()\n        for cols, threshold in zip(\n            [cols_to_group_5_pct, cols_to_group_10_pct], [0.05, 0.10]\n        )\n        for c in cols\n    },\n    orient=\"index\",\n).transpose()\ndf_ungrouped\n\n\n\n\n\n\n\n\nsource\nbrowser\nos\nchannelGrouping\nmedium\n\n\n\n\n0\ngoogle\nChrome\nMacintosh\nOrganic Search\norganic\n\n\n1\n(direct)\nSafari\nWindows\nDirect\nreferral\n\n\n2\nmall.googleplex.com\nNone\niOS\nReferral\n(none)\n\n\n3\nyoutube.com\nNone\nAndroid\nNone\nNone\n\n\n\n\n\n\n\nFor a quick demonstration of using these two lookup tables, we’ll create a dummy validation data DataFrame below with two categorical features\n\n\nCode\ndf_val = pd.DataFrame.from_records(\n    [\n        {\"source\": \"Partners\", \"browser\": \"Internet Explorer\"},\n        {\"source\": \"dfa\", \"browser\": \"new-browser\"},\n        {\"source\": \"new-source-value\", \"browser\": \"Chrome\"},\n    ]\n)\ndf_val\n\n\n\n\n\n\n\n\n\nsource\nbrowser\n\n\n\n\n0\nPartners\nInternet Explorer\n\n\n1\ndfa\nnew-browser\n\n\n2\nnew-source-value\nChrome\n\n\n\n\n\n\n\nWe’ll now apply both the lookup tables defined above using the following approach\n\nfor all columns that were grouped, create columns with a suffix _grouped which contains the value other for infrequently occurring values\nfor all columns that were not grouped, create columns with a suffix _ungrouped which contains the same values with no changes\ncombine columns with the _ungrouped and _grouped suffixes into a single column column\n\nto do this, fill missing values in the _grouped column with those in the _ungrouped column\n\ndrop original columns and rename the combined columns appropriately\n\n\ncategorical_columns_validation_data = [\"source\", \"browser\"]\nfor c in categorical_columns_validation_data:\n    # 1. replace infrequent values in columns that were grouped (add suffix _grouped)\n    df_val[f\"{c}_grouped\"] = df_val[c].map(\n        {c_grouped: \"other\" for c_grouped in df_groupings[c].tolist()}\n    )\n    # 2. keep all values in columns that were not grouped (add suffix _ungrouped)\n    df_val[f\"{c}_ungrouped\"] = df_val[c].map(\n        {c_ungrouped: c_ungrouped for c_ungrouped in df_ungrouped[c].tolist()}\n    )\n    # 3. combine columns that were replaced (_grouped) and those that were not replaced (_ungrouped)\n    df_val[f\"{c}_grouped\"] = df_val[f\"{c}_grouped\"].fillna(df_val[f\"{c}_ungrouped\"])\n# 4. drop unwanted columns and rename\ndf_val = df_val.drop(\n    columns=[\"browser_ungrouped\", \"source_ungrouped\"]\n    + categorical_columns_validation_data\n).rename(columns={f\"{c}_grouped\": c for c in categorical_columns_validation_data})\ndf_val\n\n\n\n\n\n\n\n\nsource\nbrowser\n\n\n\n\n0\nother\nother\n\n\n1\nother\nNaN\n\n\n2\nNaN\nChrome\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nIn both features of the validation data, there are new categories that were not seen in the training data. After applying the two lookup tables above, these values are replaced by Nones. We can fill these missing values using\n\nnew (or keep it as None) to indicate this is a new category\n\nthe ML model has not seen this value in the appropriate feature during training, so the predictive power of such a feature in the unseen (validation) data will likely be reduced or minimal (the model won’t know its relationship to the label y)\n\nother to group this into the infrequently occurring categories that were identified from the training data\n\nthe disadvantage is that these new categories might have a different relationship to the label label (y) than the grouped (other) category\nin such a scenario\n\nthe ML model might not able to leverage the full predictive power of such new categories in the validation (unseen) data when it makes predictions since it was not trained to learn this relationship in the training data\nthe model will make predictions based on the relationship learnt between the grouped (other) category and the label (y)\n\n\n\n\n\n\nDuring data processing (after this EDA step), we will create the training, validation and test data splits for ML development and the same workflow will be used to handle categorical features during data processing."
  },
  {
    "objectID": "notebooks/02-prepare/notebooks/02_prepare.html#key-findings",
    "href": "notebooks/02-prepare/notebooks/02_prepare.html#key-findings",
    "title": "Data Preparation",
    "section": "Key Findings",
    "text": "Key Findings\n\nNested Attributes of First-Time Visits\n\nPromotion nested column\n\nWhen view_promo = True, a visitor has viewed a promotion. If it is not viewed, then it is None\nWhen a visitor clicks a promotion after viewing it\n\nclick_promo = True\nview_promo is NULL\n\n\nProduct nested column\n\nif a product is viewed in a listing (product_count &gt; 0) during a visit, then there are only two possible values for clicked_product and viewed_product, namely True and None\na product is not viewed in a listing, then the only value in these same two columns is None\n\n\n\n\nRecommendations for Data Processing\n\nNegligible duplicates exist for\n\nvisitId\n\nthe reason for this duplication is known\nsuch duplicates are kept in the data\n\nfullvisitorid\n\nthe reason for this duplication is not known\nall but the first of such duplicates should be dropped\n\n\nThe cardinality across the combination of all categorical columns extracted from visitors’ first visit to the store can be significantly reduced by replacing infrequently occuring values by the category other\n\nsource and browser columns\n\nall categories which occur with a frequency of less than 5% should be grouped into a single value other\n\nos, channelGrouping and medium columns\n\nall categories which occur with a frequency of less than 10% should be grouped into a single value other"
  },
  {
    "objectID": "notebooks/02-prepare/notebooks/02_prepare.html#summary-of-assumptions",
    "href": "notebooks/02-prepare/notebooks/02_prepare.html#summary-of-assumptions",
    "title": "Data Preparation",
    "section": "Summary of Assumptions",
    "text": "Summary of Assumptions\n\nNegligible duplicates in fullvisitorid are found and are not well understood and so they should be dropped in the training, validation and test data splits during data transformation."
  },
  {
    "objectID": "notebooks/02-prepare/notebooks/02_prepare.html#summary-of-tasks-performed",
    "href": "notebooks/02-prepare/notebooks/02_prepare.html#summary-of-tasks-performed",
    "title": "Data Preparation",
    "section": "Summary of Tasks Performed",
    "text": "Summary of Tasks Performed\nThis step has performed the following\n\nextracted attributes from dataset to create a prepared dataset for use in EDA\n\nflattened nested columns for products and promotions\nextracted columns that should intuitively help predict probability of making a purchase on a return (future) visit\n\naddressed duplicated visits\nhandled high-cardinality categorical columns"
  },
  {
    "objectID": "notebooks/02-prepare/notebooks/02_prepare.html#limitations",
    "href": "notebooks/02-prepare/notebooks/02_prepare.html#limitations",
    "title": "Data Preparation",
    "section": "Limitations",
    "text": "Limitations\nNone."
  },
  {
    "objectID": "notebooks/02-prepare/notebooks/02_prepare.html#next-step",
    "href": "notebooks/02-prepare/notebooks/02_prepare.html#next-step",
    "title": "Data Preparation",
    "section": "Next Step",
    "text": "Next Step\nThe next step will be to perform exploratory data analysis using data prepared using the findings from this data preparation step."
  },
  {
    "objectID": "notebooks/03-eda/notebooks/03_eda.html",
    "href": "notebooks/03-eda/notebooks/03_eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Import Python modules\nCode\nimport os\nfrom calendar import day_name, month_name\nfrom datetime import datetime\nfrom glob import glob\nfrom typing import Dict, List, Tuple, Union\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pytz\nimport seaborn as sns\nfrom feature_engine.encoding import RareLabelEncoder\nfrom google.oauth2 import service_account\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import Pipeline"
  },
  {
    "objectID": "notebooks/03-eda/notebooks/03_eda.html#about",
    "href": "notebooks/03-eda/notebooks/03_eda.html#about",
    "title": "Exploratory Data Analysis",
    "section": "About",
    "text": "About\nThis step of the analysis explores the visit data for the Google Merchandise store on the Google Marketplace, based on learnings from the data preparation step.\n\nObjectives\nThe goals of this step in the analysis are the following\n\nexplore the prepared data to look for any underlying patterns that can be captured through features that might help ML modeling\nobserve the class imbalance of the labels (y) to get a sense of any resampling techniques that could aid the performance of a ML model\n\n\n\nDiscussion of Study Period for This Project\nSame as for data preparation.\n\n\nData Selection for EDA\nWith the above in mind, EDA in this step will cover the training data.\n\n\nBusiness Questions for EDA\nWe will explore this dataset by answering the following (non-exhaustive) set of business-relevant questions\n\nFor high-level channel, and the more fine-graned traffic source and traffic medium, get the correlation between the numerical attributes of visits and total visit revenue. Across all of these groupings of sources of web traffic arriving at the merchandise store, comment on the strongest and weakest correlations to revenue.\nWhich channels were responsible for the most first-visitors with a purchase on their return visit? Show this comparison on a chart by channel.\nIs there any one web-browser that return purchasers used on their first visit?\nDo visitors who make a purchase on their return visit to the store have a preference for the type of device they are using (desktop, mobile, etc.)?\nWhich traffic sources were responsible for the most return purchasers on weekdays and weekends?\nShow the fraction of visitors who did make a purchase on their return visit to the merchandise store by month and day of the month.\nShow the fraction of visitors who made a purchase on their return visit to the merchandise store by month and day of the week.\nShow the breakdown of total revenue earned by visitors who are return purchasers and those that are not return purchasers."
  },
  {
    "objectID": "notebooks/03-eda/notebooks/03_eda.html#user-inputs",
    "href": "notebooks/03-eda/notebooks/03_eda.html#user-inputs",
    "title": "Exploratory Data Analysis",
    "section": "User Inputs",
    "text": "User Inputs\nGet relative path to project root directory\n\nPROJ_ROOT_DIR = os.path.join(os.pardir)\n\nDefine the following\n\ntrain data start date\ntrain data end date\ntest data end date\nlist of categorical features to be used\ndictionary of columns to be frequency-encoded and minimum frequency thresholds to be used\n\nthresholds are 5% or 10%, based on our findings from the data preparation step7. list of categorical features to be used\n\ndictionary of columns to be frequency-encoded and minimum frequency thresholds to be used\n\nthresholds are 5% or 10%, based on our findings from the data preparation step\n\n\n\ntrain_start_date = \"20160901\"\ntrain_end_date = \"20161231\"\ntest_end_date = \"20170228\"\n\n# categorical column names\ncategorical_columns = [\n    \"bounces\",\n    \"last_action\",\n    \"source\",\n    \"medium\",\n    \"channelGrouping\",\n    \"browser\",\n    \"os\",\n    \"deviceCategory\",\n]\n\n# categorical columns to be grouped\ncols_to_group = {\n    \"5_pct\": [\"source\", \"browser\"],\n    \"10_pct\": [\"os\", \"channelGrouping\", \"medium\"],\n}\n\nRetrieve credentials for bigquery client\n\n# Google Cloud PROJECT ID\ngcp_project_id = os.environ[\"GCP_PROJECT_ID\"]\n\nGet filepath to Google Cloud Service Account JSON key\n\nraw_data_dir = os.path.join(PROJ_ROOT_DIR, \"data\", \"raw\")\ngcp_creds_fpath = glob(os.path.join(raw_data_dir, \"*.json\"))[0]\n\nAuthenticate bigquery client and get dictionary with credentials\n\ngcp_credentials = service_account.Credentials.from_service_account_file(gcp_creds_fpath)\ngcp_auth_dict = dict(gcp_project_id=gcp_project_id, gcp_creds=gcp_credentials)\n\nCreate a mapping between action type integer and label, in order to get meaningful names from the action_type column\n\nmapper = {\n    1: \"Click through of product lists\",\n    2: \"Product detail views\",\n    3: \"Add product(s) to cart\",\n    4: \"Remove product(s) from cart\",\n    5: \"Check out\",\n    6: \"Completed purchase\",\n    7: \"Refund of purchase\",\n    8: \"Checkout options\",\n    0: \"Unknown\",\n}\n\nDefine a dictionary to change datatypes of prepared data (this was originally developed in the data preparation step)\n\ndtypes_dict = {\n    \"fullvisitorid\": pd.StringDtype(),\n    \"visitId\": pd.StringDtype(),\n    \"visitNumber\": pd.Int8Dtype(),\n    \"country\": pd.StringDtype(),\n    \"quarter\": pd.Int8Dtype(),\n    \"month\": pd.Int8Dtype(),\n    \"day_of_month\": pd.Int8Dtype(),\n    \"day_of_week\": pd.Int8Dtype(),\n    \"hour\": pd.Int8Dtype(),\n    \"minute\": pd.Int8Dtype(),\n    \"second\": pd.Int8Dtype(),\n    \"source\": pd.CategoricalDtype(),  #\n    \"medium\": pd.CategoricalDtype(),  #\n    \"channelGrouping\": pd.CategoricalDtype(),  #\n    \"hits\": pd.Int16Dtype(),\n    \"bounces\": pd.CategoricalDtype(),  #\n    \"last_action\": pd.CategoricalDtype(),  #\n    \"product_detail_views\": pd.Int16Dtype(),\n    \"promos_displayed\": pd.Int16Dtype(),\n    \"promos_clicked\": pd.Int16Dtype(),\n    \"product_views\": pd.Int16Dtype(),\n    \"product_clicks\": pd.Int16Dtype(),\n    \"pageviews\": pd.Int16Dtype(),\n    \"transact_revenue\": pd.Float32Dtype(),\n    \"time_on_site\": pd.Int16Dtype(),\n    \"browser\": pd.CategoricalDtype(),  #\n    \"os\": pd.CategoricalDtype(),  #\n    \"added_to_cart\": pd.Int16Dtype(),\n    \"deviceCategory\": pd.CategoricalDtype(),  #\n}\n\nDefine a Python helper functions to perform the following\n\nexecute a SQL query using Google BigQuery\nset column datatypes of a pandas.DataFrame\ndrop duplicates based on a list of columns in a pandas.DataFrame\ncustomize the axes of a matplotlib plot\nplot a faceted bar chart using seaborn\nplot a heatmap using seaborn\n\n\ndef run_sql_query(\n    query: str,\n    gcp_project_id: str,\n    gcp_creds: os.PathLike,\n    show_dtypes: bool = False,\n    show_info: bool = False,\n    show_df: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Run query on BigQuery and return results as pandas.DataFrame.\"\"\"\n    start_time = datetime.now(pytz.timezone(\"US/Eastern\"))\n    start_time_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n    print(f\"Query execution start time = {start_time_str[:-3]}...\", end=\"\")\n    df = pd.read_gbq(\n        query,\n        project_id=gcp_project_id,\n        credentials=gcp_creds,\n        dialect=\"standard\",\n        # configuration is optional, since default for query caching is True\n        configuration={\"query\": {\"useQueryCache\": True}},\n        # use_bqstorage_api=True,\n    )\n    end_time = datetime.now(pytz.timezone(\"US/Eastern\"))\n    end_time_str = end_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n    duration = end_time - start_time\n    duration = duration.seconds + (duration.microseconds / 1_000_000)\n    print(f\"done at {end_time_str[:-3]} ({duration:.3f} seconds).\")\n    print(f\"Query returned {len(df):,} rows\")\n    if show_df:\n        with pd.option_context(\"display.max_columns\", None):\n            display(df)\n    if show_dtypes:\n        display(df.dtypes.rename(\"dtype\").to_frame().transpose())\n    if show_info:\n        df.info()\n    return df\n\n\ndef set_datatypes(df: pd.DataFrame, dtypes: Dict) -&gt; pd.DataFrame:\n    \"\"\"Set datatypes in a DataFrame using a dictionary.\"\"\"\n    df = df.astype(dtypes)\n    return df\n\n\ndef drop_duplicates(df: pd.DataFrame, subset: List[str]) -&gt; pd.DataFrame:\n    \"\"\"Drop duplicates based on a list of columns in a DataFrame.\"\"\"\n    df = df.drop_duplicates(subset=subset, keep=\"first\")\n    return df\n\n\ndef customize_splines(ax: plt.axis, edgecolor: str = \"grey\") -&gt; plt.axis:\n    \"\"\"Customize matplotlib axis properties.\"\"\"\n    ax.spines[\"left\"].set_edgecolor(edgecolor)\n    ax.spines[\"left\"].set_linewidth(1.5)\n    ax.spines[\"bottom\"].set_edgecolor(edgecolor)\n    ax.spines[\"bottom\"].set_linewidth(1.5)\n    ax.spines[\"top\"].set_edgecolor(None)\n    ax.spines[\"right\"].set_edgecolor(None)\n    ax.tick_params(\n        top=False,\n        bottom=False,\n        left=False,\n        right=False,\n        labelleft=True,\n        labelbottom=True,\n    )\n    return ax\n\n\ndef plot_faceted_grouped_bar_chart(\n    data: pd.DataFrame,\n    xvar: str,\n    yvar: str,\n    zvar: str,\n    color_by_col: str,\n    colors: List[str],\n    xvar_type: str,\n    high_corr_categories: List[str],\n    bar_order: List[str],\n) -&gt; None:\n    \"\"\"Plot faceted grouped bar chart.\"\"\"\n    g = sns.FacetGrid(\n        data,\n        row=zvar,\n        hue=color_by_col,\n        palette=colors,\n        height=3,\n        sharey=False,\n        sharex=False,\n        aspect=3,\n    )\n    g.map(sns.barplot, xvar, yvar, order=bar_order)\n    g.fig.tight_layout(h_pad=-2)\n\n    # flatten axes into a 1-d array\n    axes = g.axes.flatten()\n\n    # iterate through the axes\n    for k, ax in enumerate(axes):\n        zvar_attr = ax.get_title().split(\" = \")[-1]\n        z_value_min = data.query(f\"{zvar} == '{zvar_attr}'\")[yvar].min()\n\n        ax.set_xlabel(None)\n        ax.set_ylabel(zvar_attr.title().replace(\"_\", \" \"), fontsize=12)\n        ax.xaxis.set_tick_params(labelsize=12)\n        ax.yaxis.set_tick_params(labelsize=12)\n        ax.tick_params(size=0)\n        ax = customize_splines(ax, None)\n        if z_value_min &lt; 0:\n            ax.axhline(0, ls=\"--\", c=\"black\")\n        ax.set_title(None)\n        ptitle = (\n            \"Highest correlation to revenue is for \"\n            f\"{', '.join(high_corr_categories)} {xvar_type}\"\n        )\n        if k == 0:\n            ax.set_title(ptitle, fontweight=\"bold\", loc=\"left\")\n        if k in range(len(bar_order)):\n            ax.set_xticklabels([])\n\n\ndef show_heatmap(\n    data: pd.DataFrame,\n    ytitle: str,\n    ptitle: str,\n    cbar_params: Union[Dict[str, float], bool] = False,\n    annot_kws: Union[Dict[str, int], bool] = False,\n    fig_size: Tuple[int] = (10, 12),\n) -&gt; None:\n    \"\"\"Plot a 2D heatmap using seaborn.\"\"\"\n    fig, ax = plt.subplots(figsize=fig_size)\n    ax = sns.heatmap(\n        data,\n        cmap=\"YlOrRd\",\n        linewidth=0.5,\n        cbar=True if cbar_params else None,\n        cbar_kws=cbar_params if cbar_params else None,\n        annot=True if annot_kws else None,\n        annot_kws=annot_kws if annot_kws else None,\n        ax=ax,\n    )\n    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=12)\n    ax.set_xticklabels(ax.get_xticklabels(), fontsize=12)\n    ax.set_xlabel(None)\n    ax.set_ylabel(ytitle, fontsize=12)\n    if cbar_params:\n        cbar = ax.collections[0].colorbar\n        cbar.ax.tick_params(size=0, labelsize=12)\n    ax.tick_params(size=0)\n    ax.set_title(\n        ptitle,\n        loc=\"left\",\n        fontsize=11,\n        fontweight=\"bold\",\n    )"
  },
  {
    "objectID": "notebooks/03-eda/notebooks/03_eda.html#get-and-prepare-data",
    "href": "notebooks/03-eda/notebooks/03_eda.html#get-and-prepare-data",
    "title": "Exploratory Data Analysis",
    "section": "Get and Prepare Data",
    "text": "Get and Prepare Data\nData will be prepared for EDA based on our learnings from the data preparation step 1. SQL to get the following for visitors who made a return (future) visit to the merchandise store during the months to be included in the training data split - attributes of their first visit - these are candidate features (X) for use in ML development - one visitor is present per row - forward-looking label (y) that indicates if each visitor made a purchase on a return visit to the store - one visitor is present per row 2. drop duplicates by fullvisitorid 3. set column datatypes for all columns\nThese three steps are enclosed into a pandas.pipeline (link), which is defined below\n\nquery = f\"\"\"\n        WITH\n        -- Step 1. get visitors with a purchase on a future visit\n        next_visit_purchasers AS (\n             SELECT fullvisitorid,\n                    IF(COUNTIF(totals.transactions &gt; 0 AND totals.newVisits IS NULL) &gt; 0, True, False) AS made_purchase_on_future_visit\n             FROM `data-to-insights.ecommerce.web_analytics`\n             WHERE date BETWEEN '{train_start_date}' AND '{test_end_date}'\n             AND geoNetwork.country = 'United States'\n             GROUP BY fullvisitorid\n        ),\n        -- Steps 2. and 3. get attributes of the first visit\n        first_visit_attributes AS (\n            SELECT -- =========== GEOSPATIAL AND TEMPORAL ATTRIBUTES OF VISIT ===========\n                   geoNetwork.country,\n                   EXTRACT(QUARTER FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS quarter,\n                   EXTRACT(MONTH FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS month,\n                   EXTRACT(DAY FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS day_of_month,\n                   EXTRACT(DAYOFWEEK FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS day_of_week,\n                   EXTRACT(HOUR FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS hour,\n                   EXTRACT(MINUTE FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS minute,\n                   EXTRACT(SECOND FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS second,\n                   -- =========== VISIT AND VISITOR METADATA ===========\n                   fullvisitorid,\n                   visitId,\n                   visitNumber,\n                   DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific') AS visitStartTime,\n                   -- =========== SOURCE OF SITE TRAFFIC ===========\n                   -- source of the traffic from which the visit was initiated\n                   trafficSource.source,\n                   -- medium of the traffic from which the visit was initiated\n                   trafficSource.medium,\n                   -- referring channel connected to visit\n                   channelGrouping,\n                   -- =========== VISITOR ACTIVITY ===========\n                   -- total number of hits\n                   (CASE WHEN totals.hits &gt; 0 THEN totals.hits ELSE 0 END) AS hits,\n                   -- number of bounces\n                   (CASE WHEN totals.bounces &gt; 0 THEN totals.bounces ELSE 0 END) AS bounces,\n                   -- action performed during first visit\n                   CAST(h.eCommerceAction.action_type AS INT64) AS action_type,\n                   -- page views\n                   IFNULL(totals.pageviews, 0) AS pageviews,\n                   -- total revenue\n                   totals.totalTransactionRevenue / 1000000 AS transact_revenue,\n                   -- time on the website\n                   IFNULL(totals.timeOnSite, 0) AS time_on_site,\n                   -- whether add-to-cart was performed during visit\n                   (CASE WHEN CAST(h.eCommerceAction.action_type AS INT64) = 3 THEN 1 ELSE 0 END) AS added_to_cart,\n                   (CASE WHEN CAST(h.eCommerceAction.action_type AS INT64) = 2 THEN 1 ELSE 0 END) AS product_details_viewed,\n                   -- =========== VISITOR DEVICES ===========\n                   -- user's browser\n                   device.browser,\n                   -- user's operating system\n                   device.operatingSystem AS os,\n                   -- user's type of device\n                   device.deviceCategory,\n                   -- =========== PROMOTION ===========\n                   h.promotion,\n                   h.promotionActionInfo AS pa_info,\n                   -- =========== PRODUCT ===========\n                   h.product,\n                   -- =========== ML LABEL (DEPENDENT VARIABLE) ===========\n                   made_purchase_on_future_visit\n            FROM `data-to-insights.ecommerce.web_analytics`,\n            UNNEST(hits) AS h\n            INNER JOIN next_visit_purchasers USING (fullvisitorid)\n            WHERE date BETWEEN '{train_start_date}' AND '{train_end_date}'\n            AND geoNetwork.country = 'United States'\n            AND totals.newVisits = 1\n        ),\n        -- Step 4. get aggregated features (attributes) per visit\n        visit_attributes AS (\n            SELECT fullvisitorid,\n                   visitId,\n                   visitNumber,\n                   visitStartTime,\n                   country,\n                   quarter,\n                   month,\n                   day_of_month,\n                   day_of_week,\n                   hour,\n                   minute,\n                   second,\n                   source,\n                   medium,\n                   channelGrouping,\n                   hits,\n                   bounces,\n                   -- get the last action performed during the first visit\n                   -- (this indicates where the visitor left off at the end of their visit)\n                   MAX(action_type) AS last_action,\n                   -- get number of products whose details were viewed\n                   SUM(product_details_viewed) AS product_detail_views,\n                   -- get number of promotions displayed and clicked during the first visit\n                   COUNT(CASE WHEN pa_info IS NOT NULL THEN pa_info.promoIsView ELSE NULL END) AS promos_displayed,\n                   COUNT(CASE WHEN pa_info IS NOT NULL THEN pa_info.promoIsClick ELSE NULL END) AS promos_clicked,\n                   -- get number of products displayed and clicked during the first visit\n                   COUNT(CASE WHEN pu.isImpression IS NULL THEN NULL ELSE 1 END) AS product_views,\n                   COUNT(CASE WHEN pu.isClick IS NULL THEN NULL ELSE 1 END) AS product_clicks,\n                   pageviews,\n                   transact_revenue,\n                   time_on_site,\n                   browser,\n                   os,\n                   deviceCategory,\n                   SUM(added_to_cart) AS added_to_cart,\n                   made_purchase_on_future_visit,\n            FROM first_visit_attributes\n            LEFT JOIN UNNEST(promotion) as p\n            LEFT JOIN UNNEST(product) as pu\n            GROUP BY fullvisitorid,\n                     visitId,\n                     visitNumber,\n                     visitStartTime,\n                     country,\n                     quarter,\n                     month,\n                     day_of_month,\n                     day_of_week,\n                     hour,\n                     minute,\n                     second,\n                     source,\n                     medium,\n                     channelGrouping,\n                     hits,\n                     bounces,\n                     pageviews,\n                     transact_revenue,\n                     time_on_site,\n                     browser,\n                     os,\n                     deviceCategory,\n                     made_purchase_on_future_visit\n        )\n        SELECT *\n        FROM visit_attributes\n        \"\"\"\ndf = (\n    run_sql_query(query, **gcp_auth_dict, show_df=False)\n    .pipe(set_datatypes, dtypes=dtypes_dict)\n    .pipe(drop_duplicates, subset=[\"fullvisitorid\"])\n)\nwith pd.option_context(\"display.max_columns\", None):\n    display(df.head())\n    display(df.tail())\n\nQuery execution start time = 2023-04-13 17:13:46.242...done at 2023-04-13 17:14:07.031 (20.788 seconds).\nQuery returned 92,859 rows\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nsource\nmedium\nchannelGrouping\nhits\nbounces\nlast_action\nproduct_detail_views\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntransact_revenue\ntime_on_site\nbrowser\nos\ndeviceCategory\nadded_to_cart\nmade_purchase_on_future_visit\n\n\n\n\n0\n8163735676529750721\n1481176805\n1\n2016-12-07 22:00:05\nUnited States\n4\n12\n7\n4\n22\n0\n5\ngoogle\ncpc\nPaid Search\n5\n0\n0\n0\n0\n0\n0\n0\n5\n&lt;NA&gt;\n109\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n1\n0897634596694862660\n1475199828\n1\n2016-09-29 18:43:48\nUnited States\n3\n9\n29\n5\n18\n43\n48\ngoogle\norganic\nOrganic Search\n21\n0\n2\n2\n9\n0\n90\n2\n19\n&lt;NA&gt;\n467\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n2\n3891893273235584028\n1478844153\n1\n2016-11-10 22:02:33\nUnited States\n4\n11\n10\n5\n22\n2\n33\nyoutube.com\nreferral\nSocial\n11\n0\n2\n1\n18\n0\n36\n1\n10\n&lt;NA&gt;\n1003\nSafari (in-app)\niOS\ntablet\n0\nFalse\n\n\n3\n6952706734234495703\n1480581075\n1\n2016-12-01 00:31:15\nUnited States\n4\n12\n1\n5\n0\n31\n15\ngoogle\norganic\nOrganic Search\n9\n0\n0\n0\n27\n1\n24\n0\n8\n&lt;NA&gt;\n141\nChrome\nAndroid\nmobile\n0\nFalse\n\n\n4\n3008949133172215443\n1475213353\n1\n2016-09-29 22:29:13\nUnited States\n3\n9\n29\n5\n22\n29\n13\ngoogle\norganic\nOrganic Search\n14\n0\n2\n3\n9\n0\n24\n3\n11\n&lt;NA&gt;\n229\nChrome\nMacintosh\ndesktop\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nsource\nmedium\nchannelGrouping\nhits\nbounces\nlast_action\nproduct_detail_views\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntransact_revenue\ntime_on_site\nbrowser\nos\ndeviceCategory\nadded_to_cart\nmade_purchase_on_future_visit\n\n\n\n\n92854\n1446285229092173706\n1478190491\n1\n2016-11-03 09:28:11\nUnited States\n4\n11\n3\n5\n9\n28\n11\n(direct)\n(none)\nDirect\n3\n0\n0\n0\n0\n0\n29\n0\n3\n&lt;NA&gt;\n224\nChrome\nChrome OS\ndesktop\n0\nTrue\n\n\n92855\n927804845306410814\n1482181118\n1\n2016-12-19 12:58:38\nUnited States\n4\n12\n19\n2\n12\n58\n38\ngoogle\norganic\nOrganic Search\n3\n0\n0\n0\n0\n0\n32\n0\n3\n&lt;NA&gt;\n83\nChrome\nLinux\ndesktop\n0\nFalse\n\n\n92856\n8969674851394509099\n1482941116\n1\n2016-12-28 08:05:16\nUnited States\n4\n12\n28\n4\n8\n5\n16\nyoutube.com\nreferral\nSocial\n3\n0\n0\n0\n0\n0\n28\n0\n3\n&lt;NA&gt;\n85\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n92857\n0486118040507415508\n1477164641\n1\n2016-10-22 12:30:41\nUnited States\n4\n10\n22\n7\n12\n30\n41\nt.co\nreferral\nSocial\n3\n0\n0\n0\n0\n0\n28\n0\n3\n&lt;NA&gt;\n78\nChrome\nWindows\ndesktop\n0\nFalse\n\n\n92858\n9088496055778533168\n1482668980\n1\n2016-12-25 04:29:40\nUnited States\n4\n12\n25\n1\n4\n29\n40\nyoutube.com\nreferral\nSocial\n3\n0\n0\n0\n0\n0\n36\n0\n3\n&lt;NA&gt;\n69\nChrome\nLinux\ndesktop\n0\nFalse\n\n\n\n\n\n\n\nGet a list of the non-categorical columns\n\nnon_categorical_columns = [c for c in list(df) if c not in categorical_columns]\n\nShow the number of unique values in all categorical columns\n\n\nCode\ndf_nunique = pd.DataFrame.from_records(\n    [{\"column\": c, \"num_unique_values\": df[c].nunique()} for c in categorical_columns]\n)\ndf_nunique\n\n\n\n\n\n\n\n\n\ncolumn\nnum_unique_values\n\n\n\n\n0\nbounces\n2\n\n\n1\nlast_action\n7\n\n\n2\nsource\n116\n\n\n3\nmedium\n7\n\n\n4\nchannelGrouping\n8\n\n\n5\nbrowser\n26\n\n\n6\nos\n15\n\n\n7\ndeviceCategory\n3\n\n\n\n\n\n\n\nDefine a custom data transformation pipeline to handle the categorical columns. In the data preparation step, this was done using pandas.value_counts() to drop in frequently occurring values based on two thresholds for minimum wanted frequency 5% and 10%. Here, we will define a custom scikit-learn.pipeline that performs the same encoding using the feature-engine library, which provides a RareLabelEncoder that accepts a threshold as the parameter tol. Two such encoders are defined below with the required thresholds and they are then placed into a scikit-learn.pipeline to transform the data\n\nencoder_05 = RareLabelEncoder(\n    tol=0.05,\n    n_categories=2,\n    variables=[v for k, v in cols_to_group.items() if \"5\" in k][0],\n    replace_with=\"other\",\n)\nencoder_10 = RareLabelEncoder(\n    tol=0.10,\n    n_categories=2,\n    variables=[v for k, v in cols_to_group.items() if \"10\" in k][0],\n    replace_with=\"other\",\n)\ncategorical_transformer = Pipeline(\n    steps=[(\"enc_05\", encoder_05), (\"enc_10\", encoder_10)]\n)\npreprocessor = ColumnTransformer(\n    transformers=[(\"cat\", categorical_transformer, categorical_columns)],\n    remainder=\"passthrough\",\n)\npipe_trans = Pipeline(steps=[(\"preprocessor\", preprocessor)])\npipe_trans\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('enc_05',\n                                                                   RareLabelEncoder(n_categories=2,\n                                                                                    replace_with='other',\n                                                                                    variables=['source',\n                                                                                               'browser'])),\n                                                                  ('enc_10',\n                                                                   RareLabelEncoder(n_categories=2,\n                                                                                    replace_with='other',\n                                                                                    tol=0.1,\n                                                                                    variables=['os',\n                                                                                               'channelGrouping',\n                                                                                               'medium']))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os',\n                                                   'deviceCategory'])]))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('enc_05',\n                                                                   RareLabelEncoder(n_categories=2,\n                                                                                    replace_with='other',\n                                                                                    variables=['source',\n                                                                                               'browser'])),\n                                                                  ('enc_10',\n                                                                   RareLabelEncoder(n_categories=2,\n                                                                                    replace_with='other',\n                                                                                    tol=0.1,\n                                                                                    variables=['os',\n                                                                                               'channelGrouping',\n                                                                                               'medium']))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os',\n                                                   'deviceCategory'])]))])preprocessor: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('enc_05',\n                                                  RareLabelEncoder(n_categories=2,\n                                                                   replace_with='other',\n                                                                   variables=['source',\n                                                                              'browser'])),\n                                                 ('enc_10',\n                                                  RareLabelEncoder(n_categories=2,\n                                                                   replace_with='other',\n                                                                   tol=0.1,\n                                                                   variables=['os',\n                                                                              'channelGrouping',\n                                                                              'medium']))]),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])cat['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']RareLabelEncoderRareLabelEncoder(n_categories=2, replace_with='other',\n                 variables=['source', 'browser'])RareLabelEncoderRareLabelEncoder(n_categories=2, replace_with='other', tol=0.1,\n                 variables=['os', 'channelGrouping', 'medium'])remainderpassthroughpassthrough\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe datatype of the categorical columns is set as pd.CategoricalDtype() in the datatypes dictionary dtypes_dict earlier. This was set as pd.StringDtype() during the data preparation step. Here, this change is necessary since the RareLabelEncoder() transformer expects to see these categrical columns appear as categories and not just as strings.\n\n\n\nApply the custom data transformation pipeline to prepare the training data split for EDA\n\n_ = pipe_trans.fit(df)\ndf = pd.DataFrame(\n    pipe_trans.transform(df), columns=categorical_columns + non_categorical_columns\n)[non_categorical_columns + categorical_columns].pipe(set_datatypes, dtypes=dtypes_dict)\n\nThe cardinality of the columns before and after grouping is shown below\n\n\nCode\ndf_nunique.merge(\n    pd.DataFrame.from_records(\n        [\n            {\n                \"column\": c,\n                \"column_grouped\": c,\n                \"num_unique_values_after_grouping\": df[c].nunique(),\n            }\n            for c in categorical_columns\n        ]\n    ).assign(column_grouped=lambda df: df[\"column_grouped\"] != df[\"column\"]),\n    on=[\"column\"],\n    how=\"left\",\n)\n\n\n\n\n\n\n\n\n\ncolumn\nnum_unique_values\ncolumn_grouped\nnum_unique_values_after_grouping\n\n\n\n\n0\nbounces\n2\nFalse\n2\n\n\n1\nlast_action\n7\nFalse\n7\n\n\n2\nsource\n116\nFalse\n5\n\n\n3\nmedium\n7\nFalse\n4\n\n\n4\nchannelGrouping\n8\nFalse\n4\n\n\n5\nbrowser\n26\nFalse\n3\n\n\n6\nos\n15\nFalse\n5\n\n\n7\ndeviceCategory\n3\nFalse\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThis is identical to the output seen during the data preparation step in the Handling Categorical Columns sub-section.\n\n\n\nThe category distributions (frequencies) after grouping are shown below for all categorical columns (including those that were grouped)\n\n\nCode\ndfs_cats_groups = []\nfor c in categorical_columns:\n    # get fraction of unique values\n    df_frequencies = (\n        df[c]\n        .value_counts()\n        .rename(\"number_of_visitors\")\n        .to_frame()\n        .merge(\n            (\n                df[c].value_counts(normalize=True).rename(\"fraction_of_visitors\") * 100\n            ).to_frame(),\n            left_index=True,\n            right_index=True,\n        )\n    )\n\n    # map unique values for last_action and bounces to get meaningful names\n    if c == \"last_action\":\n        df_frequencies.index = df_frequencies.index.map(mapper)\n    if c == \"bounces\":\n        df_frequencies.index = df_frequencies.index.map({0: False, 1: True})\n\n    # get running total of fraction (cumulative sum)\n    df_frequencies = (\n        df_frequencies.sort_values(by=[\"fraction_of_visitors\"])\n        .assign(\n            cumulative_fraction_of_visitors=lambda df: df[\n                \"fraction_of_visitors\"\n            ].cumsum(),\n            column_name=c,\n        )\n        .sort_values(by=[\"fraction_of_visitors\"], ascending=False)\n    )\n\n    # rename columns\n    df_frequencies = df_frequencies.reset_index().rename(columns={c: \"column_value\"})\n    dfs_cats_groups.append(df_frequencies)\ndf_frequencies_grouped = pd.concat(dfs_cats_groups, ignore_index=True)\ncol = df_frequencies_grouped.pop(\"column_name\")\ndf_frequencies_grouped.insert(0, col.name, col)\nwith pd.option_context(\"display.max_rows\", None):\n    display(df_frequencies_grouped)\n\n\n\n\n\n\n\n\n\ncolumn_name\ncolumn_value\nnumber_of_visitors\nfraction_of_visitors\ncumulative_fraction_of_visitors\n\n\n\n\n0\nbounces\nFalse\n65505\n70.777193\n100.000000\n\n\n1\nbounces\nTrue\n27046\n29.222807\n29.222807\n\n\n2\nlast_action\nUnknown\n67511\n72.944647\n100.000000\n\n\n3\nlast_action\nProduct detail views\n14731\n15.916630\n27.055353\n\n\n4\nlast_action\nAdd product(s) to cart\n4655\n5.029659\n11.138724\n\n\n5\nlast_action\nCompleted purchase\n3097\n3.346263\n6.109064\n\n\n6\nlast_action\nCheck out\n1635\n1.766594\n2.762801\n\n\n7\nlast_action\nRemove product(s) from cart\n881\n0.951908\n0.996207\n\n\n8\nlast_action\nClick through of product lists\n41\n0.044300\n0.044300\n\n\n9\nsource\ngoogle\n43484\n46.983825\n100.000000\n\n\n10\nsource\n(direct)\n21532\n23.265011\n53.016175\n\n\n11\nsource\nmall.googleplex.com\n11777\n12.724876\n29.751164\n\n\n12\nsource\nyoutube.com\n8021\n8.666573\n17.026288\n\n\n13\nsource\nother\n7737\n8.359715\n8.359715\n\n\n14\nmedium\norganic\n38584\n41.689447\n100.000000\n\n\n15\nmedium\nreferral\n25086\n27.105056\n58.310553\n\n\n16\nmedium\n(none)\n21532\n23.265011\n31.205498\n\n\n17\nmedium\nother\n7349\n7.940487\n7.940487\n\n\n18\nchannelGrouping\nOrganic Search\n38584\n41.689447\n100.000000\n\n\n19\nchannelGrouping\nDirect\n21532\n23.265011\n58.310553\n\n\n20\nchannelGrouping\nother\n16275\n17.584899\n35.045542\n\n\n21\nchannelGrouping\nReferral\n16160\n17.460643\n17.460643\n\n\n22\nbrowser\nChrome\n69002\n74.555650\n100.000000\n\n\n23\nbrowser\nSafari\n15294\n16.524943\n25.444350\n\n\n24\nbrowser\nother\n8255\n8.919407\n8.919407\n\n\n25\nos\nMacintosh\n30610\n33.073657\n100.000000\n\n\n26\nos\nWindows\n25588\n27.647459\n66.926343\n\n\n27\nos\niOS\n13500\n14.586552\n39.278884\n\n\n28\nos\nAndroid\n11486\n12.410455\n24.692332\n\n\n29\nos\nother\n11367\n12.281877\n12.281877\n\n\n30\ndeviceCategory\ndesktop\n67355\n72.776091\n100.000000\n\n\n31\ndeviceCategory\nmobile\n21790\n23.543776\n27.223909\n\n\n32\ndeviceCategory\ntablet\n3406\n3.680133\n3.680133\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThis is identical to the output seen in df_frequencies_grouped during the data preparation step in the Handling Categorical Columns sub-section.\n\n\n\nThe training data split is now prepared and ready for use in EDA."
  },
  {
    "objectID": "notebooks/03-eda/notebooks/03_eda.html#exploratory-data-analysis",
    "href": "notebooks/03-eda/notebooks/03_eda.html#exploratory-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\n\n\n\n\n\nNotes\n\n\n\n\nWe will interchangeably refer to each row in the data that was prepared above as visits or visitors. Since duplicates by fullvisitorid have been dropped, each row represents a unique visitor.\nFor the rest of this step (EDA), we will only use the training data as mentioned earlier in order to avoid data leakage or lookahead bias.\nWe will interchangeably refer to each visitor who made a purchase on a return visit as a return purchaser.\n\n\n\nThe first few rows of the prepared training data are shown below\n\n\nCode\nwith pd.option_context(\"display.max_rows\", None):\n    display(df.head())\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\n...\nadded_to_cart\nmade_purchase_on_future_visit\nbounces\nlast_action\nsource\nmedium\nchannelGrouping\nbrowser\nos\ndeviceCategory\n\n\n\n\n0\n8163735676529750721\n1481176805\n1\n2016-12-07 22:00:05\nUnited States\n4\n12\n7\n4\n22\n...\n0\nFalse\n0\n0\ngoogle\nother\nother\nChrome\nWindows\ndesktop\n\n\n1\n0897634596694862660\n1475199828\n1\n2016-09-29 18:43:48\nUnited States\n3\n9\n29\n5\n18\n...\n0\nFalse\n0\n2\ngoogle\norganic\nOrganic Search\nChrome\nWindows\ndesktop\n\n\n2\n3891893273235584028\n1478844153\n1\n2016-11-10 22:02:33\nUnited States\n4\n11\n10\n5\n22\n...\n0\nFalse\n0\n2\nyoutube.com\nreferral\nother\nother\niOS\ntablet\n\n\n3\n6952706734234495703\n1480581075\n1\n2016-12-01 00:31:15\nUnited States\n4\n12\n1\n5\n0\n...\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\n\n\n4\n3008949133172215443\n1475213353\n1\n2016-09-29 22:29:13\nUnited States\n3\n9\n29\n5\n22\n...\n0\nFalse\n0\n2\ngoogle\norganic\nOrganic Search\nChrome\nMacintosh\ndesktop\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nFor the categorical columns, only the frequency-encoded versions are present in the prepared data.\n\n\n\nQuestion 1. For high-level channel, and the more fine-grained traffic source and traffic medium, get the correlation between the numerical attributes of visits and total visit revenue. Across all of these groupings of sources of web traffic arriving at the merchandise store, comment on the strongest and weakest correlations to revenue.\nAt the time of the first visit, we know whether the visitor will make a purchase during a return (or future) visit to the merchandise store. This is in the made_purchase_on_future_visit column. But, we have not extracted the visit revenue that was earned during that future visit, since we have only extracted its outcome. So, we will assume that this question is referring to visit revenue earned during the first visit since the columns (including totals.transact_revenue) are only retrieved for visitors’ first visits.\nDue to the high cardinality of channel and traffic source, this question will be answered for the grouped version of these columns. The following helper function will get this correlation and will be re-used for all three traffic-related columns\n\ndef get_revenue_corr_by_attribute(\n    df: pd.DataFrame, categorical_col: str, numerical_cols: List[str]\n) -&gt; pd.DataFrame:\n    \"\"\"Get correlation between numerical attrs and revenue for categorical attribute.\"\"\"\n    # get distribution of categorical column\n    df_frequencies = df[categorical_col].value_counts(normalize=True).reset_index()\n\n    # get correlation within groupby\n    df_corr = (\n        df.groupby(  # .query(f\"{categorical_col}.isin(@popular_categories)\")\n            [categorical_col, \"made_purchase_on_future_visit\"]\n        )[numerical_cols + [\"transact_revenue\"]]\n        .corr()\n        .reset_index()[\n            [\n                \"level_2\",\n                \"transact_revenue\",\n                categorical_col,\n                \"made_purchase_on_future_visit\",\n            ]\n        ]\n        # rename columns after reset_index\n        .rename(columns={\"level_2\": \"first_visit_attribute\"})\n        # get correlation to revenue only\n        .query(\"first_visit_attribute != 'transact_revenue'\")\n        # reshape data from tidy to untidy format\n        .pivot(\n            index=[\"first_visit_attribute\", categorical_col],\n            columns=[\"made_purchase_on_future_visit\"],\n            values=[\"transact_revenue\"],\n        )\n        .reset_index()\n    )\n\n    # flatten columns in untidy data\n    df_corr.columns = [\n        \"_\".join([str(sl) for sl in c]).rstrip(\"_\")\n        for c in df_corr.columns.to_flat_index().tolist()\n    ]\n\n    # rename columns in untidy data and merge with frequencies to show distribution\n    # side-by-side with distribution\n    df_corr = (\n        df_corr.rename(\n            columns={\n                \"transact_revenue_False\": \"corr_to_1st_visit_revenue_if_no_return_purchase\",\n                \"transact_revenue_True\": \"corr_to_1st_visit_revenue_if_return_purchase\",\n            }\n        )\n        # merge with frequencies\n        .merge(df_frequencies, on=[categorical_col], how=\"left\")\n        # sort for display purposes only\n        .sort_values(\n            by=[\"first_visit_attribute\", \"proportion\"],\n            ascending=[True, False],\n        )\n    )\n    return df_corr\n\nThe following are the numerical columns that can be used to explore correlations with revenue\n\nadded_to_cart\nhits\npageviews\nproduct_detail_views\ntime_on_site\n\n\n\nCode\nnumerical_cols = [\n    \"added_to_cart\",\n    \"hits\",\n    \"pageviews\",\n    \"product_detail_views\",\n    \"time_on_site\",\n]\n\n\nThe correlations to revenue are first shown by the channel. Channel is a high-level grouping of the source of visitor traffic reaching a website. The default channel groupings used in Google Analytics tracking data are available here. We will assume that the default-level groupings are being tracked by the Google merchandise store’s analytics tracking implementation.\nThese correlations for channels are shown below\n\nhigh_corr_categories = [\"Direct\", \"Referral\"]\ndf_corr_chn = get_revenue_corr_by_attribute(\n    df, \"channelGrouping\", numerical_cols\n).assign(is_direct_referral=lambda df: df[\"channelGrouping\"].isin(high_corr_categories))\ndf_corr_chn\n\n\n\n\n\n\n\n\nfirst_visit_attribute\nchannelGrouping\ncorr_to_1st_visit_revenue_if_no_return_purchase\ncorr_to_1st_visit_revenue_if_return_purchase\nproportion\nis_direct_referral\n\n\n\n\n1\nadded_to_cart\nOrganic Search\n0.458533\n0.176038\n0.416894\nFalse\n\n\n0\nadded_to_cart\nDirect\n0.285505\n0.446968\n0.232650\nTrue\n\n\n3\nadded_to_cart\nother\n0.234132\n0.311722\n0.175849\nFalse\n\n\n2\nadded_to_cart\nReferral\n0.193194\n0.362088\n0.174606\nTrue\n\n\n5\nhits\nOrganic Search\n0.435456\n0.117423\n0.416894\nFalse\n\n\n4\nhits\nDirect\n0.201733\n0.450512\n0.232650\nTrue\n\n\n7\nhits\nother\n0.231565\n-0.065063\n0.175849\nFalse\n\n\n6\nhits\nReferral\n0.053901\n0.440263\n0.174606\nTrue\n\n\n9\npageviews\nOrganic Search\n0.429098\n0.124916\n0.416894\nFalse\n\n\n8\npageviews\nDirect\n0.195127\n0.451882\n0.232650\nTrue\n\n\n11\npageviews\nother\n0.198325\n-0.099826\n0.175849\nFalse\n\n\n10\npageviews\nReferral\n0.042360\n0.437380\n0.174606\nTrue\n\n\n13\nproduct_detail_views\nOrganic Search\n0.406835\n0.078438\n0.416894\nFalse\n\n\n12\nproduct_detail_views\nDirect\n0.208041\n0.420964\n0.232650\nTrue\n\n\n15\nproduct_detail_views\nother\n0.304971\n0.022729\n0.175849\nFalse\n\n\n14\nproduct_detail_views\nReferral\n0.070961\n0.411491\n0.174606\nTrue\n\n\n17\ntime_on_site\nOrganic Search\n0.292286\n-0.107737\n0.416894\nFalse\n\n\n16\ntime_on_site\nDirect\n0.100158\n0.233914\n0.232650\nTrue\n\n\n19\ntime_on_site\nother\n0.101952\n-0.322572\n0.175849\nFalse\n\n\n18\ntime_on_site\nReferral\n0.030110\n0.524343\n0.174606\nTrue\n\n\n\n\n\n\n\nA plot of these correlations to revenue is shown below for return purchasers (corr_to_1st_visit_revenue_if_return_purchase)\n\nplot_faceted_grouped_bar_chart(\n    data=df_corr_chn,\n    xvar=\"channelGrouping\",\n    yvar=\"corr_to_1st_visit_revenue_if_return_purchase\",\n    zvar=\"first_visit_attribute\",\n    xvar_type=\"channel groupings\",\n    color_by_col=\"is_direct_referral\",\n    colors=[\"lightgrey\", \"red\", \"lightgrey\", \"red\"],\n    bar_order=[\"Organic Search\", \"Direct\", \"other\", \"Referral\"],\n    high_corr_categories=high_corr_categories,\n)\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nFor the high-level channel groupings of (visitor) traffic source, direct and referral-based traffic are responsible for the highest correlation to revenue earned during the first visit by return purchasers (see the corr_to_1st_visit_revenue_if_return_purchase column). Combined, these two sources account for approximately 40% of all web traffic (first-time visitors) to the merchandise store’s site.\nAmong all the selected numerical attributes of first-time visitors’ visits to the store, the time spent on the site by first-time visitors reaching the store through referrals, and who later returned to make a purchase, (see first_visit_attribute = time_on_site) is most strongly correlated to visit revenue earned.\n\n\n\nThe correlations are now similarly shown by the more fine-grained traffic source column\n\n\nCode\nhigh_corr_categories = [\"other\", \"mall.googleplex.com\"]\ndf_corr_source = get_revenue_corr_by_attribute(df, \"source\", numerical_cols).assign(\n    is_other_gmail=lambda df: df[\"source\"].isin(high_corr_categories)\n)\ndf_corr_source\n\n\n\n\n\n\n\n\n\nfirst_visit_attribute\nsource\ncorr_to_1st_visit_revenue_if_no_return_purchase\ncorr_to_1st_visit_revenue_if_return_purchase\nproportion\nis_other_gmail\n\n\n\n\n1\nadded_to_cart\ngoogle\n0.420374\n0.210653\n0.469838\nFalse\n\n\n0\nadded_to_cart\n(direct)\n0.285505\n0.446968\n0.232650\nFalse\n\n\n2\nadded_to_cart\nmall.googleplex.com\n0.327064\n0.476184\n0.127249\nTrue\n\n\n4\nadded_to_cart\nyoutube.com\nNaN\nNaN\n0.086666\nFalse\n\n\n3\nadded_to_cart\nother\n0.105265\n0.486743\n0.083597\nTrue\n\n\n6\nhits\ngoogle\n0.395722\n0.027843\n0.469838\nFalse\n\n\n5\nhits\n(direct)\n0.201733\n0.450512\n0.232650\nFalse\n\n\n7\nhits\nmall.googleplex.com\n0.181825\n0.679037\n0.127249\nTrue\n\n\n9\nhits\nyoutube.com\nNaN\nNaN\n0.086666\nFalse\n\n\n8\nhits\nother\n0.112838\n0.470637\n0.083597\nTrue\n\n\n11\npageviews\ngoogle\n0.385043\n0.018214\n0.469838\nFalse\n\n\n10\npageviews\n(direct)\n0.195127\n0.451882\n0.232650\nFalse\n\n\n12\npageviews\nmall.googleplex.com\n0.168270\n0.669607\n0.127249\nTrue\n\n\n14\npageviews\nyoutube.com\nNaN\nNaN\n0.086666\nFalse\n\n\n13\npageviews\nother\n0.112363\n0.465942\n0.083597\nTrue\n\n\n16\nproduct_detail_views\ngoogle\n0.387735\n0.043626\n0.469838\nFalse\n\n\n15\nproduct_detail_views\n(direct)\n0.208041\n0.420964\n0.232650\nFalse\n\n\n17\nproduct_detail_views\nmall.googleplex.com\n0.169645\n0.630595\n0.127249\nTrue\n\n\n19\nproduct_detail_views\nyoutube.com\nNaN\nNaN\n0.086666\nFalse\n\n\n18\nproduct_detail_views\nother\n0.200577\n0.472184\n0.083597\nTrue\n\n\n21\ntime_on_site\ngoogle\n0.248978\n-0.205277\n0.469838\nFalse\n\n\n20\ntime_on_site\n(direct)\n0.100158\n0.233914\n0.232650\nFalse\n\n\n22\ntime_on_site\nmall.googleplex.com\n0.063821\n0.636119\n0.127249\nTrue\n\n\n24\ntime_on_site\nyoutube.com\nNaN\nNaN\n0.086666\nFalse\n\n\n23\ntime_on_site\nother\n0.131011\n0.556708\n0.083597\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nTraffic source is the domain through which visitors reach the store’s website (link).\nDirect traffic source indicates a visitor entered the url of the store into the search bar on a browser and so directly accessed the store’s site (link).\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nDirect traffic sources again appear as one of the highest correlations (see the corr_to_1st_visit_revenue_if_return_purchase column) to first-visit revenue earned by return purchasers for all the numerical columns (pageviews, hits, etc.), but\n\ngmail\nother (less frequent)\n\ntraffic sources have a stronger correlation to revenue.\nA Google search (through the Google search engine, or source = google) during the first visit consistently has the weakest correlation to revenue earned during that visit for all numerical columns.\nCorrelations to revenue are stronger at this more granular level (by traffic source) than by the high-level channel groupings. For example, compare correlations in corr_to_1st_visit_revenue_if_return_purchase for first_visit_attribute = hits in df_corr_chn to the same in df_corr_source.\n\n\n\nThis is finally repeated using the same helper function for the traffic medium column\n\n\nCode\nhigh_corr_categories = [\"referral\", \"(none)\"]\ndf_corr_medium = get_revenue_corr_by_attribute(df, \"medium\", numerical_cols).assign(\n    is_none_referral=lambda df: df[\"medium\"].isin(high_corr_categories)\n)\ndf_corr_medium\n\n\n\n\n\n\n\n\n\nfirst_visit_attribute\nmedium\ncorr_to_1st_visit_revenue_if_no_return_purchase\ncorr_to_1st_visit_revenue_if_return_purchase\nproportion\nis_none_referral\n\n\n\n\n1\nadded_to_cart\norganic\n0.458533\n0.176038\n0.416894\nFalse\n\n\n3\nadded_to_cart\nreferral\n0.191701\n0.361836\n0.271051\nTrue\n\n\n0\nadded_to_cart\n(none)\n0.285505\n0.446968\n0.232650\nTrue\n\n\n2\nadded_to_cart\nother\n0.262166\n0.315144\n0.079405\nFalse\n\n\n5\nhits\norganic\n0.435456\n0.117423\n0.416894\nFalse\n\n\n7\nhits\nreferral\n0.054482\n0.439544\n0.271051\nTrue\n\n\n4\nhits\n(none)\n0.201733\n0.450512\n0.232650\nTrue\n\n\n6\nhits\nother\n0.261931\n-0.064471\n0.079405\nFalse\n\n\n9\npageviews\norganic\n0.429098\n0.124916\n0.416894\nFalse\n\n\n11\npageviews\nreferral\n0.042913\n0.436655\n0.271051\nTrue\n\n\n8\npageviews\n(none)\n0.195127\n0.451882\n0.232650\nTrue\n\n\n10\npageviews\nother\n0.228142\n-0.099564\n0.079405\nFalse\n\n\n13\nproduct_detail_views\norganic\n0.406835\n0.078438\n0.416894\nFalse\n\n\n15\nproduct_detail_views\nreferral\n0.072927\n0.411055\n0.271051\nTrue\n\n\n12\nproduct_detail_views\n(none)\n0.208041\n0.420964\n0.232650\nTrue\n\n\n14\nproduct_detail_views\nother\n0.324045\n0.024184\n0.079405\nFalse\n\n\n17\ntime_on_site\norganic\n0.292286\n-0.107737\n0.416894\nFalse\n\n\n19\ntime_on_site\nreferral\n0.035118\n0.524371\n0.271051\nTrue\n\n\n16\ntime_on_site\n(none)\n0.100158\n0.233914\n0.232650\nTrue\n\n\n18\ntime_on_site\nother\n0.063565\n-0.322501\n0.079405\nFalse\n\n\n\n\n\n\n\nA plot of these correlations to revenue is shown below for return purchasers (corr_to_1st_visit_revenue_if_return_purchase)\n\nplot_faceted_grouped_bar_chart(\n    data=df_corr_medium,\n    xvar=\"medium\",\n    yvar=\"corr_to_1st_visit_revenue_if_return_purchase\",\n    zvar=\"first_visit_attribute\",\n    xvar_type=\"medium groupings\",\n    color_by_col=\"is_none_referral\",\n    colors=[\"lightgrey\", \"red\", \"red\", \"lightgrey\"],\n    bar_order=[\"organic\", \"referral\", \"(none)\", \"other\"],\n    high_corr_categories=high_corr_categories,\n)\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nTraffic medium reflects the type of traffic source that reaches the store’s website (link).\nTraffic medium is a more higher-level grouping of visitor traffic than traffic source.\n(none) refers to traffic that originates from sources which tracking information is not available. Some reasons (1, 2) for this occurrence include a\n\nvisitor directly entering the url into a browser\nvisitor clicking a browser bookmark\nvisitor clicks a link in a PDF document\netc.\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nAgain referrals appear as one of the strongest correlations to first-visit revenue. Unknown mediums give the second strongest correlation to revenue.\nCorrelations to revenue are weaker for this aggregation of traffic than by the lower-level traffic source grouping.\n\n\n\n\n\n\n\n\n\nOverall Observations for Correlations to Revenue Based on Groupings by Traffic\n\n\n\n\nRevenue earned during a first visit to the merchandise store is most strongly correlated to numerical attributes of the first visit for referral-based or direct/unknown traffic sources.\nAttributes of visitors reaching the site through a Google search have the weakest correlation to first-visit revenue earned.\nhits and pageviews are correlated to eachother. These two attributes are also correlated to product detail views, although this is weaker than the correlation between hits and pageviews. We can see the inter-correlation between these three columns from the similar\n\nvalues in the respective DataFrames (df_corr_chn, df_corr_source and df_corr_medium)\nbar relative heights in the charts\n\n\n\n\nQuestion 2. Which channels were responsible for the most first-time visitors with a purchase on their return visit? Show this comparison on a chart by channel.\n\ndf_agg_by_channel = (\n    # groupby and count fullvisitorid\n    df.groupby([\"made_purchase_on_future_visit\", \"channelGrouping\"], as_index=False)[\n        \"fullvisitorid\"\n    ]\n    .count()\n    .rename(columns={\"fullvisitorid\": \"num_visitors\"})\n    # LEFT JOIN with frequency counts by outcome (whether purchase was made or not)\n    .merge(\n        df[\"made_purchase_on_future_visit\"]\n        .value_counts()\n        .rename(\"total_visitors\")\n        .reset_index(),\n        on=\"made_purchase_on_future_visit\",\n        how=\"left\",\n    )\n    # calculate fraction from count\n    .assign(frac_visitors=lambda df: 100 * df[\"num_visitors\"] / df[\"total_visitors\"])\n    # ORDER BY for display purposes only\n    .sort_values(by=[\"made_purchase_on_future_visit\", \"num_visitors\"], ascending=False)\n)\ndf_agg_by_channel\n\n\n\n\n\n\n\n\nmade_purchase_on_future_visit\nchannelGrouping\nnum_visitors\ntotal_visitors\nfrac_visitors\n\n\n\n\n6\nTrue\nReferral\n1842\n4250\n43.341176\n\n\n4\nTrue\nDirect\n1642\n4250\n38.635294\n\n\n5\nTrue\nOrganic Search\n579\n4250\n13.623529\n\n\n7\nTrue\nother\n187\n4250\n4.400000\n\n\n1\nFalse\nOrganic Search\n38005\n88301\n43.040283\n\n\n0\nFalse\nDirect\n19890\n88301\n22.525226\n\n\n3\nFalse\nother\n16088\n88301\n18.219499\n\n\n2\nFalse\nReferral\n14318\n88301\n16.214992\n\n\n\n\n\n\n\nA plot showing the fraction of return purchasers by channel (frac_visitors) is shown below\n\nfig = plt.figure(figsize=(11, 4))\ngrid = plt.GridSpec(1, 1)\nax1 = fig.add_subplot(grid[0, 0])\n\nfor ax, c in zip([ax1], [True]):\n    data = df_agg_by_channel.query(f\"made_purchase_on_future_visit == {c}\")\n    sns.barplot(\n        x=data[\"channelGrouping\"],\n        y=data[\"frac_visitors\"],\n        palette=[\"red\", \"lightgrey\", \"red\", \"lightgrey\"],\n        ax=ax,\n    )\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\n    ax.xaxis.set_tick_params(labelsize=12)\n    ax.yaxis.set_tick_params(labelsize=12)\n    ax.tick_params(size=0)\n    ax.axhline(0, ls=\"--\", c=\"black\")\n    ax.set_title(\n        (\n            \"Direct and Referrals dominate Google Search as most \"\n            \"popular channel used by return purchasers\"\n        ),\n        loc=\"left\",\n        fontweight=\"bold\",\n        fontsize=11,\n    )\n    ax = customize_splines(ax, None)\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n(From the table df_agg_by_channel) Organic Search (Google search engine) and direct arrivals are the two dominant sources of first-time visitors to the site who do not make a purchase on a return visit.\n(From the chart) By comparison, when visitors who do make a purchase on their return visit reach the site, they are reaching the store through referrals (eg. NYTimes.com, etc.) or by directly entering the URL directly into their browser search bar. Organic search accounts for a relatively smaller fraction of such return purchasers.\n\n\n\nQuestion 3. Is there any one web-browser that return purchasers used on their first visit?\n\ndf_agg_by_browser = (\n    # groupby and count fullvisitorid\n    df.groupby([\"made_purchase_on_future_visit\", \"browser\"], as_index=False)[\n        \"fullvisitorid\"\n    ]\n    .count()\n    .rename(columns={\"fullvisitorid\": \"num_visitors\"})\n    # LEFT JOIN with frequency counts by outcome (whether purchase was made or not)\n    .merge(\n        df[\"made_purchase_on_future_visit\"]\n        .value_counts()\n        .rename(\"total_visitors\")\n        .reset_index(),\n        on=\"made_purchase_on_future_visit\",\n        how=\"left\",\n    )\n    # calculate fraction from count\n    .assign(frac_visitors=lambda df: 100 * df[\"num_visitors\"] / df[\"total_visitors\"])\n    # ORDER BY for display purposes only\n    .sort_values(by=[\"made_purchase_on_future_visit\", \"num_visitors\"], ascending=False)\n)\ndf_agg_by_browser\n\n\n\n\n\n\n\n\nmade_purchase_on_future_visit\nbrowser\nnum_visitors\ntotal_visitors\nfrac_visitors\n\n\n\n\n3\nTrue\nChrome\n4102\n4250\n96.517647\n\n\n4\nTrue\nSafari\n95\n4250\n2.235294\n\n\n5\nTrue\nother\n53\n4250\n1.247059\n\n\n0\nFalse\nChrome\n64900\n88301\n73.498601\n\n\n1\nFalse\nSafari\n15199\n88301\n17.212716\n\n\n2\nFalse\nother\n8202\n88301\n9.288683\n\n\n\n\n\n\n\nA plot showing the fraction of return purchasers by web browser (frac_visitors) is shown below\n\nfig = plt.figure(figsize=(8, 6))\ngrid = plt.GridSpec(2, 1, hspace=0)\nax1 = fig.add_subplot(grid[0, 0])\nax2 = fig.add_subplot(grid[1, 0])\n\nfor ax, c in zip([ax1, ax2], [True, False]):\n    data = df_agg_by_browser.query(f\"made_purchase_on_future_visit == {c}\")\n    sns.barplot(\n        x=data[\"browser\"],\n        y=data[\"frac_visitors\"],\n        palette=[\"red\", \"lightgrey\", \"lightgrey\"],\n        ax=ax,\n    )\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\n    ax.xaxis.set_tick_params(labelsize=12)\n    ax.yaxis.set_tick_params(labelsize=12)\n    ax.tick_params(size=0)\n    ax.axhline(0, ls=\"--\", c=\"black\")\n    if c:\n        ax.set_title(\n            \"Chrome was more predominant among return purchasers\",\n            loc=\"left\",\n            fontweight=\"bold\",\n        )\n        ax.set_xticklabels([])\n    ax = customize_splines(ax, None)\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nGoogle Chrome is the dominant browser used during the first visit by visitors who return to make a future purchase.\nThere are more first-time visitors that access the store using Safari using that visit who don’t make a purchase on their return visit, but even in this case Chrome is more frequently used.\n\n\n\nQuestion 4. Do visitors who make a purchase on their return visit to the store have a preference for the type of device (desktop, mobile, etc.) they are using during their first visit?\n\ndf_agg_by_device = (\n    # groupby and count fullvisitorid\n    df.groupby([\"made_purchase_on_future_visit\", \"deviceCategory\"], as_index=False)[\n        \"fullvisitorid\"\n    ]\n    .count()\n    .rename(columns={\"fullvisitorid\": \"num_visitors\"})\n    # LEFT JOIN with frequency counts by outcome (whether purchase was made or not)\n    .merge(\n        df[\"made_purchase_on_future_visit\"]\n        .value_counts()\n        .rename(\"total_visitors\")\n        .reset_index(),\n        on=\"made_purchase_on_future_visit\",\n        how=\"left\",\n    )\n    # calculate fraction from count\n    .assign(frac_visitors=lambda df: 100 * df[\"num_visitors\"] / df[\"total_visitors\"])\n    # ORDER BY for display purposes only\n    .sort_values(by=[\"made_purchase_on_future_visit\", \"num_visitors\"], ascending=False)\n)\ndf_agg_by_device\n\n\n\n\n\n\n\n\nmade_purchase_on_future_visit\ndeviceCategory\nnum_visitors\ntotal_visitors\nfrac_visitors\n\n\n\n\n3\nTrue\ndesktop\n4091\n4250\n96.258824\n\n\n4\nTrue\nmobile\n131\n4250\n3.082353\n\n\n5\nTrue\ntablet\n28\n4250\n0.658824\n\n\n0\nFalse\ndesktop\n63264\n88301\n71.645848\n\n\n1\nFalse\nmobile\n21659\n88301\n24.528601\n\n\n2\nFalse\ntablet\n3378\n88301\n3.825551\n\n\n\n\n\n\n\nA plot showing the fraction of return purchasers by type of computing device (frac_visitors) is shown below\n\nfig = plt.figure(figsize=(8, 6))\ngrid = plt.GridSpec(2, 1, hspace=0)\nax1 = fig.add_subplot(grid[0, 0])\nax2 = fig.add_subplot(grid[1, 0])\n\nfor ax, c in zip([ax1, ax2], [True, False]):\n    data = df_agg_by_device.query(f\"made_purchase_on_future_visit == {c}\")\n    sns.barplot(\n        x=data[\"deviceCategory\"],\n        y=data[\"frac_visitors\"],\n        palette=[\"red\", \"lightgrey\", \"lightgrey\"],\n        ax=ax,\n    )\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\n    ax.xaxis.set_tick_params(labelsize=12)\n    ax.yaxis.set_tick_params(labelsize=12)\n    ax.tick_params(size=0)\n    ax.axhline(0, ls=\"--\", c=\"black\")\n    if c:\n        ax.set_title(\n            \"Desktops were more predominant among return purchasers\",\n            loc=\"left\",\n            fontweight=\"bold\",\n        )\n        ax.set_xticklabels([])\n    ax = customize_splines(ax, None)\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThis dataset was from 2016-2017. As use of mobile phones grown since then, these numbers may have changed in favor of increased use of mobile devices (even if desktop remains the dominant type of device being currently used).\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThere are relatively more visitors whow use their mobile phones for their initial visit and who don’t make a return purchase than those that do return to make a purchase. Nonetheless, regardless of whether a future purchase is made, first-time visitors are overwhelmingly accessing the store’s website from their desktops.\n\n\n\nQuestion 5. Which traffic sources were responsible for the most return purchasers on weekdays and weekends?\nThe following columns will be used for the questions involving datetime attributes\n\nfullvisitorid\nmonth (1 - 12)\nday_of_month (1 - 31)\nday_of_week (1 - 7)\nhour\nsource (traffic source after grouping)\nmade_purchase_on_future_visit (ML label column, y)\n\n\n\nCode\ndatetime_label_cols = [\n    \"fullvisitorid\",\n    \"month\",\n    \"day_of_month\",\n    \"day_of_week\",\n    \"hour\",\n    \"source\",\n    \"made_purchase_on_future_visit\",\n]\n\n\nShow the fullvisitorid, datetime and ML label (y) columns from the prepared data\n\n\nCode\ndf[datetime_label_cols].head()\n\n\n\n\n\n\n\n\n\nfullvisitorid\nmonth\nday_of_month\nday_of_week\nhour\nsource\nmade_purchase_on_future_visit\n\n\n\n\n0\n8163735676529750721\n12\n7\n4\n22\ngoogle\nFalse\n\n\n1\n0897634596694862660\n9\n29\n5\n18\ngoogle\nFalse\n\n\n2\n3891893273235584028\n11\n10\n5\n22\nyoutube.com\nFalse\n\n\n3\n6952706734234495703\n12\n1\n5\n0\ngoogle\nFalse\n\n\n4\n3008949133172215443\n9\n29\n5\n22\ngoogle\nFalse\n\n\n\n\n\n\n\nDefinie mapping dictionaries to change month and day of week from integers to strings (month name and day name)\n\nmonth_mapper = dict(zip(range(1, 12 + 1), month_name[1:]))\nday_mapper = dict(zip(range(1, 7 + 1), [day_name[-1]] + day_name[:-1]))\n\nApply these mapping dictionaries to the prepared data to get new columns with the _name suffix and append a column to indicate if the day of the week is a weekday or weekend\n\ndf_datetime = (\n    # map month to month name\n    df.assign(month_name=lambda df: df[\"month\"].map(month_mapper))\n    # map day of week to day name\n    .assign(day_name=lambda df: df[\"day_of_week\"].map(day_mapper))\n    # map day of week to weekend check (boolean column)\n    .assign(is_weekend=lambda df: df[\"day_of_week\"].isin([1, 7]))\n)\ndf_datetime[\n    datetime_label_cols[:-1]\n    + [\"month_name\", \"is_weekend\", \"day_name\"]\n    + [datetime_label_cols[-1]]\n].head()\n\n\n\n\n\n\n\n\nfullvisitorid\nmonth\nday_of_month\nday_of_week\nhour\nsource\nmonth_name\nis_weekend\nday_name\nmade_purchase_on_future_visit\n\n\n\n\n0\n8163735676529750721\n12\n7\n4\n22\ngoogle\nDecember\nFalse\nWednesday\nFalse\n\n\n1\n0897634596694862660\n9\n29\n5\n18\ngoogle\nSeptember\nFalse\nThursday\nFalse\n\n\n2\n3891893273235584028\n11\n10\n5\n22\nyoutube.com\nNovember\nFalse\nThursday\nFalse\n\n\n3\n6952706734234495703\n12\n1\n5\n0\ngoogle\nDecember\nFalse\nThursday\nFalse\n\n\n4\n3008949133172215443\n9\n29\n5\n22\ngoogle\nSeptember\nFalse\nThursday\nFalse\n\n\n\n\n\n\n\nPerform the GROUP BY over\n\ntraffic source\nis_weekend\nwhether return purchase was made\n\nand get fraction of visitors\n\ndf_datetime_agg_source = (\n    # groupby and count fullvisitorid\n    df_datetime.groupby(\n        [\"source\", \"is_weekend\", \"made_purchase_on_future_visit\"],\n        as_index=False,\n    )\n    .agg({\"fullvisitorid\": \"count\"})\n    .rename(columns={\"fullvisitorid\": \"num_visitors\"})\n    .sort_values(by=[\"made_purchase_on_future_visit\", \"source\"])\n    # LEFT JOIN with frequency counts by outcome (whether purchase was made or not)\n    .merge(\n        df[\"made_purchase_on_future_visit\"]\n        .value_counts()\n        .rename(\"total_visitors\")\n        .reset_index(),\n        on=\"made_purchase_on_future_visit\",\n        how=\"left\",\n    )\n    # calculate fraction from count\n    .assign(frac_visitors=lambda df: 100 * df[\"num_visitors\"] / df[\"total_visitors\"])\n)\ndf_datetime_agg_source\n\n\n\n\n\n\n\n\nsource\nis_weekend\nmade_purchase_on_future_visit\nnum_visitors\ntotal_visitors\nfrac_visitors\n\n\n\n\n0\n(direct)\nFalse\nFalse\n16261\n88301\n18.415420\n\n\n1\n(direct)\nTrue\nFalse\n3629\n88301\n4.109806\n\n\n2\ngoogle\nFalse\nFalse\n32535\n88301\n36.845562\n\n\n3\ngoogle\nTrue\nFalse\n10254\n88301\n11.612553\n\n\n4\nmall.googleplex.com\nFalse\nFalse\n9349\n88301\n10.587649\n\n\n5\nmall.googleplex.com\nTrue\nFalse\n1149\n88301\n1.301231\n\n\n6\nother\nFalse\nFalse\n5957\n88301\n6.746243\n\n\n7\nother\nTrue\nFalse\n1149\n88301\n1.301231\n\n\n8\nyoutube.com\nFalse\nFalse\n5913\n88301\n6.696413\n\n\n9\nyoutube.com\nTrue\nFalse\n2105\n88301\n2.383891\n\n\n10\n(direct)\nFalse\nTrue\n1346\n4250\n31.670588\n\n\n11\n(direct)\nTrue\nTrue\n296\n4250\n6.964706\n\n\n12\ngoogle\nFalse\nTrue\n562\n4250\n13.223529\n\n\n13\ngoogle\nTrue\nTrue\n133\n4250\n3.129412\n\n\n14\nmall.googleplex.com\nFalse\nTrue\n1120\n4250\n26.352941\n\n\n15\nmall.googleplex.com\nTrue\nTrue\n159\n4250\n3.741176\n\n\n16\nother\nFalse\nTrue\n508\n4250\n11.952941\n\n\n17\nother\nTrue\nTrue\n123\n4250\n2.894118\n\n\n18\nyoutube.com\nFalse\nTrue\n2\n4250\n0.047059\n\n\n19\nyoutube.com\nTrue\nTrue\n1\n4250\n0.023529\n\n\n\n\n\n\n\nA heatmap is shown below for return purchasers (frac_visitors) and similarly for visitors who did not make a purchase on a return visit\n\nfig = plt.figure(figsize=(12, 4))\ngrid = plt.GridSpec(2, 1, hspace=0.3)\nax1 = fig.add_subplot(grid[0, 0])\nax2 = fig.add_subplot(grid[1, 0])\n\nfor ax, c, ptitle_substr, source in zip(\n    [ax1, ax2],\n    [True, False],\n    [\"returning purchasers\", \"returning non-purchasers\"],\n    [\"direct access\", \"Google Search\"],\n):\n    data = (\n        # filter\n        (\n            df_datetime_agg_source.query(\n                f\"made_purchase_on_future_visit == {c}\"\n            ).assign(\n                is_weekend=lambda df: df[\"is_weekend\"].map(\n                    {False: \"weekday\", True: \"weekend\"}\n                )\n            )\n        )\n        # reshape for heatmap\n        .pivot(\n            index=\"source\",\n            columns=\"is_weekend\",\n            values=\"frac_visitors\",\n        )\n        .astype(float)\n        .transpose()\n    )\n\n    # plot heatmap\n    ax = sns.heatmap(data, cmap=\"YlOrRd\", cbar_kws={\"pad\": 0.01}, linewidth=0.5, ax=ax)\n\n    # customize heatmap\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\n    ax.xaxis.set_tick_params(labelsize=12)\n    ax.yaxis.set_tick_params(labelsize=12, rotation=0)\n    ax.set_title(\n        f\"Most {ptitle_substr} access the site through {source}\",\n        loc=\"left\",\n        fontweight=\"bold\",\n    )\n    cbar = ax.collections[0].colorbar\n    cbar.ax.tick_params(size=0, labelsize=12)\n    if c:\n        ax.set_xticklabels([])\n    ax.tick_params(size=0)\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nDirect channels and referrals were the found earlier to be the dominant channels used by return purchasers (made_purchase_on_future_visit = True). So it is not surprising that the same two sources are the dominant ones bringing return purchasers to the site on weekedays and weekends in the top subplot. Among referrals, Gmail is the dominant referral source bringing return purchasers to the store.\n\n\n\nQuestion 6. Show the fraction of visitors who did make a purchase on their return visit to the merchandise store by month and day of the month.\n\ndf_agg = df.groupby([\"month\", \"day_of_month\"], as_index=False).agg(\n    {\"made_purchase_on_future_visit\": [\"sum\"], \"fullvisitorid\": \"count\"}\n)\ndf_agg.columns = [\"_\".join(c).rstrip(\"_\") for c in df_agg.columns.to_flat_index()]\ndf_agg = df_agg.rename(\n    columns={\n        \"made_purchase_on_future_visit_sum\": \"return_purchasers\",\n        \"fullvisitorid_count\": \"return_visitors\",\n    }\n).assign(\n    frac_return_purchasers=lambda df: 100\n    * df[\"return_purchasers\"]\n    / df[\"return_visitors\"]\n)\ndisplay(df_agg.head())\ndisplay(df_agg.tail())\n\n\n\n\n\n\n\n\nmonth\nday_of_month\nreturn_purchasers\nreturn_visitors\nfrac_return_purchasers\n\n\n\n\n0\n9\n1\n21\n719\n2.920723\n\n\n1\n9\n2\n16\n637\n2.511774\n\n\n2\n9\n3\n3\n387\n0.775194\n\n\n3\n9\n4\n7\n349\n2.005731\n\n\n4\n9\n5\n10\n468\n2.136752\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth\nday_of_month\nreturn_purchasers\nreturn_visitors\nfrac_return_purchasers\n\n\n\n\n117\n12\n27\n35\n666\n5.255255\n\n\n118\n12\n28\n29\n578\n5.017301\n\n\n119\n12\n29\n23\n578\n3.979239\n\n\n120\n12\n30\n12\n456\n2.631579\n\n\n121\n12\n31\n14\n411\n3.406326\n\n\n\n\n\n\n\nReshape into untidy data for heatmap\n\ndata = (\n    df_agg.query(\"month &gt;= 9\")\n    .assign(month_name=lambda df: df[\"month\"].map(month_mapper))\n    .pivot(index=\"day_of_month\", columns=\"month_name\", values=\"frac_return_purchasers\")\n    .astype(float)[month_name[1:][-4:]]\n)\ndata.reset_index()\n\n\n\n\n\n\n\nmonth_name\nday_of_month\nSeptember\nOctober\nNovember\nDecember\n\n\n\n\n0\n1\n2.920723\n1.856764\n6.064690\n5.893358\n\n\n1\n2\n2.511774\n1.927711\n6.045340\n5.807814\n\n\n2\n3\n0.775194\n3.901734\n5.628743\n4.012346\n\n\n3\n4\n2.005731\n2.795031\n5.248619\n5.641749\n\n\n4\n5\n2.136752\n3.342884\n6.525912\n8.067227\n\n\n5\n6\n1.804511\n2.080624\n5.765766\n5.980066\n\n\n6\n7\n2.857143\n2.524038\n8.717949\n6.422925\n\n\n7\n8\n2.374302\n2.448211\n4.845815\n4.978541\n\n\n8\n9\n3.392330\n1.498127\n6.774194\n6.532181\n\n\n9\n10\n2.375297\n4.490291\n6.464380\n4.417178\n\n\n10\n11\n1.179245\n2.747909\n8.038147\n6.019656\n\n\n11\n12\n2.797203\n4.750594\n3.267974\n5.974395\n\n\n12\n13\n3.738318\n3.864169\n6.109980\n6.043046\n\n\n13\n14\n3.096774\n4.405874\n5.542725\n4.382826\n\n\n14\n15\n2.914573\n0.946970\n5.823068\n5.307263\n\n\n15\n16\n2.834008\n1.828154\n6.195652\n4.173765\n\n\n16\n17\n1.295337\n4.599212\n8.187773\n3.623188\n\n\n17\n18\n3.053435\n3.345725\n6.415094\n3.136763\n\n\n18\n19\n4.657534\n2.613480\n5.871886\n5.152225\n\n\n19\n20\n4.194858\n2.334152\n6.370370\n3.604359\n\n\n20\n21\n3.364738\n2.610114\n6.418219\n3.607666\n\n\n21\n22\n2.368866\n1.893939\n6.991261\n4.244694\n\n\n22\n23\n2.184874\n2.119461\n8.542141\n2.597403\n\n\n23\n24\n1.262626\n2.680653\n6.099111\n3.608247\n\n\n24\n25\n1.576577\n2.882483\n6.596859\n1.969365\n\n\n25\n26\n5.117566\n5.383848\n8.543417\n4.149378\n\n\n26\n27\n2.538787\n3.942652\n6.285714\n5.255255\n\n\n27\n28\n3.389831\n5.096419\n9.018891\n5.017301\n\n\n28\n29\n1.655172\n4.022989\n8.114200\n3.979239\n\n\n29\n30\n2.398801\n3.287197\n6.719717\n2.631579\n\n\n30\n31\nNaN\n5.555556\nNaN\n3.406326\n\n\n\n\n\n\n\nShow heatmap\n\nshow_heatmap(\n    data,\n    \"Day of Month\",\n    \"% of return purchasers was higher in November 2016 than neighbouring months\",\n    {\"pad\": 0.01},\n    fig_size=(10, 12),\n)\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThe fraction of daily visitors who made a purchase on their return visit to the store (or, return purchasers) peaked during November 2016 before dropping in December. This trend not only implies seasonality in bulk buying trends at the store but also, the higher fraction before Christmas and the New Year festivities is critical to the store’s strategic planning.\n\n\n\nQuestion 7. Show the fraction of visitors who made a purchase on their return visit to the merchandise store by month and day of the week.\nAggregate to get number of\n\nvisitors\nchannels\n\nused by visitors who did and did not make purchase on a return visit\n\ndf_agg = df.groupby(\n    [\"month\", \"day_of_week\", \"made_purchase_on_future_visit\"], as_index=False\n).agg({\"fullvisitorid\": \"count\", \"channelGrouping\": \"nunique\"})\ndf_agg = df_agg.rename(\n    columns={\"channelGrouping\": \"num_channels\", \"fullvisitorid\": \"return_visitors\"}\n)\ndf_agg.head()\n\n\n\n\n\n\n\n\nmonth\nday_of_week\nmade_purchase_on_future_visit\nreturn_visitors\nnum_channels\n\n\n\n\n0\n9\n1\nFalse\n1579\n4\n\n\n1\n9\n1\nTrue\n31\n4\n\n\n2\n9\n2\nFalse\n2535\n4\n\n\n3\n9\n2\nTrue\n101\n4\n\n\n4\n9\n3\nFalse\n2670\n4\n\n\n\n\n\n\n\nPivot the data so that the number of visitors who did and did not make a return purchase are shown as separate columns (repeat for number of channels used by visitors who did and did not make a return purchase)\n\ndf_agg_untidy = df_agg.pivot(\n    index=[\"month\", \"day_of_week\"],\n    columns=[\"made_purchase_on_future_visit\"],\n    values=[\"return_visitors\", \"num_channels\"],\n).reset_index()\ndf_agg_untidy.columns = [\n    \"_\".join([str(c) for c in c_list]).rstrip(\"_\")\n    for c_list in df_agg_untidy.columns.to_flat_index()\n]\ndf_agg_untidy.head()\n\n\n\n\n\n\n\n\nmonth\nday_of_week\nreturn_visitors_False\nreturn_visitors_True\nnum_channels_False\nnum_channels_True\n\n\n\n\n0\n9\n1\n1579\n31\n4\n4\n\n\n1\n9\n2\n2535\n101\n4\n4\n\n\n2\n9\n3\n2670\n85\n4\n4\n\n\n3\n9\n4\n2867\n94\n4\n4\n\n\n4\n9\n5\n3653\n93\n4\n4\n\n\n\n\n\n\n\nGet total number of visitors and channels\n\ndf_agg_untidy[\"num_channels\"] = (\n    df_agg_untidy[\"num_channels_False\"] + df_agg_untidy[\"num_channels_True\"]\n)\ndf_agg_untidy[\"return_visitors\"] = (\n    df_agg_untidy[\"return_visitors_False\"] + df_agg_untidy[\"return_visitors_True\"]\n)\ndf_agg_untidy.head()\n\n\n\n\n\n\n\n\nmonth\nday_of_week\nreturn_visitors_False\nreturn_visitors_True\nnum_channels_False\nnum_channels_True\nnum_channels\nreturn_visitors\n\n\n\n\n0\n9\n1\n1579\n31\n4\n4\n8\n1610\n\n\n1\n9\n2\n2535\n101\n4\n4\n8\n2636\n\n\n2\n9\n3\n2670\n85\n4\n4\n8\n2755\n\n\n3\n9\n4\n2867\n94\n4\n4\n8\n2961\n\n\n4\n9\n5\n3653\n93\n4\n4\n8\n3746\n\n\n\n\n\n\n\nConvert number of visitors and channels into fractions of visitors and channels\n\ndf_agg_untidy[\"frac_channels_used_by_return_purchasers\"] = 100 * (\n    df_agg_untidy[\"num_channels_True\"] / df_agg_untidy[\"num_channels\"]\n)\ndf_agg_untidy[\"frac_return_purchasers\"] = 100 * (\n    df_agg_untidy[\"return_visitors_True\"] / df_agg_untidy[\"return_visitors\"]\n)\ndf_agg_untidy.sample(5)\n\n\n\n\n\n\n\n\nmonth\nday_of_week\nreturn_visitors_False\nreturn_visitors_True\nnum_channels_False\nnum_channels_True\nnum_channels\nreturn_visitors\nfrac_channels_used_by_return_purchasers\nfrac_return_purchasers\n\n\n\n\n18\n11\n5\n3077\n219\n4\n4\n8\n3296\n50.0\n6.644417\n\n\n12\n10\n6\n2813\n107\n4\n4\n8\n2920\n50.0\n3.664384\n\n\n25\n12\n5\n4452\n235\n4\n4\n8\n4687\n50.0\n5.013868\n\n\n24\n12\n4\n3420\n175\n4\n4\n8\n3595\n50.0\n4.867872\n\n\n22\n12\n2\n4093\n266\n4\n4\n8\n4359\n50.0\n6.102317\n\n\n\n\n\n\n\nMelt data to get back to GROUP BY format, and only keep columns with fractions (drop columns with numbers)\n\ndf_agg_tidy = df_agg_untidy.melt(\n    id_vars=[\"month\", \"day_of_week\"],\n    value_vars=[\"frac_channels_used_by_return_purchasers\", \"frac_return_purchasers\"],\n)\ndf_agg_tidy.head()\n\n\n\n\n\n\n\n\nmonth\nday_of_week\nvariable\nvalue\n\n\n\n\n0\n9\n1\nfrac_channels_used_by_return_purchasers\n50.0\n\n\n1\n9\n2\nfrac_channels_used_by_return_purchasers\n50.0\n\n\n2\n9\n3\nfrac_channels_used_by_return_purchasers\n50.0\n\n\n3\n9\n4\nfrac_channels_used_by_return_purchasers\n50.0\n\n\n4\n9\n5\nfrac_channels_used_by_return_purchasers\n50.0\n\n\n\n\n\n\n\nReshape into untidy data for heatmap\n\ndata = (\n    df_agg_tidy.query(\"month &gt;= 9\")\n    .query(\"variable == 'frac_return_purchasers'\")\n    .assign(day_name=lambda df: df[\"day_of_week\"].map(day_mapper))\n    .assign(month_name=lambda df: df[\"month\"].map(month_mapper))\n    .pivot(index=\"day_name\", columns=\"month_name\", values=\"value\")\n    .loc[list(day_name)][month_name[1:][-4:]]\n)\ndata\n\n\n\n\n\n\n\nmonth_name\nSeptember\nOctober\nNovember\nDecember\n\n\nday_name\n\n\n\n\n\n\n\n\nMonday\n3.831563\n4.208624\n7.665178\n6.102317\n\n\nTuesday\n3.085299\n2.921231\n6.609712\n5.221260\n\n\nWednesday\n3.174603\n4.089527\n6.861616\n4.867872\n\n\nThursday\n2.482648\n3.084911\n6.644417\n5.013868\n\n\nFriday\n2.682339\n3.664384\n6.577307\n4.763050\n\n\nSaturday\n1.446541\n2.252615\n6.338652\n3.883495\n\n\nSunday\n1.925466\n2.159661\n6.163328\n4.429240\n\n\n\n\n\n\n\nShow heatmap\n\nshow_heatmap(\n    data,\n    None,\n    \"% of return purchasers was consistently lower on weekends than weekdays\",\n    False,\n    annot_kws={\"size\": 14},\n    fig_size=(9, 4),\n)\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThe pool of visitors who are return purchasers shows weak evidence of following the structure of the work week since the fraction of return purchasers dropped on weekends (Saturday and Sunday) compared to weekedays.\n\n\n\nQuestion 8. Show the breakdown of total revenue earned by visitors who are return purchasers and those that are not return purchasers.\n\n\nCode\ndf.groupby([\"made_purchase_on_future_visit\"])[\n    \"transact_revenue\"\n].sum().reset_index().assign(\n    total_first_visit_revenue=lambda df: df[\"transact_revenue\"].sum()\n).assign(\n    frac_transact_revenue=lambda df: 100\n    * df[\"transact_revenue\"]\n    / df[\"total_first_visit_revenue\"]\n).query(\n    \"made_purchase_on_future_visit == True\"\n)\n\n\n\n\n\n\n\n\n\nmade_purchase_on_future_visit\ntransact_revenue\ntotal_first_visit_revenue\nfrac_transact_revenue\n\n\n\n\n1\nTrue\n90105.070312\n485441.625\n18.561464\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nReturn purchasers account for nearly 20% of total revenue earned during the first visit."
  },
  {
    "objectID": "notebooks/03-eda/notebooks/03_eda.html#key-findings",
    "href": "notebooks/03-eda/notebooks/03_eda.html#key-findings",
    "title": "Exploratory Data Analysis",
    "section": "Key Findings",
    "text": "Key Findings\n\nProducts and promotions on the store’s website are not being viewed or clicked often by first-time visitors to the site.\nDuring the months covered by the training data, only\n\n10% of all visitors to the store added a product to their shopping cart during their first visit\n4% of all first-time visitors to the store made a purchase on a return visit\n\nCorrelations between numerical attributes of visitors’ first visits and revenue earned during the first visit\n\ngrouping traffic by channel\n\ndirect traffic and referrals account for approximately 40% of all return purchasers and are responsible for the highest correlation to revenue earned during the first visit by visitors who returned to make a purchase during a return (future) visit.\n\ngrouping visitor traffic by source\n\ngmail and the combined other (less frequently used) traffic sources show the highest correlation to revenue. Direct sources ranks third.\ncorrelations are stronger at the source level (more granular) than at the channel or traffic medium level (both of which are higher-level groupings of traffic sources)\nsome of the columns namely hits, pageviews and products with detail views are correlated to eachother, so one or more of these features should be excluded during ML development\n\n\nGoogle Chrome is the dominant browser used during the first visit of visitors who return to make a future purchase.\nFor those visitors that do return and make a future purchase, they are predominantly doing so by accessing the store’s site from a desktop computer.\nBy traffic source, direct channels and referrals are the dominant sources bringing return purchasers to the site on both weekedays and weekends.\nReturn purchasers peaked during November 2016 before dropping in December.\nReturn purchasers were higher in number on weekdays than on weekends."
  },
  {
    "objectID": "notebooks/03-eda/notebooks/03_eda.html#summary-of-assumptions",
    "href": "notebooks/03-eda/notebooks/03_eda.html#summary-of-assumptions",
    "title": "Exploratory Data Analysis",
    "section": "Summary of Assumptions",
    "text": "Summary of Assumptions\nNone."
  },
  {
    "objectID": "notebooks/03-eda/notebooks/03_eda.html#summary-of-tasks-performed",
    "href": "notebooks/03-eda/notebooks/03_eda.html#summary-of-tasks-performed",
    "title": "Exploratory Data Analysis",
    "section": "Summary of Tasks Performed",
    "text": "Summary of Tasks Performed\nThis step has performed non-exhaustive EDA for insights into the prepared data."
  },
  {
    "objectID": "notebooks/03-eda/notebooks/03_eda.html#limitations",
    "href": "notebooks/03-eda/notebooks/03_eda.html#limitations",
    "title": "Exploratory Data Analysis",
    "section": "Limitations",
    "text": "Limitations\n\nLimited EDA has been performed. For columns for which we have not performed EDA, we will be relying on our intuition to determine whether they will be useful as ML features."
  },
  {
    "objectID": "notebooks/03-eda/notebooks/03_eda.html#next-step",
    "href": "notebooks/03-eda/notebooks/03_eda.html#next-step",
    "title": "Exploratory Data Analysis",
    "section": "Next Step",
    "text": "Next Step\nThe next step will transform the raw data in this dataset, in order to prepare it for machine learning, as follows\n\nextract the attributes of each visit that were recommended in this step\ndrop duplicates by fullvisitorid\nhandle high-cardinality categorical columns\n\nThis will be done separately for training, validation and test data splits. As with the (current) EDA step, we will again use the learnings from data preparation in order to avoid preparing data for ML development that suffers from data leakage/lookahead bias.\nThe data will first be split into training, validation and test splits before 1. dropping duplicates 2. creating a lookup table using the training data to create category groupings for the categorical features - as shown in this step, these groupings are used to address the issue of high-cardinality categorical features during ML model development"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_transform.html",
    "href": "notebooks/04-transform/notebooks/04_transform.html",
    "title": "Data Transformation",
    "section": "",
    "text": "Import Python modules\nCode\nimport os\nfrom datetime import datetime\nfrom functools import reduce\nfrom glob import glob\nfrom typing import Dict, List, Union\n\nimport numpy as np\nimport pandas as pd\nimport pytz\nfrom feature_engine.encoding import RareLabelEncoder\nfrom google.oauth2 import service_account\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import Pipeline"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_transform.html#about",
    "href": "notebooks/04-transform/notebooks/04_transform.html#about",
    "title": "Data Transformation",
    "section": "About",
    "text": "About\n\nObjective\nThis step of the analysis prepares data for ML model training. Training, validation and test data splits are created in order to support training a ML model to predict the propensity of new visitors to the Google Merchandise store on the Google Marketplace to make a purchase on a future visit.\n\n\nOverview of Data Transformation\nThe raw data in the dataset is provided per visit. The ML model needs to be trained to make predictions at the visit level for each visitor who made a purchase on a return visit to the store. However, useful features might exist based on\n\nactions performed by a visitor within each visit\n\nnumber of items that were added to a shopping cart during the first visit\nwhether a purchase was completed on the first visit\netc.\n\ndevice used by the visitor during the visit\n\nAll actions performed per visit are found in a nested column hits for each visit so the raw data for this column must be exploded from one row per visit to one row per action in order to extract these action-based features. Exploding a nested row is similar to the .explode() DataFrame in pandas (link). Then, the data is aggregated again by visit.\nExploding the hits column results in multiple rows where the sequence of actions is important. Features can be created based on this sequence. As an example, if a visit ended in an item(s) being added to the shopping cart, then the last action performed in the visit is a add-to-cart (encoded as an integer 3). So, a feature will be created to indicate that last action of each visit and this requires exploding the hits column, which is a nested column. Similarly, to get the browser used during a visit, a nested value device.browser needs to be extracted from the device column. Exploding the device column is not necessary for this purpose, since the there is only one browser used per visit (per the definition of a visit) so, when exploded, all rows for the visit contain the same browser. hits is a nested column in the data since it contains a list of dictionaries, while device is a dictionary. We can just directly access device.browser from the nested device column, which is supported by BigQuery. For this reason, only the hits column in the raw data needs to be exploded.\nNested columns are exploded are\n\nhits\n\nNested columns that are used without exploding are\n\ntotals\ntrafficSource\ndevice\n\n\n\nFeature Selection\nFeatures are selected based on\n\nEDA performed in previous step\nintuition about features that might be predictive of a new visitor making a purchase on a future visit to the Merchandise store\n\nThe following are the types of features that are selected 1. User-Facing Features - name of browser - type of operating system (one of Windows, Mac, Linux) - referring channel - type of device 2. General Features - hits - bounces - page views - time spent on the Marketplace website\nAs a reminder, in order to get these columns, we have developed the following approach in previous steps and the same will be used here\n\nget visitors who made a purchase on their return visit (this comes from the query immediately above)\nGet all features for the first visit (totals.newVisits = 1) for all visitors\nINNER JOIN features with the visitors who made a purchase on their return visit\n\nthis provides the features for only the visitors who made a purchase on their return visit, since we are not interested in using data for visitors who did not make a purchase on their return visit\n\nGROUP BY visit and get the last action that was performed in that visit\n\nML modeling will be performed against data at the visit level since we need to predict propensity to make a purchase during a future visit\nthe UNNEST function has exploded nested actions per visit on separate rows, so the exploded data in the first_visit_attributes CTE is per action\nsince the ML model will be trained on visits, a GROUP BY is required to convert actions into visits\nsince we are only retrieving the first visit for reach visitor (from 2. above), getting the last action that was performed in that visit indicates how far the visitor advanced in the purchase process during their first visit to the marketplace\n\nintuitively, this seems like it could be an indicator of whether the visitor will make a purchase on a return visit to the Google Merchandise store\n\na unique visit is defined by the combination of the following columns\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\n\nand so a GROUP BY must be performed over these columns. The following columns are included as part of the definition of a visit\n\ncolumns of aggregated stats per visit\n\nhits\nbounces\npageviews\ntime_on_site\n\ncolumns showing features of the traffic source that initiated a visitor’s visit\n\nsource\nmedium\nchannelGrouping\n\ncolumns with features of the visitor’s device used during a visit\n\nbrowser\nos (operating system)\ndeviceCategory (Windows, Mac or Linux)\n\n(as mentioned above) the label column is reported per visit\n\nmade_purchase_on_future_visit\n\n\nThese are columns that only change at the visit level, so we have grouped over these columns. See the two previous steps (data preparation and EDA) for more details about the structure of this SQL query to retrieve these features from the raw data.\n\n\n\n\nFeature Processing\nWithout any reduction in cardinality, the source and os (operating system) columns present the biggest problem due to their large number of unique sub-categories. If one-hot encoding is used to transform the categorical features in the training data, then tree-based models will perform poorly since the one-hot encoded data will be sparse. Bucketing to reduce the cardinality will help the performance of tree-based models. XGBoost has specific guidance to handle categorical data (1, 2).\nIn order to avoid categorical features with a high cardinality, we have only kept the most common categories for each categorical feature. A generic category of Other was assigned to all other (less commonly occurring) categories. A bucketing strategy, where all categories accounting for less than 5% or 10% of all observations per categorical have been grouped into the same bucket strongly reduced the cardinality in the training data and will again be used in this step. In the data preparation step, this approach was called frequency encoding and its main disadvantage is that the predictive power of such a feature might be reduced by such a bucketing approach.\n\n\nData\nThe raw data from the BigQuery table, that was used in the preceding (EDA) step, is again used here.\n\n\nTimeframe for Study\nFor the entire project, we have assumed that the current date is March 1, 2017. The data splits to be created are\n\ntraining\n\nSeptember 1, 2016 to December 31, 2016\n\nvalidation\n\nJanuary 1, 2017 to January 31, 2017\n\ntest\n\nFebruary 1, 2017 to February 28, 2017\n\n\nAs we mentioned in the data preparation step, during ML model development, we are only choosing first-time visitors who made a purchase on a return visit to the store between September 1, 2016 and February 31, 2017. This means each selected visitor initially visited the store starting on September 1, 2016. During this first visit, they may or may not have made a purchase. The same visitor also made a return visit to the store during which they made a purchase. This return visit is allowed to occur no later than February 28, 2017.\nWith this in mind, in the\n\ntraining data\n\neach visitor made their first visit as early as September 1, 2016\n\nthis first visit gives us the ML features (X)\n\neach such visitor could have made a return visit, in which they made a purchase, as late as February 28, 2017\n\nthe outcome of this future visit (purchase or no purchase) gives us the ML label (y)\nsince today is March 1, 2017, we know the label (y) for all these visitors\n\nfor this reason, when we prepare our data for ML development, our data preparation does not suffer from data leakage/lookahead bias (this was also discussed in the preceding two steps - data preparation and EDA)\n\nnote that each such visitor only needs to make a purchase on one or more future visits before February 28, 2017\n\n\nvalidation data\n\neach visitor made their first visit as early as January 1, 2017\neach such visitor could have made a return visit, in which they made a purchase, as late as February 28, 2017\n\ntest data\n\neach visitor made their first visit as early as February 1, 2017\neach such visitor could have made a return visit, in which they made a purchase, as late as February 28, 2017\n\n\n\n\nAssumptions\nNone.\n\n\nOutput\nA file containing the features and labels will be exported for the\n\ntraining\nvalidation\ntest\n\ndata splits."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_transform.html#user-inputs",
    "href": "notebooks/04-transform/notebooks/04_transform.html#user-inputs",
    "title": "Data Transformation",
    "section": "User Inputs",
    "text": "User Inputs\nGet relative path to project root directory\n\nPROJ_ROOT_DIR = os.path.join(os.pardir)\n\nDefine the following\n\ntrain data start date\ntrain data end date\nvalidation data start date\nvalidation data end date\ntest data start date\ntest data end date\nlist of categorical features to be used\ndictionary of columns to be frequency-encoded and minimum frequency thresholds to be used\n\nthresholds are 5% or 10%, based on our findings from the data preparation step\n\n\n\n# start and end dates\ntrain_start_date = \"20160901\"\ntrain_end_date = \"20161231\"\nval_start_date = \"20170101\"\nval_end_date = \"20170131\"\ntest_start_date = \"20170201\"\ntest_end_date = \"20170228\"\n\n# categorical column names\ncategorical_columns = [\n    \"bounces\",\n    \"last_action\",\n    \"source\",\n    \"medium\",\n    \"channelGrouping\",\n    \"browser\",\n    \"os\",\n    \"deviceCategory\",\n]\n\n# categorical columns to be grouped\ncols_to_group = {\n    \"5_pct\": [\"source\", \"browser\"],\n    \"10_pct\": [\"os\", \"channelGrouping\", \"medium\"],\n}\n\nGet path to data/processed in which the transformed data splits (training, validation and test) produced by this step will be exported\n\nprocessed_data_dir = os.path.join(PROJ_ROOT_DIR, \"data\", \"processed\")\n\nRetrieve credentials for bigquery client\n\n# Google Cloud PROJECT ID\ngcp_project_id = os.environ[\"GCP_PROJECT_ID\"]\n\nGet filepath to Google Cloud Service Account JSON key\n\nraw_data_dir = os.path.join(PROJ_ROOT_DIR, \"data\", \"raw\")\ngcp_creds_fpath = glob(os.path.join(raw_data_dir, \"*.json\"))[0]\n\nAuthenticate bigquery client and get dictionary with credentials\n\ngcp_credentials = service_account.Credentials.from_service_account_file(gcp_creds_fpath)\ngcp_auth_dict = dict(gcp_project_id=gcp_project_id, gcp_creds=gcp_credentials)\n\nCreate a mapping between action type integer and label, in order to get meaningful names from the action_type column\n\nmapper = {\n    1: \"Click through of product lists\",\n    2: \"Product detail views\",\n    3: \"Add product(s) to cart\",\n    4: \"Remove product(s) from cart\",\n    5: \"Check out\",\n    6: \"Completed purchase\",\n    7: \"Refund of purchase\",\n    8: \"Checkout options\",\n    0: \"Unknown\",\n}\n\nDefine a dictionary to change datatypes of prepared data (this was originally developed in the data preparation step)\n\ndtypes_dict = {\n    \"fullvisitorid\": pd.StringDtype(),\n    \"visitId\": pd.StringDtype(),\n    \"visitNumber\": pd.Int8Dtype(),\n    \"country\": pd.StringDtype(),\n    \"quarter\": pd.Int8Dtype(),\n    \"month\": pd.Int8Dtype(),\n    \"day_of_month\": pd.Int8Dtype(),\n    \"day_of_week\": pd.Int8Dtype(),\n    \"hour\": pd.Int8Dtype(),\n    \"minute\": pd.Int8Dtype(),\n    \"second\": pd.Int8Dtype(),\n    \"source\": pd.CategoricalDtype(),  #\n    \"medium\": pd.CategoricalDtype(),  #\n    \"channelGrouping\": pd.CategoricalDtype(),  #\n    \"hits\": pd.Int16Dtype(),\n    \"bounces\": pd.CategoricalDtype(),  #\n    \"last_action\": pd.CategoricalDtype(),  #\n    \"promos_displayed\": pd.Int16Dtype(),\n    \"promos_clicked\": pd.Int16Dtype(),\n    \"product_views\": pd.Int16Dtype(),\n    \"product_clicks\": pd.Int16Dtype(),\n    \"pageviews\": pd.Int16Dtype(),\n    \"time_on_site\": pd.Int16Dtype(),\n    \"browser\": pd.CategoricalDtype(),  #\n    \"os\": pd.CategoricalDtype(),  #\n    \"added_to_cart\": pd.Int16Dtype(),\n    \"deviceCategory\": pd.CategoricalDtype(),  #\n    \"made_purchase_on_future_visit\": pd.BooleanDtype(),\n}\n\nDefine a Python helper function to perform the following\n\nexecute a SQL query using Google BigQuery\nset column datatypes of a pandas.DataFrame\ndrop duplicates based on a list of columns in a pandas.DataFrame\n\n\ndef run_sql_query(\n    query: str,\n    gcp_project_id: str,\n    gcp_creds: os.PathLike,\n    show_dtypes: bool = False,\n    show_info: bool = False,\n    show_df: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Run query on BigQuery and return results as pandas.DataFrame.\"\"\"\n    start_time = datetime.now(pytz.timezone(\"US/Eastern\"))\n    start_time_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n    print(f\"Query execution start time = {start_time_str[:-3]}...\", end=\"\")\n    df = pd.read_gbq(\n        query,\n        project_id=gcp_project_id,\n        credentials=gcp_creds,\n        dialect=\"standard\",\n        # configuration is optional, since default for query caching is True\n        configuration={\"query\": {\"useQueryCache\": True}},\n        # use_bqstorage_api=True,\n    )\n    end_time = datetime.now(pytz.timezone(\"US/Eastern\"))\n    end_time_str = end_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n    duration = end_time - start_time\n    duration = duration.seconds + (duration.microseconds / 1_000_000)\n    print(f\"done at {end_time_str[:-3]} ({duration:.3f} seconds).\")\n    print(f\"Query returned {len(df):,} rows\")\n    if show_df:\n        with pd.option_context(\"display.max_columns\", None):\n            display(df)\n    if show_dtypes:\n        display(df.dtypes.rename(\"dtype\").to_frame().transpose())\n    if show_info:\n        df.info()\n    return df\n\n\ndef set_datatypes(df: pd.DataFrame, dtypes: Dict) -&gt; pd.DataFrame:\n    \"\"\"Set DataFrame datatypes using dictionary.\"\"\"\n    df = df.astype(dtypes)\n    return df\n\n\ndef drop_duplicates(df: pd.DataFrame, subset: List[str]) -&gt; pd.DataFrame:\n    \"\"\"Drop duplicates.\"\"\"\n    df = df.drop_duplicates(subset=subset, keep=\"first\")\n    return df"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_transform.html#get-data",
    "href": "notebooks/04-transform/notebooks/04_transform.html#get-data",
    "title": "Data Transformation",
    "section": "Get Data",
    "text": "Get Data\nDefine the same custom data transformation pipeline to handle the categorical columns that was developed in the data preparation step and used in the EDA step\n\nencoder_05 = RareLabelEncoder(\n    tol=0.05,\n    n_categories=2,\n    variables=[v for k, v in cols_to_group.items() if \"5\" in k][0],\n    replace_with=\"other\",\n)\nencoder_10 = RareLabelEncoder(\n    tol=0.10,\n    n_categories=2,\n    variables=[v for k, v in cols_to_group.items() if \"10\" in k][0],\n    replace_with=\"other\",\n)\ncategorical_transformer = Pipeline(\n    steps=[(\"enc_05\", encoder_05), (\"enc_10\", encoder_10)]\n)\npreprocessor = ColumnTransformer(\n    transformers=[(\"cat\", categorical_transformer, categorical_columns)],\n    remainder=\"passthrough\",\n)\npipe_trans = Pipeline(steps=[(\"preprocessor\", preprocessor)])\npipe_trans\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('enc_05',\n                                                                   RareLabelEncoder(n_categories=2,\n                                                                                    replace_with='other',\n                                                                                    variables=['source',\n                                                                                               'browser'])),\n                                                                  ('enc_10',\n                                                                   RareLabelEncoder(n_categories=2,\n                                                                                    replace_with='other',\n                                                                                    tol=0.1,\n                                                                                    variables=['os',\n                                                                                               'channelGrouping',\n                                                                                               'medium']))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os',\n                                                   'deviceCategory'])]))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('cat',\n                                                  Pipeline(steps=[('enc_05',\n                                                                   RareLabelEncoder(n_categories=2,\n                                                                                    replace_with='other',\n                                                                                    variables=['source',\n                                                                                               'browser'])),\n                                                                  ('enc_10',\n                                                                   RareLabelEncoder(n_categories=2,\n                                                                                    replace_with='other',\n                                                                                    tol=0.1,\n                                                                                    variables=['os',\n                                                                                               'channelGrouping',\n                                                                                               'medium']))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os',\n                                                   'deviceCategory'])]))])preprocessor: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('cat',\n                                 Pipeline(steps=[('enc_05',\n                                                  RareLabelEncoder(n_categories=2,\n                                                                   replace_with='other',\n                                                                   variables=['source',\n                                                                              'browser'])),\n                                                 ('enc_10',\n                                                  RareLabelEncoder(n_categories=2,\n                                                                   replace_with='other',\n                                                                   tol=0.1,\n                                                                   variables=['os',\n                                                                              'channelGrouping',\n                                                                              'medium']))]),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])cat['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']RareLabelEncoderRareLabelEncoder(n_categories=2, replace_with='other',\n                 variables=['source', 'browser'])RareLabelEncoderRareLabelEncoder(n_categories=2, replace_with='other', tol=0.1,\n                 variables=['os', 'channelGrouping', 'medium'])remainderpassthroughpassthrough\n\n\nA helper function will be created to programmatically load data from BigQuery based on the desired start and end dates. The function accepts the following\n\nstart and end dates for which data is to be retrieved\n\nthese dates will be different for the training, validation and test splits\n\nstart date for the training data and end date for the test data\n\nthese two dates define the period over which ML model development will occur\nthese are used to retrieve visitors who made a purchase on a return (future) visit to the store during this period\n\n\nThe function is defined below\n\ndef get_sql_query(\n    split_start_date: str,\n    split_end_date: str,\n    train_split_start_date: str,\n    test_split_end_date: str,\n) -&gt; str:\n    \"\"\"Assemble query to retrieve attributes about first visits.\"\"\"\n    query_str = f\"\"\"\n            WITH\n            -- Step 1. get visitors with a purchase on a future visit\n            next_visit_purchasers AS (\n                 SELECT fullvisitorid,\n                        IF(COUNTIF(totals.transactions &gt; 0 AND totals.newVisits IS NULL) &gt; 0, True, False) AS made_purchase_on_future_visit\n                 FROM `data-to-insights.ecommerce.web_analytics`\n                 WHERE date BETWEEN '{train_split_start_date}' AND '{test_split_end_date}'\n                 AND geoNetwork.country = 'United States'\n                 GROUP BY fullvisitorid\n            ),\n            -- Steps 2. and 3. get attributes of the first visit\n            first_visit_attributes AS (\n                SELECT -- =========== GEOSPATIAL AND TEMPORAL ATTRIBUTES OF VISIT ===========\n                       geoNetwork.country,\n                       EXTRACT(QUARTER FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS quarter,\n                       EXTRACT(MONTH FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS month,\n                       EXTRACT(DAY FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS day_of_month,\n                       EXTRACT(DAYOFWEEK FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS day_of_week,\n                       EXTRACT(HOUR FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS hour,\n                       EXTRACT(MINUTE FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS minute,\n                       EXTRACT(SECOND FROM DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific')) AS second,\n                       -- =========== VISIT AND VISITOR METADATA ===========\n                       fullvisitorid,\n                       visitId,\n                       visitNumber,\n                       DATETIME(TIMESTAMP(TIMESTAMP_SECONDS(visitStartTime)), 'US/Pacific') AS visitStartTime,\n                       -- =========== SOURCE OF SITE TRAFFIC ===========\n                       -- source of the traffic from which the visit was initiated\n                       trafficSource.source,\n                       -- medium of the traffic from which the visit was initiated\n                       trafficSource.medium,\n                       -- referring channel connected to visit\n                       channelGrouping,\n                       -- =========== VISITOR ACTIVITY ===========\n                       -- total number of hits\n                       (CASE WHEN totals.hits &gt; 0 THEN totals.hits ELSE 0 END) AS hits,\n                       -- number of bounces\n                       (CASE WHEN totals.bounces &gt; 0 THEN totals.bounces ELSE 0 END) AS bounces,\n                       -- action performed during first visit\n                       CAST(h.eCommerceAction.action_type AS INT64) AS action_type,\n                       -- page views\n                       IFNULL(totals.pageviews, 0) AS pageviews,\n                       -- time on the website\n                       IFNULL(totals.timeOnSite, 0) AS time_on_site,\n                       -- whether add-to-cart was performed during visit\n                       (CASE WHEN CAST(h.eCommerceAction.action_type AS INT64) = 3 THEN 1 ELSE 0 END) AS added_to_cart,\n                       -- =========== VISITOR DEVICES ===========\n                       -- user's browser\n                       device.browser,\n                       -- user's operating system\n                       device.operatingSystem AS os,\n                       -- user's type of device\n                       device.deviceCategory,\n                       -- =========== PROMOTION ===========\n                       h.promotion,\n                       h.promotionActionInfo AS pa_info,\n                       -- =========== PRODUCT ===========\n                       h.product,\n                       -- =========== ML LABEL (DEPENDENT VARIABLE) ===========\n                       made_purchase_on_future_visit\n                FROM `data-to-insights.ecommerce.web_analytics`,\n                UNNEST(hits) AS h\n                INNER JOIN next_visit_purchasers USING (fullvisitorid)\n                WHERE date BETWEEN '{split_start_date}' AND '{split_end_date}'\n                AND geoNetwork.country = 'United States'\n                AND totals.newVisits = 1\n            ),\n            -- Step 4. get aggregated features (attributes) per visit\n            visit_attributes AS (\n                SELECT fullvisitorid,\n                       visitId,\n                       visitNumber,\n                       visitStartTime,\n                       country,\n                       quarter,\n                       month,\n                       day_of_month,\n                       day_of_week,\n                       hour,\n                       minute,\n                       second,\n                       source,\n                       medium,\n                       channelGrouping,\n                       hits,\n                       bounces,\n                       -- get the last action performed during the first visit\n                       -- (this indicates where the visitor left off at the end of their visit)\n                       MAX(action_type) AS last_action,\n                       -- get number of promotions displayed and clicked during the first visit\n                       COUNT(CASE WHEN pa_info IS NOT NULL THEN pa_info.promoIsView ELSE NULL END) AS promos_displayed,\n                       COUNT(CASE WHEN pa_info IS NOT NULL THEN pa_info.promoIsClick ELSE NULL END) AS promos_clicked,\n                       -- get number of products displayed and clicked during the first visit\n                       COUNT(CASE WHEN pu.isImpression IS NULL THEN NULL ELSE 1 END) AS product_views,\n                       COUNT(CASE WHEN pu.isClick IS NULL THEN NULL ELSE 1 END) AS product_clicks,\n                       pageviews,\n                       time_on_site,\n                       browser,\n                       os,\n                       deviceCategory,\n                       SUM(added_to_cart) AS added_to_cart,\n                       made_purchase_on_future_visit,\n                FROM first_visit_attributes\n                LEFT JOIN UNNEST(promotion) as p\n                LEFT JOIN UNNEST(product) as pu\n                GROUP BY fullvisitorid,\n                         visitId,\n                         visitNumber,\n                         visitStartTime,\n                         country,\n                         quarter,\n                         month,\n                         day_of_month,\n                         day_of_week,\n                         hour,\n                         minute,\n                         second,\n                         source,\n                         medium,\n                         channelGrouping,\n                         hits,\n                         bounces,\n                         pageviews,\n                         time_on_site,\n                         browser,\n                         os,\n                         deviceCategory,\n                         made_purchase_on_future_visit\n            )\n            SELECT *\n            FROM visit_attributes\n            \"\"\"\n    return query_str\n\n\nCreate Training Data\nThe training data will be prepared following the identical approach used in the EDA step, namely\n\nload data from Google BigQuery dataset using the Python bigquery client\nset datatypes to support frequency encoding of categorical features\n\nall categorical features must have the datatype pd.CategoricalDtype()\n\ndrop duplicates by fullvisitorid\nperform frequency-encoding on categorical features to reduce cardinality\n\n\n# load data from BigQuery dataset, set datatypes and drop duplicates\nquery = get_sql_query(train_start_date, train_end_date, train_start_date, test_end_date)\ndf_train = (\n    run_sql_query(query, **gcp_auth_dict, show_df=False)\n    .pipe(set_datatypes, dtypes=dtypes_dict)\n    .pipe(drop_duplicates, subset=[\"fullvisitorid\"])\n)\nprint(\n    f\"Got {len(df_train):,} rows and {df_train.shape[1]:,} columns \"\n    \"after dropping duplicates\"\n)\n\n# perform frequency-encoding on categorical features\n# # Get a list of the non-categorical columns\nnon_categorical_columns = [c for c in list(df_train) if c not in categorical_columns]\n# # Apply the custom data transformation pipeline to prepare the training data split\n_ = pipe_trans.fit(df_train)\ndf_train = pd.DataFrame(\n    pipe_trans.transform(df_train),\n    columns=categorical_columns + non_categorical_columns,\n)[non_categorical_columns + categorical_columns].pipe(set_datatypes, dtypes=dtypes_dict)\nprint(\n    f\"Got {len(df_train):,} rows and {df_train.shape[1]:,} columns after \"\n    \"frequency-encoding categorical features\"\n)\n\nwith pd.option_context(\"display.max_columns\", None):\n    display(df_train.head())\n    display(df_train.tail())\n\nQuery execution start time = 2023-04-13 17:38:11.057...done at 2023-04-13 17:38:31.166 (20.109 seconds).\nQuery returned 92,859 rows\nGot 92,551 rows and 29 columns after dropping duplicates\nGot 92,551 rows and 29 columns after frequency-encoding categorical features\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nhits\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntime_on_site\nadded_to_cart\nmade_purchase_on_future_visit\nbounces\nlast_action\nsource\nmedium\nchannelGrouping\nbrowser\nos\ndeviceCategory\n\n\n\n\n0\n4180680121446408775\n1476943855\n1\n2016-10-19 23:10:55\nUnited States\n4\n10\n19\n4\n23\n10\n55\n5\n0\n0\n0\n0\n5\n292\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\n\n\n1\n3072592563711482446\n1476880065\n1\n2016-10-19 05:27:45\nUnited States\n4\n10\n19\n4\n5\n27\n45\n18\n27\n1\n36\n0\n9\n317\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nAndroid\nmobile\n\n\n2\n1687301606877489412\n1477794145\n1\n2016-10-29 19:22:25\nUnited States\n4\n10\n29\n7\n19\n22\n25\n5\n9\n0\n0\n0\n5\n54\n0\nFalse\n0\n0\nyoutube.com\nreferral\nother\nother\nWindows\ndesktop\n\n\n3\n796191439564725883\n1473279331\n1\n2016-09-07 13:15:31\nUnited States\n3\n9\n7\n4\n13\n15\n31\n7\n9\n0\n0\n0\n7\n2494\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nWindows\ndesktop\n\n\n4\n9194147359170837949\n1478035636\n1\n2016-11-01 14:27:16\nUnited States\n4\n11\n1\n3\n14\n27\n16\n7\n9\n0\n0\n0\n4\n174\n0\nFalse\n0\n0\nyoutube.com\nreferral\nother\nChrome\nAndroid\nmobile\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nhits\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntime_on_site\nadded_to_cart\nmade_purchase_on_future_visit\nbounces\nlast_action\nsource\nmedium\nchannelGrouping\nbrowser\nos\ndeviceCategory\n\n\n\n\n92546\n6912704476581951344\n1481395682\n1\n2016-12-10 10:48:02\nUnited States\n4\n12\n10\n7\n10\n48\n2\n3\n0\n0\n30\n0\n3\n70\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nWindows\ndesktop\n\n\n92547\n253693082293291936\n1475785004\n1\n2016-10-06 13:16:44\nUnited States\n4\n10\n6\n5\n13\n16\n44\n3\n0\n0\n27\n0\n3\n30\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nSafari\niOS\nmobile\n\n\n92548\n0745341923593722891\n1481152121\n1\n2016-12-07 15:08:41\nUnited States\n4\n12\n7\n4\n15\n8\n41\n3\n0\n0\n36\n0\n3\n182\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nMacintosh\ndesktop\n\n\n92549\n3099760389535632682\n1475112188\n1\n2016-09-28 18:23:08\nUnited States\n3\n9\n28\n4\n18\n23\n8\n3\n0\n0\n9\n0\n3\n40\n0\nFalse\n0\n0\nother\nreferral\nother\nother\niOS\nmobile\n\n\n92550\n7973052081416467006\n1482541637\n1\n2016-12-23 17:07:17\nUnited States\n4\n12\n23\n6\n17\n7\n17\n3\n0\n0\n33\n0\n3\n64\n0\nFalse\n0\n0\nyoutube.com\nreferral\nother\nChrome\nMacintosh\ndesktop\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nGoogle Analytics uses visits and sessions interchangeably (1, 2)\n\na visit identifies a user reaching the marketplace website\n\na visitor can have multiple visits since they can visit the marketplace website multiple times\n\na session captures a visitor’s interactions on the site during a visit\n\na session begins at the start of the visit and ends after 30 minutes of inactivity by the visitor\n\n\nEach row here corresponds to a single action performed by a single visitor during a visit.\nThe made_purchase_on_future_visit is the label for ML training. However, this column is currently shown at the user action level (since the visits were exploded using the UNNEST function on the hits column). The label value only changes at the visit level since we only know if a visitor will make a purchase on their return (or future) visit after that visit has ended and that applies to the entire visit. So, we can aggregate over this column (include this column in the GROUP BY) in order to get it at the visit level\n\nthis column indicates if a visitor makes a purchase during their next visit\na ML model will be trained to predict this probability (propensity) of making a purchase during the return visit to the Merchandise store\nthe ML model will be trained on features of the same visitor’s first visit\nthis is a forward-looking label (y)\n\nWe had to select totals.newVisits = 1 since we only wanted ML features from the first visit. We can’t use features from the return visit since we want to predict the outcome of the return visit before of that visit has occurred. Earlier, we selected visitors who made a purchase on a future visit. So, for these visitors, the ML features (X) will be extracted from these visitors’ first visit only.**\n\n\n\nThe start and end visitStartTimes match those specified by the required start (train_start_date) and end (train_end_date) dates of the training data split\n\n\nCode\nassert df_train[\"visitStartTime\"].min().month == int(train_start_date[4:6])\nassert df_train[\"visitStartTime\"].max().month == int(train_end_date[4:6])\nprint(\n    f\"Visit Start Times: {df_train['visitStartTime'].min().strftime('%Y-%m-%d %H:%M:%S')} - \"\n    f\"{df_train['visitStartTime'].max().strftime('%Y-%m-%d %H:%M:%S')}\"\n)\n\n\nVisit Start Times: 2016-09-01 00:07:35 - 2016-12-31 23:56:56\n\n\nBelow is the class imbalance in the ML labels (y_train) from the training data and the number of unique values in all categorical columns\n\n\nCode\ndisplay(\n    (\n        100\n        * df_train[\"made_purchase_on_future_visit\"]\n        .value_counts(normalize=True)\n        .rename(\"fraction\")\n        .to_frame()\n    )\n    .merge(\n        df_train[\"made_purchase_on_future_visit\"]\n        .value_counts()\n        .rename(\"number\")\n        .to_frame(),\n        how=\"left\",\n        left_index=True,\n        right_index=True,\n    )\n    .reset_index()\n)\ndisplay(\n    pd.DataFrame.from_records(\n        [\n            {\n                \"column\": c,\n                \"num_unique_values\": df_train[c].nunique(),\n            }\n            for c in categorical_columns\n        ]\n    )\n)\n\n\n\n\n\n\n\n\n\nmade_purchase_on_future_visit\nfraction\nnumber\n\n\n\n\n0\nFalse\n95.407937\n88301\n\n\n1\nTrue\n4.592063\n4250\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolumn\nnum_unique_values\n\n\n\n\n0\nbounces\n2\n\n\n1\nlast_action\n7\n\n\n2\nsource\n5\n\n\n3\nmedium\n4\n\n\n4\nchannelGrouping\n4\n\n\n5\nbrowser\n3\n\n\n6\nos\n5\n\n\n7\ndeviceCategory\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nClass Imbalance\n\nwe will want to consider random undersampling, or downsampling to improve the imbalance ratio from approximately 20:1 in favor of the majority class to a ratio such as 10:1 or 5:1. This will help the ML model in two ways\n\nreducing the degree of imbalance will learn from a relatively larger number of positive examples (visitor did make a purchase on a future visit - we are interested in these visitors) than in the raw data which will have a larger number negative examples (visitor did not make a purchase on a future visit - we are not interested in these visitors for the current business use-case)\nkeeping the true class imbalance is also inefficient in terms of training time since the model spends most of its time learning from uninteresting examples\n\nreducing the class imbalance results in shorter model training times\n\n\nOther approaches to handle the class imbalance are\n\ndo nothing\n\ntrain the model using the true distribution of the classes\nif a model trained on the true imbalanced distribution can generalize to to unseen data then no undersampling is required\nthe disadvantage of this approach is that longer training time will be required\n\nuse a data-augmentation technique such as SMOTE\n\n\n\n\n\nShow a count of missing values in all the columns\n\n\nCode\ndf_train.isna().sum().rename(\"missing\").reset_index().rename(\n    columns={\"index\": \"column\"}\n)\n\n\n\n\n\n\n\n\n\ncolumn\nmissing\n\n\n\n\n0\nfullvisitorid\n0\n\n\n1\nvisitId\n0\n\n\n2\nvisitNumber\n0\n\n\n3\nvisitStartTime\n0\n\n\n4\ncountry\n0\n\n\n5\nquarter\n0\n\n\n6\nmonth\n0\n\n\n7\nday_of_month\n0\n\n\n8\nday_of_week\n0\n\n\n9\nhour\n0\n\n\n10\nminute\n0\n\n\n11\nsecond\n0\n\n\n12\nhits\n0\n\n\n13\npromos_displayed\n0\n\n\n14\npromos_clicked\n0\n\n\n15\nproduct_views\n0\n\n\n16\nproduct_clicks\n0\n\n\n17\npageviews\n0\n\n\n18\ntime_on_site\n0\n\n\n19\nadded_to_cart\n0\n\n\n20\nmade_purchase_on_future_visit\n0\n\n\n21\nbounces\n0\n\n\n22\nlast_action\n0\n\n\n23\nsource\n0\n\n\n24\nmedium\n0\n\n\n25\nchannelGrouping\n0\n\n\n26\nbrowser\n0\n\n\n27\nos\n0\n\n\n28\ndeviceCategory\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nSince the Google Analytics data is reported by visit, missing values in the General features columns\n\nhits\nbounces\npageviews\ntime_on_site\n\nwould indicate zeros. Zeros were already present in the raw data so there were no missing values in these columns that had to be filled as part of the data preparation SQL query used here.\nThe manually chosen set of User-Facing features (all of which are categoricals) don’t have missing values.\nFor both types of features, other choices than the ones made here could have missing values. In that case, feature imputation would be necessary without data leakage/lookahead bias. For the choices in this step, imputing missing values is not necessary in the features aggregated at the visit level.\n\n\n\nWe’ll now assessing zeros in the General Features Columns\n\nhits\n\nhits are users’ interactions on the merchandise store’s website that sends data to the Google Analytics server (1, 2)\na visit is a group of hits (1)\nexamples include\n\nviewing a page\nsocial media interactions, like sharing or liking content using share on social media buttons\ne-commerce interactions (add to cart, remove from cart, make purchase, etc.)\nuser timings (loading a page, loading an image, clicking a button)\netc.\n\nthere is no occurrence of zero hits in the Merchandise Store’s Google Analytics dataset, so the minimum number of hits is 1, likely since viewing a page (which always occurs) is considered a hit\n\ntime_on_site (total duration of a single visit)\n\nreaching the site triggers the start of a visit\ntime on site is calculated as the difference between the timestamp of the last and first pages of a visit\na zero indicates the user did not navigate to further pages or trigger events on the site after reaching the site (1)\n\nbounces\n\na bounce is when a single request is submitted from scripts embedded in the merchandise store’s website to the Google Analytics server (1)\n\nif a bounce occurs, then time spent on the site is zero and a single page has most likely been viewed (pageviews)\na zero indicates the absence of a bounced visit, while 1 indicates a bounce was present\n\n\n\nIf a bounce occurs then, by definition, the following are true\n\nzero time on site\na single hit is registered\n(predominantly) a single page gets viewed\n\n\n# time on site versus bounces\ndisplay(\n    (\n        (\n            100\n            * df_train.query(\"bounces == 1\")[\"time_on_site\"].value_counts(\n                normalize=True\n            )\n        )\n        .rename(\"frequency\")\n        .to_frame()\n        .reset_index()\n        .rename(columns={\"index\": \"time_on_site\"})\n    )\n    .assign(split=\"train\")\n    .assign(time_on_site=0)\n    .iloc[[0]]\n)\n\n# number of pages viewed versus bounces\ndisplay(\n    (\n        (100 * df_train.query(\"bounces == 1\")[\"pageviews\"].value_counts(normalize=True))\n        .rename(\"frequency\")\n        .to_frame()\n        .reset_index()\n        .rename(columns={\"index\": \"pageviews\"})\n    )\n    .assign(split=\"train\")\n    .assign(time_on_site=0)\n    .iloc[[0]]\n)\n\n# hits versus bounces\ndisplay(\n    (\n        (100 * df_train.query(\"bounces == 1\")[\"hits\"].value_counts(normalize=True))\n        .rename(\"frequency\")\n        .to_frame()\n        .reset_index()\n        .rename(columns={\"index\": \"hits\"})\n    )\n    .assign(split=\"train\")\n    .assign(time_on_site=0)\n    .iloc[[0]]\n)\n\n\n\n\n\n\n\n\ntime_on_site\nfrequency\nsplit\n\n\n\n\n0\n0\n100.0\ntrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npageviews\nfrequency\nsplit\ntime_on_site\n\n\n\n\n0\n1\n100.0\ntrain\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhits\nfrequency\nsplit\ntime_on_site\n\n\n\n\n0\n1\n99.167869\ntrain\n0\n\n\n\n\n\n\n\nSimilarly, if zero time is spent on the site then this is almost always associated with\n\na bounce occurs\nsingle page view\nsingle hit\n\n\n# bounce\ndisplay(\n    (\n        (\n            100\n            * df_train.query(\"time_on_site == 0\")[\"bounces\"].value_counts(\n                normalize=True\n            )\n        )\n        .rename(\"frequency\")\n        .to_frame()\n        .reset_index()\n        .rename(columns={\"index\": \"bounces\"})\n    )\n    .assign(split=\"train\")\n    .assign(time_on_site=0)\n    .iloc[[0]]\n)\n# single page view\ndisplay(\n    (\n        (\n            (\n                100\n                * df_train.query(\"time_on_site == 0\")[\"pageviews\"].value_counts(\n                    normalize=True\n                )\n            )\n            .rename(\"frequency\")\n            .to_frame()\n            .reset_index()\n            .rename(columns={\"index\": \"pageviews\"})\n        )\n    )\n    .assign(split=\"train\")\n    .assign(time_on_site=0)\n    .iloc[[0]]\n)\n# hits\ndisplay(\n    (\n        (100 * df_train.query(\"time_on_site == 0\")[\"hits\"].value_counts(normalize=True))\n        .rename(\"frequency\")\n        .to_frame()\n        .reset_index()\n        .rename(columns={\"index\": \"hits\"})\n    )\n    .assign(split=\"train\")\n    .assign(time_on_site=0)\n    .iloc[[0]]\n)\n\n\n\n\n\n\n\n\nbounces\nfrequency\nsplit\ntime_on_site\n\n\n\n\n0\n1\n99.830164\ntrain\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npageviews\nfrequency\nsplit\ntime_on_site\n\n\n\n\n0\n1\n99.863393\ntrain\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhits\nfrequency\nsplit\ntime_on_site\n\n\n\n\n0\n1\n99.010522\ntrain\n0\n\n\n\n\n\n\n\nSimilarly, if a single hit occurs on the e-commerce site then this is almost always associated with\n\na bounce occurs\nsingle page view\nzero time on site\n\n\n# hits versus bounces\ndisplay(\n    (\n        (100 * df_train.query(\"hits == 1\")[\"bounces\"].value_counts(normalize=True))\n        .rename(\"frequency\")\n        .to_frame()\n        .reset_index()\n        .rename(columns={\"index\": \"bounces\"})\n    )\n    .assign(split=\"train\")\n    .assign(hits=1)\n    .iloc[[0]]\n)\n\n# hits versus number of pages viewed\ndisplay(\n    (\n        (100 * df_train.query(\"hits == 1\")[\"pageviews\"].value_counts(normalize=True))\n        .rename(\"frequency\")\n        .to_frame()\n        .reset_index()\n        .rename(columns={\"index\": \"pageviews\"})\n    )\n    .assign(split=\"train\")\n    .assign(hits=1)\n    .iloc[[0]]\n)\n\n# hits versus time on site\ndisplay(\n    (\n        (100 * df_train.query(\"hits == 1\")[\"time_on_site\"].value_counts(normalize=True))\n        .rename(\"frequency\")\n        .to_frame()\n        .reset_index()\n        .rename(columns={\"index\": \"time_on_site\"})\n    )\n    .assign(split=\"train\")\n    .assign(hits=1)\n    .iloc[[0]]\n)\n\n\n\n\n\n\n\n\nbounces\nfrequency\nsplit\nhits\n\n\n\n\n0\n1\n99.988813\ntrain\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npageviews\nfrequency\nsplit\nhits\n\n\n\n\n0\n1\n99.988813\ntrain\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime_on_site\nfrequency\nsplit\nhits\n\n\n\n\n0\n0\n100.0\ntrain\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nBased on the above, it might be worth training a ML model with only one of these General Features (hits, time_on_site, pageviews).\nbounces is a binary column and should be treated as a categorical ML feature and not as a numerical feature.\n\n\n\n\n\nCreate Validation Data\nUsing the same approach, the validation data split is now created by only changing\n\ntrain_start_date to val_start_date\ntrain_end_date to val_end_date\n\nin the first_visit_attributes CTE in order to capture the validation data and categorical features are encoded using the custom pipeline that was trained using the training data\n\n# load data from BigQuery dataset, set datatypes and drop duplicates\nquery = get_sql_query(val_start_date, val_end_date, train_start_date, test_end_date)\ndf_val = (\n    run_sql_query(query, **gcp_auth_dict, show_df=False)\n    .pipe(set_datatypes, dtypes=dtypes_dict)\n    .pipe(drop_duplicates, subset=[\"fullvisitorid\"])\n)\nprint(\n    f\"Got {len(df_val):,} rows  and {df_train.shape[1]:,} columns\"\n    \"after dropping duplicates\"\n)\n\n# perform frequency-encoding on categorical features\n# # Apply the custom data transformation pipeline to prepare the training data split\ndf_val = pd.DataFrame(\n    pipe_trans.transform(df_val), columns=categorical_columns + non_categorical_columns\n)[non_categorical_columns + categorical_columns].pipe(set_datatypes, dtypes=dtypes_dict)\nprint(\n    f\"Got {len(df_val):,} rows and {df_val.shape[1]:,} columns after \"\n    \"frequency-encoding categorical features\"\n)\n\nwith pd.option_context(\"display.max_columns\", None):\n    display(df_val.head())\n    display(df_val.tail())\n\nQuery execution start time = 2023-04-13 17:38:37.871...done at 2023-04-13 17:38:43.515 (5.644 seconds).\nQuery returned 21,208 rows\nGot 21,177 rows  and 29 columnsafter dropping duplicates\nGot 21,177 rows and 29 columns after frequency-encoding categorical features\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nhits\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntime_on_site\nadded_to_cart\nmade_purchase_on_future_visit\nbounces\nlast_action\nsource\nmedium\nchannelGrouping\nbrowser\nos\ndeviceCategory\n\n\n\n\n0\n7443659111332807488\n1484063672\n1\n2017-01-10 07:54:32\nUnited States\n1\n1\n10\n3\n7\n54\n32\n11\n9\n0\n66\n0\n11\n151\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nother\ndesktop\n\n\n1\n5597527133395896902\n1485228391\n1\n2017-01-23 19:26:31\nUnited States\n1\n1\n23\n2\n19\n26\n31\n4\n9\n0\n12\n0\n4\n68\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nSafari\niOS\ntablet\n\n\n2\n9166144922111078017\n1483730457\n1\n2017-01-06 11:20:57\nUnited States\n1\n1\n6\n6\n11\n20\n57\n6\n9\n0\n0\n0\n3\n27\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nMacintosh\ndesktop\n\n\n3\n4913593613905335447\n1485720519\n1\n2017-01-29 12:08:39\nUnited States\n1\n1\n29\n1\n12\n8\n39\n5\n9\n0\n0\n0\n5\n122\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nWindows\ndesktop\n\n\n4\n3123113931923419625\n1485363986\n1\n2017-01-25 09:06:26\nUnited States\n1\n1\n25\n4\n9\n6\n26\n20\n54\n0\n39\n0\n20\n494\n0\nFalse\n0\n5\ngoogle\norganic\nOrganic Search\nChrome\nWindows\ndesktop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nhits\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntime_on_site\nadded_to_cart\nmade_purchase_on_future_visit\nbounces\nlast_action\nsource\nmedium\nchannelGrouping\nbrowser\nos\ndeviceCategory\n\n\n\n\n21172\n7261133747877726873\n1485301932\n1\n2017-01-24 15:52:12\nUnited States\n1\n1\n24\n3\n15\n52\n12\n3\n0\n0\n32\n0\n3\n28\n0\nFalse\n0\n0\n(direct)\n(none)\nDirect\nChrome\nMacintosh\ndesktop\n\n\n21173\n1814053538655247996\n1483319025\n1\n2017-01-01 17:03:45\nUnited States\n1\n1\n1\n1\n17\n3\n45\n3\n0\n0\n28\n0\n3\n49\n0\nFalse\n0\n0\n(direct)\n(none)\nDirect\nChrome\nMacintosh\ndesktop\n\n\n21174\n6632475970380148823\n1485256352\n1\n2017-01-24 03:12:32\nUnited States\n1\n1\n24\n3\n3\n12\n32\n3\n0\n0\n36\n0\n1\n29\n0\nFalse\n0\n0\nyoutube.com\nreferral\nother\nSafari\nMacintosh\ndesktop\n\n\n21175\n5357307819206012105\n1483378607\n1\n2017-01-02 09:36:47\nUnited States\n1\n1\n2\n2\n9\n36\n47\n3\n0\n0\n18\n0\n3\n66\n0\nFalse\n0\n0\nother\nreferral\nother\nChrome\nMacintosh\ndesktop\n\n\n21176\n2767207647504105236\n1483984592\n1\n2017-01-09 09:56:32\nUnited States\n1\n1\n9\n2\n9\n56\n32\n3\n0\n0\n31\n0\n3\n132\n0\nFalse\n0\n0\nyoutube.com\nreferral\nother\nChrome\nWindows\ndesktop\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nEarlier, it was mentioned that totals.newVisits = 1 gives the first visit while totals.newVisits = NULL gives the future visit. In the training data, the first visit was picked up using this filter condition as a SQL filter. This allowed features from the first visit to be extracted. The label was whether the visitor made a purchase during their return visit. Here, the validation data uses the same approach. Per the business use-case, we want to predict new visitor’s propensity of making a purchase during their return (or future) visit.\nThe ML model will be trained using training data which only captures the first visit and this first visit occurs during the months covered by the training data only. The model will be validated using validation data that similarly covers the first visit that occurs during the months covered by the validation data only. The label of the validation data is analogous to that from the training data in that it indicates whether these new visitors (in the validation data split) made a purchase during a future visit.\nWith this in mind, similar to the training data, we can get that first visit of visitors in the validation data using totals.newVisits = 1 in the validation data SQL query above. For this reason, we will not use totals.newVisits = NULL in the SQL query to build the validation or test data splits.\nThe visitors in the training data do not need to be the same as those in the validation (or testing) data splits. Visitor ID will not be used as a feature during training, validation or evaluation (test data). Only the attributes of their first visit will be used since we are not interested in training a ML model for specific visitors who are identified by their ID.\n\n\n\nThe start and end visitStartTimes match those specified by the required start (val_start_date) and end (val_end_date) dates of the validation data split\n\n\nCode\nassert df_val[\"visitStartTime\"].min().month == int(val_start_date[4:6])\nassert df_val[\"visitStartTime\"].max().month == int(val_end_date[4:6])\nprint(\n    f\"Visit Start Times: {df_val['visitStartTime'].min().strftime('%Y-%m-%d %H:%M:%S')} - \"\n    f\"{df_val['visitStartTime'].max().strftime('%Y-%m-%d %H:%M:%S')}\"\n)\n\n\nVisit Start Times: 2017-01-01 00:04:32 - 2017-01-31 23:57:51\n\n\n\n\nCreate Test Data\nFinally, the test data split is created using a similar approach to the validation data split (only the dates in the first_visit_attributes CTE are changed in order to capture the test data)\n\n# load data from BigQuery dataset, set datatypes and drop duplicates\nquery = get_sql_query(test_start_date, test_end_date, train_start_date, test_end_date)\ndf_test = (\n    run_sql_query(query, **gcp_auth_dict, show_df=False)\n    .pipe(set_datatypes, dtypes=dtypes_dict)\n    .pipe(drop_duplicates, subset=[\"fullvisitorid\"])\n)\nprint(\n    f\"Got {len(df_test):,} rows  and {df_test.shape[1]:,} columns\"\n    \"after dropping duplicates\"\n)\n\n# perform frequency-encoding on categorical features\n# # Apply the custom data transformation pipeline to prepare the training data split\ndf_test = pd.DataFrame(\n    pipe_trans.transform(df_test), columns=categorical_columns + non_categorical_columns\n)[non_categorical_columns + categorical_columns].pipe(set_datatypes, dtypes=dtypes_dict)\nprint(\n    f\"Got {len(df_test):,} rows and {df_test.shape[1]:,} columns after \"\n    \"frequency-encoding categorical features\"\n)\n\nwith pd.option_context(\"display.max_columns\", None):\n    display(df_test.head())\n    display(df_test.tail())\n\nQuery execution start time = 2023-04-13 17:38:43.708...done at 2023-04-13 17:38:48.259 (4.552 seconds).\nQuery returned 20,180 rows\nGot 20,164 rows  and 29 columnsafter dropping duplicates\nGot 20,164 rows and 29 columns after frequency-encoding categorical features\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nhits\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntime_on_site\nadded_to_cart\nmade_purchase_on_future_visit\nbounces\nlast_action\nsource\nmedium\nchannelGrouping\nbrowser\nos\ndeviceCategory\n\n\n\n\n0\n6667770433164438858\n1487630668\n1\n2017-02-20 14:44:28\nUnited States\n1\n2\n20\n2\n14\n44\n28\n7\n9\n0\n39\n0\n7\n75\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nWindows\ndesktop\n\n\n1\n861894924191702840\n1486761793\n1\n2017-02-10 13:23:13\nUnited States\n1\n2\n10\n6\n13\n23\n13\n7\n18\n0\n0\n0\n7\n38\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nWindows\ndesktop\n\n\n2\n987663115799971454\n1487812839\n1\n2017-02-22 17:20:39\nUnited States\n1\n2\n22\n4\n17\n20\n39\n4\n9\n0\n0\n0\n4\n21\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nMacintosh\ndesktop\n\n\n3\n7090908358039029290\n1487412580\n1\n2017-02-18 02:09:40\nUnited States\n1\n2\n18\n7\n2\n9\n40\n6\n9\n0\n12\n0\n6\n540\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nother\nWindows\ndesktop\n\n\n4\n294074179132707998\n1487899413\n1\n2017-02-23 17:23:33\nUnited States\n1\n2\n23\n5\n17\n23\n33\n5\n18\n0\n3\n0\n5\n117\n0\nFalse\n0\n0\ngoogle\nother\nother\nChrome\nMacintosh\ndesktop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nhits\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntime_on_site\nadded_to_cart\nmade_purchase_on_future_visit\nbounces\nlast_action\nsource\nmedium\nchannelGrouping\nbrowser\nos\ndeviceCategory\n\n\n\n\n20159\n171945862803080966\n1487825038\n1\n2017-02-22 20:43:58\nUnited States\n1\n2\n22\n4\n20\n43\n58\n3\n0\n0\n39\n0\n3\n26\n0\nFalse\n0\n0\n(direct)\n(none)\nDirect\nSafari\niOS\nmobile\n\n\n20160\n6600545612381993294\n1487380495\n1\n2017-02-17 17:14:55\nUnited States\n1\n2\n17\n6\n17\n14\n55\n3\n0\n0\n15\n0\n2\n22\n1\nFalse\n0\n3\nother\nreferral\nother\nChrome\nMacintosh\ndesktop\n\n\n20161\n258929636492512392\n1487293726\n1\n2017-02-16 17:08:46\nUnited States\n1\n2\n16\n5\n17\n8\n46\n3\n0\n0\n5\n0\n3\n1640\n0\nFalse\n0\n0\nother\nreferral\nother\nother\nAndroid\nmobile\n\n\n20162\n7064613315175563493\n1487258784\n1\n2017-02-16 07:26:24\nUnited States\n1\n2\n16\n5\n7\n26\n24\n3\n0\n0\n36\n0\n3\n53\n0\nFalse\n0\n0\nyoutube.com\nreferral\nother\nother\nWindows\ndesktop\n\n\n20163\n3684942462385225804\n1487279963\n1\n2017-02-16 13:19:23\nUnited States\n1\n2\n16\n5\n13\n19\n23\n3\n0\n0\n29\n0\n3\n117\n0\nFalse\n0\n0\nyoutube.com\nreferral\nother\nother\nWindows\ndesktop\n\n\n\n\n\n\n\nThe start and end visitStartTimes match those specified by the required start (test_start_date) and end (test_end_date) dates of the test data split\n\n\nCode\nassert df_test[\"visitStartTime\"].min().month == int(test_start_date[4:6])\nassert df_test[\"visitStartTime\"].max().month == int(test_end_date[4:6])\nprint(\n    f\"Visit Start Times: {df_test['visitStartTime'].min().strftime('%Y-%m-%d %H:%M:%S')} - \"\n    f\"{df_test['visitStartTime'].max().strftime('%Y-%m-%d %H:%M:%S')}\"\n)\n\n\nVisit Start Times: 2017-02-01 00:00:06 - 2017-02-28 23:54:37"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_transform.html#discussion-of-duplicates-during-data-preparation",
    "href": "notebooks/04-transform/notebooks/04_transform.html#discussion-of-duplicates-during-data-preparation",
    "title": "Data Transformation",
    "section": "Discussion of Duplicates During Data Preparation",
    "text": "Discussion of Duplicates During Data Preparation\n\nBy splitting data for ML, at the time of training the ML model, we are assuming that we don’t yet have the validation or testing data during ML model development so we would not know if duplicates are or are not present in those data splits.\nIf such a model is performant enough to be deployed to production, the same will apply to out-of-sample (unseen) visitors’ visit data. The features that would be needed in production are the same as those that would be needed during ML development. So, when we do access the out-of-sample data in production, we again would want only the first visit per visitor so that (as was done during ML development) we can predict their propensity to make a purchase during a future visit. In order to accomplish this, when we were to get access to the unseen data in production, we could easily\n\nconsider the first valid visit per visitor (i.e. per fullvisitorid)\ncreate features from this visit and make a prediction (inference) of propensity to purchase during a return (or future) visit\nfor subsequent visits\n\ncheck for duplicates by fullvisitorid\ndrop any duplicated (subsequent) visits by the same fullvisitorid (since an inference prediction has already been made for this visitor)\n\n\nand this workflow does not involve data leakage or lookahead bias.\nFor this reason, we can drop this type of duplicate in the validation and test data splits without being affected by data leakage or lookahead bias."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_transform.html#export-to-disk",
    "href": "notebooks/04-transform/notebooks/04_transform.html#export-to-disk",
    "title": "Data Transformation",
    "section": "Export to Disk",
    "text": "Export to Disk\nShow datatypes for all data splits\n\n\nCode\ndfs = []\nfor df, split in zip([df_train, df_val, df_test], [\"train\", \"val\", \"test\"]):\n    dfs.append(\n        df.dtypes.rename(f\"datatype_{split}\")\n        .reset_index()\n        .rename(columns={\"index\": \"column\"})\n    )\ndf_dtypes = reduce(\n    lambda left, right: pd.merge(left, right, on=[\"column\"], how=\"outer\"), dfs\n)\ndf_dtypes\n\n\n\n\n\n\n\n\n\ncolumn\ndatatype_train\ndatatype_val\ndatatype_test\n\n\n\n\n0\nfullvisitorid\nstring[python]\nstring[python]\nstring[python]\n\n\n1\nvisitId\nstring[python]\nstring[python]\nstring[python]\n\n\n2\nvisitNumber\nInt8\nInt8\nInt8\n\n\n3\nvisitStartTime\ndatetime64[ns]\ndatetime64[ns]\ndatetime64[ns]\n\n\n4\ncountry\nstring[python]\nstring[python]\nstring[python]\n\n\n5\nquarter\nInt8\nInt8\nInt8\n\n\n6\nmonth\nInt8\nInt8\nInt8\n\n\n7\nday_of_month\nInt8\nInt8\nInt8\n\n\n8\nday_of_week\nInt8\nInt8\nInt8\n\n\n9\nhour\nInt8\nInt8\nInt8\n\n\n10\nminute\nInt8\nInt8\nInt8\n\n\n11\nsecond\nInt8\nInt8\nInt8\n\n\n12\nhits\nInt16\nInt16\nInt16\n\n\n13\npromos_displayed\nInt16\nInt16\nInt16\n\n\n14\npromos_clicked\nInt16\nInt16\nInt16\n\n\n15\nproduct_views\nInt16\nInt16\nInt16\n\n\n16\nproduct_clicks\nInt16\nInt16\nInt16\n\n\n17\npageviews\nInt16\nInt16\nInt16\n\n\n18\ntime_on_site\nInt16\nInt16\nInt16\n\n\n19\nadded_to_cart\nInt16\nInt16\nInt16\n\n\n20\nmade_purchase_on_future_visit\nboolean\nboolean\nboolean\n\n\n21\nbounces\ncategory\ncategory\ncategory\n\n\n22\nlast_action\ncategory\ncategory\ncategory\n\n\n23\nsource\ncategory\ncategory\ncategory\n\n\n24\nmedium\ncategory\ncategory\ncategory\n\n\n25\nchannelGrouping\ncategory\ncategory\ncategory\n\n\n26\nbrowser\ncategory\ncategory\ncategory\n\n\n27\nos\ncategory\ncategory\ncategory\n\n\n28\ndeviceCategory\ncategory\ncategory\ncategory\n\n\n\n\n\n\n\nShow info for DataFrame with training data\n\n\nCode\ndf_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 92551 entries, 0 to 92550\nData columns (total 29 columns):\n #   Column                         Non-Null Count  Dtype         \n---  ------                         --------------  -----         \n 0   fullvisitorid                  92551 non-null  string        \n 1   visitId                        92551 non-null  string        \n 2   visitNumber                    92551 non-null  Int8          \n 3   visitStartTime                 92551 non-null  datetime64[ns]\n 4   country                        92551 non-null  string        \n 5   quarter                        92551 non-null  Int8          \n 6   month                          92551 non-null  Int8          \n 7   day_of_month                   92551 non-null  Int8          \n 8   day_of_week                    92551 non-null  Int8          \n 9   hour                           92551 non-null  Int8          \n 10  minute                         92551 non-null  Int8          \n 11  second                         92551 non-null  Int8          \n 12  hits                           92551 non-null  Int16         \n 13  promos_displayed               92551 non-null  Int16         \n 14  promos_clicked                 92551 non-null  Int16         \n 15  product_views                  92551 non-null  Int16         \n 16  product_clicks                 92551 non-null  Int16         \n 17  pageviews                      92551 non-null  Int16         \n 18  time_on_site                   92551 non-null  Int16         \n 19  added_to_cart                  92551 non-null  Int16         \n 20  made_purchase_on_future_visit  92551 non-null  boolean       \n 21  bounces                        92551 non-null  category      \n 22  last_action                    92551 non-null  category      \n 23  source                         92551 non-null  category      \n 24  medium                         92551 non-null  category      \n 25  channelGrouping                92551 non-null  category      \n 26  browser                        92551 non-null  category      \n 27  os                             92551 non-null  category      \n 28  deviceCategory                 92551 non-null  category      \ndtypes: Int16(8), Int8(8), boolean(1), category(8), datetime64[ns](1), string(3)\nmemory usage: 7.2 MB\n\n\nShow info for DataFrame with validation data\n\n\nCode\ndf_val.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 21177 entries, 0 to 21176\nData columns (total 29 columns):\n #   Column                         Non-Null Count  Dtype         \n---  ------                         --------------  -----         \n 0   fullvisitorid                  21177 non-null  string        \n 1   visitId                        21177 non-null  string        \n 2   visitNumber                    21177 non-null  Int8          \n 3   visitStartTime                 21177 non-null  datetime64[ns]\n 4   country                        21177 non-null  string        \n 5   quarter                        21177 non-null  Int8          \n 6   month                          21177 non-null  Int8          \n 7   day_of_month                   21177 non-null  Int8          \n 8   day_of_week                    21177 non-null  Int8          \n 9   hour                           21177 non-null  Int8          \n 10  minute                         21177 non-null  Int8          \n 11  second                         21177 non-null  Int8          \n 12  hits                           21177 non-null  Int16         \n 13  promos_displayed               21177 non-null  Int16         \n 14  promos_clicked                 21177 non-null  Int16         \n 15  product_views                  21177 non-null  Int16         \n 16  product_clicks                 21177 non-null  Int16         \n 17  pageviews                      21177 non-null  Int16         \n 18  time_on_site                   21177 non-null  Int16         \n 19  added_to_cart                  21177 non-null  Int16         \n 20  made_purchase_on_future_visit  21177 non-null  boolean       \n 21  bounces                        21177 non-null  category      \n 22  last_action                    21177 non-null  category      \n 23  source                         21177 non-null  category      \n 24  medium                         21177 non-null  category      \n 25  channelGrouping                21177 non-null  category      \n 26  browser                        21177 non-null  category      \n 27  os                             21177 non-null  category      \n 28  deviceCategory                 21177 non-null  category      \ndtypes: Int16(8), Int8(8), boolean(1), category(8), datetime64[ns](1), string(3)\nmemory usage: 1.7 MB\n\n\nShow info for DataFrame with test data\n\n\nCode\ndf_test.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20164 entries, 0 to 20163\nData columns (total 29 columns):\n #   Column                         Non-Null Count  Dtype         \n---  ------                         --------------  -----         \n 0   fullvisitorid                  20164 non-null  string        \n 1   visitId                        20164 non-null  string        \n 2   visitNumber                    20164 non-null  Int8          \n 3   visitStartTime                 20164 non-null  datetime64[ns]\n 4   country                        20164 non-null  string        \n 5   quarter                        20164 non-null  Int8          \n 6   month                          20164 non-null  Int8          \n 7   day_of_month                   20164 non-null  Int8          \n 8   day_of_week                    20164 non-null  Int8          \n 9   hour                           20164 non-null  Int8          \n 10  minute                         20164 non-null  Int8          \n 11  second                         20164 non-null  Int8          \n 12  hits                           20164 non-null  Int16         \n 13  promos_displayed               20164 non-null  Int16         \n 14  promos_clicked                 20164 non-null  Int16         \n 15  product_views                  20164 non-null  Int16         \n 16  product_clicks                 20164 non-null  Int16         \n 17  pageviews                      20164 non-null  Int16         \n 18  time_on_site                   20164 non-null  Int16         \n 19  added_to_cart                  20164 non-null  Int16         \n 20  made_purchase_on_future_visit  20164 non-null  boolean       \n 21  bounces                        20164 non-null  category      \n 22  last_action                    20164 non-null  category      \n 23  source                         20164 non-null  category      \n 24  medium                         20164 non-null  category      \n 25  channelGrouping                20164 non-null  category      \n 26  browser                        20164 non-null  category      \n 27  os                             20164 non-null  category      \n 28  deviceCategory                 20164 non-null  category      \ndtypes: Int16(8), Int8(8), boolean(1), category(8), datetime64[ns](1), string(3)\nmemory usage: 1.6 MB\n\n\nThe training data is now exported to disk\n\nfpath_train = os.path.join(processed_data_dir, \"train_processed.parquet.gzip\")\ndf_train.to_parquet(fpath_train, index=False, compression='gzip', engine='pyarrow')\n\nThe validation data is now exported to disk\n\nfpath_val = os.path.join(processed_data_dir, \"val_processed.parquet.gzip\")\ndf_val.to_parquet(fpath_val, index=False, compression='gzip', engine='pyarrow')\n\nThe testing data is now exported to disk\n\nfpath_test = os.path.join(processed_data_dir, \"test_processed.parquet.gzip\")\ndf_test.to_parquet(fpath_test, index=False, compression='gzip', engine='pyarrow')"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_transform.html#etl-workflow-to-transform-data",
    "href": "notebooks/04-transform/notebooks/04_transform.html#etl-workflow-to-transform-data",
    "title": "Data Transformation",
    "section": "ETL Workflow to Transform Data",
    "text": "ETL Workflow to Transform Data\nWe will now define an ETL workflow to perform the end-to-end transformation using Python functions that use scikit-learn.Pipeline and pandas.pipe. Such a workflow can be quickly re-used during ML development without needing to re-run all the Python code in this data transformation step and will allow the ability to change\n\nstart and end dates of raw data\n\nthis could be useful to add more training data if the ML model is not capturing variability in unseen data\n\nthresholds used when frequency-encoding categorical features\n\nthis could be useful to increase or decrease the cardinality of categorical features\n\nincreasing cardinality might help improve the predictive power of such features\ndecreasing cardinality might help improve (reduce) ML model training duration and explainability\n\n\n\nThe ETL workflow will accept the following parameters\n\nstart and end date for each data split\ndictionary with frequency encoding thresholds and categorical columns to which these thresholds must be applied\n\ncurrently these thresholds are set to 5% and 10% based on the data preparation step\n\ncolumns to use when dropping duplicates\n\ncurrently a single column fullvisitorid is used to drop duplicates, based on the data preparation step\n\nfeature datatypes\n\nthese are neeed before and after encoding categorical features\n\npath to folder in which to save transformed data\n\nthis is a path to a folder on the local disk where this analysis is being run\n\n\nThe following components are defined below\n\nhelper functions to\n\ncreate scikit-learn data transformation pipeline\nclean data\n\nworkflow functions to\n\nextract raw data from Google BigQuery\ntransform raw data using scikit-learn data transformation pipeline\nload transformed data into file in data/processed\n\n\nHelper functions\n\ndef make_data_transformation_pipeline(cols_to_group: Dict[str, List[str]]) -&gt; Pipeline:\n    \"\"\"Create sklearn pipeline to transform cleaned data.\"\"\"\n    # define frequency-encoders for categorical columns that will require\n    # a minimum frequency for categories\n    categorical_transformer = Pipeline(\n        steps=[\n            (\n                f\"enc_{(k.split('_')[0]).zfill(2)}\",\n                RareLabelEncoder(\n                    tol=int(k.split(\"_\")[0]) / 100,\n                    n_categories=2,\n                    variables=v,\n                    replace_with=\"other\",\n                ),\n            )\n            for k, v in cols_to_group.items()\n        ]\n    )\n    preprocessor = ColumnTransformer(\n        transformers=[(\"cat\", categorical_transformer, categorical_columns)],\n        remainder=\"passthrough\",\n    )\n\n    # create overall data transformation pipeline\n    pipe = Pipeline(steps=[(\"preprocessor\", preprocessor)])\n    return pipe\n\n\ndef clean_data(\n    df: pd.DataFrame, datatypes_dict: Dict, subset: List[str]\n) -&gt; pd.DataFrame:\n    \"\"\"Perform data cleaning.\"\"\"\n    df = (\n        # set column datatypes\n        df.pipe(set_datatypes, dtypes=datatypes_dict)\n        # drop duplicates\n        .pipe(drop_duplicates, subset=subset)\n    )\n    print(\n        f\"Got {len(df):,} rows and {df.shape[1]:,} columns \" \"after dropping duplicates\"\n    )\n    return df\n\nWorkflow functions\n\ndef extract_data(\n    split_start_date: str,\n    split_end_date: str,\n    train_start_date: str,\n    test_end_date: str,\n) -&gt; pd.DataFrame:\n    \"\"\"Retrieve data from Google BigQuery dataset.\"\"\"\n    query = get_sql_query(\n        split_start_date, split_end_date, train_start_date, test_end_date\n    )\n    df = run_sql_query(query, **gcp_auth_dict, show_df=False)\n    return df\n\n\ndef transform_data(\n    df: pd.DataFrame,\n    categorical_columns: List[str],\n    datatypes_dict: Dict,\n    pipe: Union[Pipeline, None] = None,\n) -&gt; List[Union[pd.DataFrame, Pipeline]]:\n    \"\"\"Transform features in data.\"\"\"\n    # clean data\n    df = df.pipe(clean_data, datatypes_dict=datatypes_dict, subset=[\"fullvisitorid\"])\n\n    # Get a list of the non-categorical columns\n    non_categorical_columns = [c for c in list(df) if c not in categorical_columns]\n\n    # train a data transformation pipeline to perform frequency-encoding on\n    # categorical features in data\n    # - this is needed for training data split only\n    if pipe:\n        _ = pipe_trans.fit(df)\n\n    # Apply a trained data transformation pipeline to perform frequency-encoding on\n    # categorical features in data\n    # - this is needed for training, validation and test data splits\n    cols_transformed = categorical_columns + non_categorical_columns\n    df = pd.DataFrame(pipe_trans.transform(df), columns=cols_transformed)[\n        non_categorical_columns + categorical_columns\n    ]\n\n    # set datatypes after data transformation\n    df = df.pipe(set_datatypes, dtypes=datatypes_dict)\n\n    print(\n        f\"Got {len(df):,} rows and {df.shape[1]:,} columns after \"\n        \"frequency-encoding categorical features\"\n    )\n    return [df, pipe]\n\n\ndef load_data(\n    df: pd.DataFrame, processed_data_dir: str, split_type: str = \"train\"\n) -&gt; None:\n    \"\"\"Save data to file on local disk.\"\"\"\n    fpath = os.path.join(processed_data_dir, f\"{split_type}_processed.parquet.gzip\")\n    df.to_parquet(fpath, index=False, compression=\"gzip\", engine=\"pyarrow\")\n    print(f\"Exported data to {fpath}\")\n\n\n\n\n\n\n\nNotes\n\n\n\nThese workflow functions also depend on the following helper functions (defined earlier in this step)\n\nset_datatypes()\ndrop_duplicates()\nget_sql_query()\nrun_sql_query()\n\nIf such a workflow is to be used in future steps in the analysis (eg. ML development), then both the\n\nworkflow functions\nhelper functions\n\nmust be defined in that step.\n\n\nWe’ll now demonstrate how this ETL workflow to retrieve and transform data gives the identical output to transformed data using the non-ETL (manual) workflow described earlier in this step.\nFirst, define data transformation pipeline\n\npipe = make_data_transformation_pipeline(cols_to_group)\n\nNext, run ETL workflow to create training data and train the data transformation pipeline\n\ndf_train_v2, pipe_trained = extract_data(\n    train_start_date,\n    train_end_date,\n    train_start_date,\n    test_end_date,\n).pipe(\n    transform_data,\n    categorical_columns=categorical_columns,\n    datatypes_dict=dtypes_dict,\n    pipe=pipe,\n)\ndf_train_v2.pipe(load_data, processed_data_dir, \"train\")\n\nQuery execution start time = 2023-04-13 17:38:48.599...done at 2023-04-13 17:39:06.459 (17.859 seconds).\nQuery returned 92,859 rows\nGot 92,551 rows and 29 columns after dropping duplicates\nGot 92,551 rows and 29 columns after frequency-encoding categorical features\nExported data to ../data/processed/train_processed.parquet.gzip\n\n\n\n\n\n\n\n\nNotes\n\n\n\nHere, we also collect the trained data transformation pipeline. This can be directly used to transform validation and test data splits without re-training.\n\n\nNext, run ETL workflow to create validation data, with the transformation pipeline that was trained using the training data\n\ndf_val_v2, _ = extract_data(\n    val_start_date,\n    val_end_date,\n    train_start_date,\n    test_end_date,\n).pipe(\n    transform_data,\n    categorical_columns=categorical_columns,\n    datatypes_dict=dtypes_dict,\n    pipe=pipe_trained,\n)\ndf_val_v2.pipe(load_data, processed_data_dir, \"val\")\n\nQuery execution start time = 2023-04-13 17:39:07.551...done at 2023-04-13 17:39:12.790 (5.240 seconds).\nQuery returned 21,208 rows\nGot 21,177 rows and 29 columns after dropping duplicates\nGot 21,177 rows and 29 columns after frequency-encoding categorical features\nExported data to ../data/processed/val_processed.parquet.gzip\n\n\n\n\n\n\n\n\nNotes\n\n\n\nHere, we do not need to collect the data transformation pipeline since it was already trained using the training data.\n\n\nFinally, run the ETL workflow to create test data, with the transformation pipeline that was trained using the training data\n\ndf_test_v2, _ = extract_data(\n    test_start_date,\n    test_end_date,\n    train_start_date,\n    test_end_date,\n).pipe(\n    transform_data,\n    categorical_columns=categorical_columns,\n    datatypes_dict=dtypes_dict,\n    pipe=pipe_trained,\n)\ndf_test_v2.pipe(load_data, processed_data_dir, \"test\")\n\nQuery execution start time = 2023-04-13 17:39:13.145...done at 2023-04-13 17:39:18.113 (4.968 seconds).\nQuery returned 20,180 rows\nGot 20,164 rows and 29 columns after dropping duplicates\nGot 20,164 rows and 29 columns after frequency-encoding categorical features\nExported data to ../data/processed/test_processed.parquet.gzip\n\n\n\n\n\n\n\n\nNotes\n\n\n\nAgain, we do not need to collect the data transformation pipeline since it was already trained using the training data.\n\n\nVerify that the manual and ETL approaches give the identical output for training, validation and test data splits after data transformation\n\nassert df_train.equals(df_train_v2)\nassert df_val.equals(df_val_v2)\nassert df_test.equals(df_test_v2)"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_transform.html#summary-of-tasks-performed",
    "href": "notebooks/04-transform/notebooks/04_transform.html#summary-of-tasks-performed",
    "title": "Data Transformation",
    "section": "Summary of Tasks Performed",
    "text": "Summary of Tasks Performed\nThis step performed the following\n\nOverall\n\ntraining, validation and test data splits were created that can be used to address the objective of training a ML model to predict a new visitor’s propensity to make a purchase from the merchandise store on the Google Marketplace during February 2017\n\nData Transformation\n\nFeatures in the prepared data splits were created at the visit level. Since the objective is to predict propensity to make a purchase during a future visit, features should also be at the level of visits. Based on Google Analytics’ definition, visits were defined by the combination of the\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\n\ncolumns.\n\nData Splits\n\nthe data was split by month of the year\nnew visitors in the training data, who returned to the Google Merchandise Store during the period of months covering the training data, do not need to also be in the validation or test data splits\n\nthis is not a problem since the current project’s business use-case is targeting new visitors and not the same/existing visitors\n\n\nFeature Selection\n\nThe data splits were created using a subset of columns provided for visitor transactions on the store’s website. These columns were selected based on\n\nexploratory data analysis preformed in the preceding step\nintuition about factors that would be predictive of a new visitor’s propensity (probability) of making a purchase on a future visit to the store\n\nIt might be best to start by training a ML model with one of the General Features (time_on_site, hits, pageviews) and only add more if necessary to improve performance\n\nFeature Processing\n\napproaches to process features were adopted based on frequencies observed in the training data during the EDA step\n\ncategorical features were bucketed\n\nbased on analysis in the current step\n\nbounces was shown i a binary column and should be treated as a categorical ML feature\ncandidates for handling class imbalance during ML training are\n\nundersampling\nno changes\nSMOTE\n\n\n\nDefined ETL workflow\n\nthis supports quickly changing parameters of the data transformation pipeline in each data split during future steps"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_transform.html#summary-of-assumptions",
    "href": "notebooks/04-transform/notebooks/04_transform.html#summary-of-assumptions",
    "title": "Data Transformation",
    "section": "Summary of Assumptions",
    "text": "Summary of Assumptions\nNone."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_transform.html#limitations",
    "href": "notebooks/04-transform/notebooks/04_transform.html#limitations",
    "title": "Data Transformation",
    "section": "Limitations",
    "text": "Limitations\nNone."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_transform.html#next-step",
    "href": "notebooks/04-transform/notebooks/04_transform.html#next-step",
    "title": "Data Transformation",
    "section": "Next Step",
    "text": "Next Step\nThe next step will develop a baseline model using the transformed data splits."
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html",
    "href": "notebooks/05-development/notebooks/05_development.html",
    "title": "Baseline Model Development",
    "section": "",
    "text": "Import Python modules\nCode\nimport json\nimport os\nfrom datetime import datetime\nfrom typing import Dict, Union\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn.metrics as skm\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import GridSearchCV, PredefinedSplit\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.utils import check_array, check_random_state\nfrom sklearn.utils.estimator_checks import check_estimator\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted"
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html#about",
    "href": "notebooks/05-development/notebooks/05_development.html#about",
    "title": "Baseline Model Development",
    "section": "About",
    "text": "About\n\nObjective\nThis step trains a baseline model for predicting visitors’ propensity to make a purchase on a future visit to the merchandise store on the Google Marketplace, using binary classification.\nIn ML terms, propensity is the likelihood to buy on a return visit and this comes from the prediction probability, which is the probability that a predicted outcome belongs to one of the two known binary classes (in the current case, the known classes are will or will not purchase on future visit).\n\n\nData\nCurrently, three data splits with transformed data were created by the previous step\n\ntrain\nvalidation\ntest\n\nPredictive models will be developed using the training data split and validated against the validation data split. This will be used to optimize hyperparameters.\nBased on validation scores, the best\n\nmodel\nfeatures\n\nwill then be\n\ntrained using the combined training and validation split\nevaluated using the test split\n\nThis workflow is used for\n\nBaseline model development (this step)\n\nfeatures are not used\n\nMachine Learning (next step)\n\nfeatures will be used\n\n\n\n\nBaseline Model Requirements\nIn this project, ML is used to predict visitor propensity (probability) so the chosen baseline distribution must produce a right-skewed distribution (histogram) of probabilities. This would indicate that most visitors are predicted to have close to a zero percent probability of making a purchase on a return visit, which is what we saw during EDA in a previous step.\nWith this in mind, we have the following two requirements of a suitable baseline model for predicting visitors’ propensity to make a purchase on a future visit 1. If we use a binary classification discrimination threshold of 0.5, then a right-skewed histogram of probabilities will result in most visitors having a probablity of less than the threshold (0.5) meaning they would be classified as not likely to make a purchase on a return visit to the merchandise store. This would lead to a predicted class imbalance, which is what we want since we observed this class imbalance in the training data. However, just having a class imbalance is not enough. We need a strong class imbalance since approximately 5% of visitors were observed to make a purchase on a return visit. So, we need to modify the the skew (shape) of the baseline distribution that is chosen to match this expected class imbalance seen in the training data. 2. Since we require probabilities, we need to choose a distribution that can be produce floating point values in the range of 0.0 to 1.0.\n\n\nBaseline Model Choices\nA normal distribution histogram would have a peak in the center, which would indicate that most visitors have between a 25% and 75% chance of making a return purchase, with the peak at 50%. This would not give a strongly right-skewed histogram. In turn, this would not give a strong class imbalance. So, a normal distribution is not a suitable distribution for baseline predictions of visitors’ propensity to make a return purchase. By comparison, a beta distribution’s hyperparameters (a and b) can be chosen to skew the histogram so that most customers have a low probability (propensity) to make a return purchase. This is more realistic for the current use-case and we would want to choose its hyperparameters to give a class balance of close to 95%-5%. For these reasons, an appropriately shaped beta (skewed) distribution is a suitable baseline model for the current use-case.\nIn order to do this, we will generate randomly sampled predicted probablities (propensity scores) from the beta distribution (1, 2) that is strongly right skewed (positive skew). In Python, this can be done using\n\nscipy, under the scipy.stats module (see the beta distribution documentation for the scipy package)\nnumpy, through numpy.random.beta() (link)\n\nsklearn offers a DummyClassifier (link) that is able to randomly make predictions naively. One naive strategy to make predictions is called stratified, which generates predictions that are close to the class imbalance observed in the labels (y). This is another suitable choice here due to the imbalance obseved during data preparation. Here, the predicted probabilities are either 0s or 1s since this model makes predictions naively and it is only intended for use as a baseline.\n\n\nOverall Approach\nDuring baseline model developmen (or validation), the a and b hyperparameters of the beta distribution (baselline model) will be optimized using the validation data. Since this is a baseline model, model training is not required so ML features are not used. Instead, the predicted probabilities (propensities) will be randomly sampled from the beta distribution. Hyperparameter optimization is required so that the shape of the distribution is chosen to match the class imbalance seen in the validation data. During baseline model evaluation, the beta distribution (baseline model) with the best hyperparameters will be used to make predictions of visitors’ probabilities (propensities) of a future purchase in the test data.\nFor the built-in sklearn DummyClassifier, the optimal choice of hyperparameter is stratified, so no hyperparameter tuning is performed for this baseline model.\n\n\nEvaluation Metrics\nDue to the class imbalance, evaluation metrics can be difficult to interpret. The precision will generally be very low since there are so few minority class observations in the training data (we assume this is similar to the distribution in the validation and test splits).\nBased on this constraint about class imbalance, we will choose the following metrics\n\nF0.5-score (places more weight on precision than recall)\n\nin the project scope, this was identified to be the primary ML metric\n\nArea under the precision recall curve\n\nthis is ideal for imbalanced data, where the priority is on maximizing the precision and recall of the minority class\n\nuplift\n\nsort visitors in validation or test data split by predicted probability score\n\nthe predicted probability score comes from the baseline model (randomly sampled values between 0 and 1 from the beta distribution) or ML model\n\ndivide visitors into 20 percentiles (or ventiles) based on the sorted probability score\ncalculate uplift as the ratio of the average minority class probability in the top 5% (best performing ventile) to the average minority class probability across all ventiles\nthe interpretation of uplift is the factor by which the baseline or ML model is more effective than a random model\n\na random model would draw from a normal distribution, which does not have a histogram of prediction probabilities that is correctly shaped to reflect visitor buying patterns (class balance) that we saw during EDA\ndue to this, all baseline models explored in this step will be better than a random model\n\nthe model with the best performance will have the highest uplift\n\n\n\n\nAssumptions\n\nThe class distribution (or class balance) seen in the training data (explored during EDA) is similar to the distribution in the validation and test data splits.\n\n\n\nLimitations\n\nThe definition of uplift does not make a comparison of the predicted and true class labels. Instead, it only defines ventiles based on the predicted probability. This is acceptable since uplift is meant to compare a model’s predictive power to a random model. However, this also means it is possible to have a non-zero uplift with an inaccurate model. For this reason, the other metric (area under the precision-recall curve and F0.5-score) must also be used to evaluate model performance, in addition to uplift.\n\n\n\nNotation\n\nHard predictions refer to class labels (0, 1 or True, False). Soft predictions refer to prectited probabilities (floating point values between 0 and 1).\nTo convert soft predictions to hard labels, a fixed discrimination threshold of 0.5 will be used."
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html#user-inputs",
    "href": "notebooks/05-development/notebooks/05_development.html#user-inputs",
    "title": "Baseline Model Development",
    "section": "User Inputs",
    "text": "User Inputs\nGet relative path to project root directory\n\nPROJ_ROOT_DIR = os.path.join(os.pardir)\n\nDefine the different types of features in the transformed data\n\ncategoricals\nnumericals\ncategorical features not used in this step\nmetadata features (visit ID, visit number, etc.) for each visit\ndatetime features not used in this step\n\n\ncategorical_features = [\n    \"deviceCategory\",\n    \"bounces\",\n    \"channelGrouping\",\n    \"medium\",\n    \"source\",\n]\nnumerical_features = [\n    \"hits\",\n    \"promos_displayed\",\n    \"promos_clicked\",\n    \"product_views\",\n    \"product_clicks\",\n    \"pageviews\",\n    \"time_on_site\",\n]\n\ncategorical_features_unused = [\"last_action\"]\nmetadata_features_unused = [\n    \"fullvisitorid\",\n    \"visitId\",\n    \"visitNumber\",\n    \"visitStartTime\",\n]\ndatetime_features_unused = [\n    \"quarter\",\n    \"month\",\n    \"day_of_month\",\n    \"day_of_week\",\n    \"hour\",\n    \"minute\",\n    \"second\",\n]\n\nGet path to data/processed in which the transformed data splits (training, validation and test) produced by the (preceding) data transformation step were exported\n\ndata_dir = os.path.join(PROJ_ROOT_DIR, \"data\")\nprocessed_data_dir = os.path.join(PROJ_ROOT_DIR, \"data\", \"processed\")\nmodels_dir = os.path.join(PROJ_ROOT_DIR, \"models\")\n\nDefine a Python helper function to change probabilities into labels, using a user-specified discrimination threshold\n\ndef convert_soft_to_hard_labels(\n    y_pred_proba: pd.Series, disc_threshold: float = 0.5\n) -&gt; pd.Series:\n    \"\"\"Convert probabilities to labels.\"\"\"\n    y_pred = (y_pred_proba &gt; disc_threshold).astype(int)\n    return y_pred\n\nBelow is a helper function to calculate the uplift based on ventiles. The approach used is as follows\n\ncalculate the average predicted probablity (visitor propensity to purchase on future visit) per ventiles\nextract the average predicted probablity of the top ventile\nget the average predicted probability across all ventiles\ntake the ratio of 2. and 3. above to get the uplift\n\n\ndef calculate_uplift_score_using_ventiles(\n    df: pd.DataFrame, num_quantiles: int = 20\n) -&gt; float:\n    \"\"\"Calculate uplift score using ventiles.\"\"\"\n    # 1. get average probability score of minority class for all ventiles\n    df_class_1_avg_score_by_ventile = (\n        df.query(\"true == True\")\n        .groupby([\"ventile\"], as_index=False)\n        .agg({\"pred_proba\": \"mean\"})\n        .rename(columns={\"pred_proba\": \"avg_score\"})\n    )\n\n    # 2. get average probability score of minority class for top ventile\n    # (numerator of uplift formula)\n    df_class_1_avg_score_top_ventile = df_class_1_avg_score_by_ventile.query(\n        f\"ventile == {num_quantiles-1}\"\n    )\n    if not df_class_1_avg_score_top_ventile.empty:\n        avg_score_top_ventile = df_class_1_avg_score_top_ventile[\"avg_score\"].squeeze()\n    else:\n        avg_score_top_ventile = 0\n\n    # 3. get average probability score of minority class for all ventiles\n    # (denominator of uplift formula)\n    avg_score_all_ventiles = df.query(\"true == True\")[\"pred_proba\"].mean()\n\n    # 4. use ratio to calculate uplift score\n    uplift_score = avg_score_top_ventile / avg_score_all_ventiles\n    return uplift_score\n\nA helper function is defined to retrieve the following metrics that were discussed earlier\n\narea under precision recall curve\nuplift score\n\nthis function generates ventiles and then calls calculate_uplift_score_using_ventiles() to calculate this metric\n\n\n\ndef get_pr_auc(y_true, y_pred_proba, sample_weights) -&gt; float:\n    \"\"\".\"\"\"\n    precision, recall, _ = skm.precision_recall_curve(\n        y_true, y_pred_proba, pos_label=1, sample_weight=sample_weights\n    )\n    auc_score = skm.auc(recall, precision)\n    return auc_score\n\n\ndef get_uplift_score(\n    y_true: np.ndarray, y_pred_proba: np.ndarray, num_ventiles: int = 20\n) -&gt; float:\n    \"\"\"Assign ventiles to predicted probabilities and get uplift.\"\"\"\n    # create DataFrame with reverse-sorted ventiles\n    df = (\n        # create DataFrame from true labels and predicted probabilities\n        pd.DataFrame(\n            zip(*[y_true, y_pred_proba]),\n            columns=[\"true\", \"pred_proba\"],\n        )\n        # convert soft predictions to hard labels\n        .assign(pred=lambda df: (df[\"pred_proba\"] &gt; 0.5).astype(int))\n        # assign ventile\n        .assign(\n            ventile=lambda df: pd.qcut(x=df[\"pred_proba\"], q=num_ventiles, labels=False)\n        )\n        # sort in descending order of predicted probability\n        .sort_values(by=[\"pred_proba\"], ascending=False)\n    )\n    # calculate uplift score\n    uplift_score = calculate_uplift_score_using_ventiles(df, num_ventiles)\n    return uplift_score\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThis calculation of uplift requires a DataFrame with the ventiles assigned to each visitor, based on the predicted probability (pred_proba column). Ventile number should be a column with the name ventile. For ventiles, each bin has a width of 5%. Ventiles are assigned using pandas.cut() with q=20 (link), so the ventile column has values in the range of 0 to 19 where 19 corresponds to the top ventile (visitors who made a purchase on their return visit to the store who are also in the top 5% of predicted probability scores).\n\n\n\nA helper function is defined to get the following metrics\n\nevaluation metrics relevant to the use-case\n\nF0.5-score (places more weight on precision than recall)\n\nprovided by sklearn\n\nArea under the precision recall curve\n\nusing custom helper function above\n\nuplift\n\nusing custom helper function above\n\n\nother evaluation metrics that are provided by sklearn (for informational purposes only)\n\naccuracy\nbalanced accuracy\nprecision\nrecall\nF1-score (places equal weight on precision and recall)\nF2-score (places more weight on recall than precision)\nbrier score\n\n\n\ndef get_metrics(\n    y_true, y_pred, y_pred_proba, average=\"binary\", zero_division=\"warn\"\n) -&gt; Dict[str, float]:\n    \"\"\"Calculate sklearn evaluation metrics.\"\"\"\n    sample_weights = None\n\n    # get area under precision-recall curve\n    pr_auc_score = get_pr_auc(y_true, y_pred_proba, sample_weights)\n\n    # assemble summary dict to compute metrics\n    metrics_dict = dict(\n        # accuracy\n        accuracy=skm.accuracy_score(y_true, y_pred),\n        # balanced accuracy\n        balanced_accuracy=skm.balanced_accuracy_score(y_true, y_pred),\n        # precision\n        precision=skm.precision_score(\n            y_true,\n            y_pred,\n            average=average,\n            sample_weight=sample_weights,\n            zero_division=zero_division,\n        ),\n        # recall\n        recall=skm.recall_score(\n            y_true,\n            y_pred,\n            average=average,\n            sample_weight=sample_weights,\n            zero_division=zero_division,\n        ),\n        # f1\n        f1=skm.f1_score(\n            y_true,\n            y_pred,\n            average=average,\n            sample_weight=sample_weights,\n            zero_division=zero_division,\n        ),\n        # f-0.5\n        f05=skm.fbeta_score(\n            y_true,\n            y_pred,\n            beta=0.5,\n            average=average,\n            sample_weight=sample_weights,\n            zero_division=zero_division,\n        ),\n        # f2\n        f2=skm.fbeta_score(\n            y_true,\n            y_pred,\n            beta=2.0,\n            average=average,\n            sample_weight=sample_weights,\n            zero_division=zero_division,\n        ),\n        # brier score\n        brier=skm.brier_score_loss(\n            y_true, y_pred, sample_weight=sample_weights, pos_label=1\n        ),\n        # area under precision-recall curve (calculated above)\n        pr_auc=pr_auc_score,\n    )\n    return metrics_dict\n\n\n\n\n\n\n\nNotes\n\n\n\n\nFor average, the value chosen is binary so that the metric is calculated and returned for the minority class (visitor made purchase on return visit to merchandise store) only.\nsample weights are not used to calculate the metrics.\n\n\n\nBelow is a wrapper function that calls the two helper functions defined above in order to calculate\n\nvarious evaluation metrics\npercent difference beteeen class imbalance in true and predicted data\nmetadata\n\nThe four steps in this function are\n\ncall get_metrics() in order to compute metrics\ncall calculate_uplift_score_using_ventiles() in order to compute uplift\ncalculate the percent difference between the true and predicted class imbalance\nget metadata\n\nmodel hyperparameters\n\ntype of model (baseline or ML)\n\n\n\n\ndef calculate_metrics(\n    y_true: pd.Series,\n    y_pred: pd.Series,\n    y_pred_proba: pd.Series,\n    df: pd.DataFrame,\n    params: Dict[str, Union[int, float]],\n    num_quantiles: int = 20,\n    model_type: str = \"baseline\",\n    average: str = \"binary\",\n    zero_division: str = \"warn\",\n) -&gt; Dict[str, float]:\n    \"\"\"Score predicted values against true values.\"\"\"\n    # 1. standard metrics\n    scores = get_metrics(\n        y_true,\n        y_pred,\n        y_pred_proba,\n        average=average,\n        zero_division=zero_division,\n    )\n\n    # 2. uplift score\n    uplift_score = calculate_uplift_score_using_ventiles(df, num_quantiles)\n\n    # 3. percent difference between true and predicted class imbalance\n    scores.update(\n        {\n            \"uplift\": uplift_score,\n            \"true_class_imbalance\": y_true.value_counts(normalize=True)\n            .to_frame()\n            .transpose()\n            .assign(class_imbalance=lambda df: df[False] / df[True])[\"class_imbalance\"]\n            .squeeze(),\n            \"pred_class_imbalance\": y_pred.value_counts(normalize=True)\n            .to_frame()\n            .transpose()\n            .assign(class_imbalance=lambda df: df[False] / df[True])[\"class_imbalance\"]\n            .squeeze(),\n        }\n    )\n    scores.update(\n        {\n            \"pct_chg_class_imbalance\": 100\n            * (scores[\"pred_class_imbalance\"] - scores[\"true_class_imbalance\"])\n            / scores[\"true_class_imbalance\"]\n        }\n    )\n\n    # 4. metadata\n    scores.update({\"params\": json.dumps(params), \"model_type\": model_type})\n    return scores\n\nBelow is a helper function to customize the axes of a matplotlib plot\n\ndef customize_axis(ax) -&gt; None:\n    \"\"\"Customize matplotlib axis properties.\"\"\"\n    ax.spines[\"left\"].set_edgecolor(\"black\")\n    ax.spines[\"left\"].set_linewidth(1.5)\n    ax.spines[\"bottom\"].set_edgecolor(\"black\")\n    ax.spines[\"bottom\"].set_linewidth(1.5)\n    ax.spines[\"top\"].set_edgecolor(\"whitesmoke\")\n    ax.spines[\"top\"].set_linewidth(1.5)\n    ax.spines[\"right\"].set_edgecolor(\"whitesmoke\")\n    ax.spines[\"right\"].set_linewidth(1.5)\n    ax.grid(which=\"both\", axis=\"both\", color=\"gainsboro\", zorder=3)"
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html#get-data",
    "href": "notebooks/05-development/notebooks/05_development.html#get-data",
    "title": "Baseline Model Development",
    "section": "Get Data",
    "text": "Get Data\nLoad the transformed data for the training, validation and test data splits\n\ndf_train = pd.read_parquet(\n    os.path.join(processed_data_dir, \"train_processed.parquet.gzip\")\n)\ndf_val = pd.read_parquet(os.path.join(processed_data_dir, \"val_processed.parquet.gzip\"))\ndf_test = pd.read_parquet(\n    os.path.join(processed_data_dir, \"test_processed.parquet.gzip\")\n)\ndf_test.head()\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\ncountry\nquarter\nmonth\nday_of_month\nday_of_week\nhour\n...\nadded_to_cart\nmade_purchase_on_future_visit\nbounces\nlast_action\nsource\nmedium\nchannelGrouping\nbrowser\nos\ndeviceCategory\n\n\n\n\n0\n6667770433164438858\n1487630668\n1\n2017-02-20 14:44:28\nUnited States\n1\n2\n20\n2\n14\n...\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nWindows\ndesktop\n\n\n1\n861894924191702840\n1486761793\n1\n2017-02-10 13:23:13\nUnited States\n1\n2\n10\n6\n13\n...\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nWindows\ndesktop\n\n\n2\n987663115799971454\n1487812839\n1\n2017-02-22 17:20:39\nUnited States\n1\n2\n22\n4\n17\n...\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nChrome\nMacintosh\ndesktop\n\n\n3\n7090908358039029290\n1487412580\n1\n2017-02-18 02:09:40\nUnited States\n1\n2\n18\n7\n2\n...\n0\nFalse\n0\n0\ngoogle\norganic\nOrganic Search\nother\nWindows\ndesktop\n\n\n4\n294074179132707998\n1487899413\n1\n2017-02-23 17:23:33\nUnited States\n1\n2\n23\n5\n17\n...\n0\nFalse\n0\n0\ngoogle\nother\nother\nChrome\nMacintosh\ndesktop\n\n\n\n\n5 rows × 29 columns\n\n\n\nCreate a new split with the combination of the training and validation data splits\n\ndf_train_val = pd.concat(\n    [df_train.assign(split=\"train\"), df_val.assign(split=\"val\")], ignore_index=True\n)\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe split column is appended at the end of the training data split and it contains the string 'train'. This is repeated for the validation split. This is done so we can filter out each of the three splits (train and validation) later.\nThe best hyperparameters will be determined by training a model on the training split and scoring its predictions on the validation split. The best hyperparameters are those for the model with the highest score on the validation split. Using the best hyperparameters, the model is then trained on the combined training and validation split (df_train_val) and then evaluated against the test split. For this reason, df_train_val has been created here.\n\n\n\nShuffle the data in the\n\ncombined training and validation\ntest\n\ndata splits\n\ndf_train_val = df_train_val.sample(frac=1.0, random_state=88)\ndf_test = df_test.sample(frac=1.0, random_state=88)\n\nBaseline model development will be performed using this combined training and validation split. The best baseline model will be evaluated using the test data split."
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html#get-features-and-label",
    "href": "notebooks/05-development/notebooks/05_development.html#get-features-and-label",
    "title": "Baseline Model Development",
    "section": "Get Features and Label",
    "text": "Get Features and Label\nSeparate features from the target in the train and validation splits\n\nX_train = df_train_val.query(\"split == 'train'\").drop(\n    columns=[\"made_purchase_on_future_visit\"]\n)[numerical_features]\ny_train = df_train_val.query(\"split == 'train'\")[\n    \"made_purchase_on_future_visit\"\n].astype(int)\n\nX_val = df_train_val.query(\"split == 'val'\").drop(\n    columns=[\"made_purchase_on_future_visit\"]\n)[numerical_features]\ny_val = df_train_val.query(\"split == 'val'\")[\"made_purchase_on_future_visit\"].astype(\n    int\n)\n\nSeparate features from the target in the combined train-validation split\n\nX_train_val = df_train_val.drop(columns=[\"made_purchase_on_future_visit\"])[\n    numerical_features\n]\ny_train_val = df_train_val[\"made_purchase_on_future_visit\"].astype(int)\n\nSeparate features from the target in the test split\n\nX_test = df_test.drop(columns=[\"made_purchase_on_future_visit\"])[numerical_features]\ny_test = df_test[\"made_purchase_on_future_visit\"].astype(int)"
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html#ml-pipelines",
    "href": "notebooks/05-development/notebooks/05_development.html#ml-pipelines",
    "title": "Baseline Model Development",
    "section": "ML Pipelines",
    "text": "ML Pipelines\n\nCustom Classifier\nDefine a sklearn custom estimator that randomly draws samples from the beta distribution using the beta method of a numpy random generator.\n\nclass BetaDistClassifier(BaseEstimator, ClassifierMixin):\n    \"\"\"Make predictions based on a continuous beta probability distribution.\"\"\"\n\n    def __init__(\n        self,\n        threshold: float = 0.5,\n        a: float = 2.31,\n        b: float = 0.627,\n        random_state=None,\n    ):\n        \"\"\"\n        Initializes BetaDistClassifier.\n\n        Args:\n            a: alpha parameter of beta distribution\n            b: beta parameter of beta distribution\n            threshold: classification discrimination threshold\n            random_state: Controls randomness when sampling from beta distribution\n        \"\"\"\n        self.random_state = random_state\n        self.threshold = threshold\n        self.a = a\n        self.b = b\n\n    def fit(self, X, y):\n        \"\"\"Fit estimator.\"\"\"\n        # self.n_features_in_ = X.shape[1]\n        # self.feature_names_in_ = X.columns.to_numpy()\n\n        # Check that X and y have correct shape\n        X, y = check_X_y(X, y)\n\n        # Store the classes seen during fit\n        self.classes_ = unique_labels(y)\n\n        self.X_ = X\n        self.y_ = y\n        self.random_state_ = check_random_state(self.random_state)\n\n        # set attribute for scikit-learn\n        self.fitted_ = True\n        return self\n\n    def predict_proba(self, X):\n        # Check if fit has been called\n        check_is_fitted(self)\n\n        # Input validation\n        X = check_array(X)\n\n        # randomly sample from beta distribution with specified params a and b\n        n_samples = X.shape[0]\n        y_proba = self.random_state_.beta(a=self.a, b=self.b, size=n_samples)\n\n        # return 2D array for predictions per class (1st column is majority class\n        # and second column is minority class)\n        y_proba = np.stack([1 - y_proba, y_proba]).T\n        return y_proba\n\n    def predict(self, X):\n        \"\"\"Predict class for X.\"\"\"\n        # Check if fit has been called\n        check_is_fitted(self)\n\n        # Input validation\n        X = check_array(X)\n\n        # get class labels from predicted probabilities\n        y = (self.predict_proba(X) &gt; self.threshold).astype(int)[:, 1]\n        return y\n\n    def get_params(self, deep=True):\n        \"\"\"Get parameters for this estimator.\"\"\"\n        return {\n            \"a\": self.a,\n            \"b\": self.b,\n            \"threshold\": self.threshold,\n            \"random_state\": self.random_state,\n        }\n\n    def set_params(self, **parameters):\n        \"\"\"Set the parameters of this estimator.\"\"\"\n        for parameter, value in parameters.items():\n            setattr(self, parameter, value)\n        return self\n\n\nclf_beta = BetaDistClassifier(a=0.25, b=2.371, random_state=88)\n\n\n\nBuilt-In Dummy Classifier\nDefine the sklearn built-in DummyClassifier with stratified sampling to replicate the class-imbalance observed in the training data\n\nclf_dummy = DummyClassifier(strategy=\"stratified\", random_state=88)\n\n\n\nHandling Missing Values\nMissing values are not present in this dataset. See the discussion in the data transformation step for more details.\n\n\nFeature Selection\nFeatures are not used in a baseline model.\n\n\nFeature Processing\nFeatures are not used in a baseline model. With this in mind, encoding categorical features or scaling numerical features is not required for a baseline model.\n\n\nEnd-to-End ML Pipelines\nDefine ML pipelines for all baseline models (classifiers) to be compared in this step\n\npipe_beta = Pipeline(steps=[(\"clf\", clf_beta)])\npipe_dummy = Pipeline(steps=[(\"clf\", clf_dummy)])"
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html#ml-training-using-validation-data",
    "href": "notebooks/05-development/notebooks/05_development.html#ml-training-using-validation-data",
    "title": "Baseline Model Development",
    "section": "ML Training using Validation Data",
    "text": "ML Training using Validation Data\n\nSplitting Data for Validation\nHyper-parameter tuning is performed for the DummyClassifier and BetaDistClassifier using GridSearchCV. This will be done using the validation data without performing cross-validation. This can be performed using the PredefinedSplit from sklearn (link).\nThis is used below to split the combined training and validation data\n\nsplit_index = [-1] * len(df_train_val.query(\"split == 'train'\")) + [0] * len(\n    df_train_val.query(\"split == 'val'\")\n)\npds = PredefinedSplit(test_fold=split_index)\n\n\n\n\n\n\n\nNotes\n\n\n\n\nBy using this PredefinedSplit, we get the following benefits\n\nthe baseline models are trained on the training data split, which is obtained using the split == 'train' filter\nthe class imbalance used in the strategy='stratified' comes from this training data split only without retrieving any observations from the validation data\nwe can use native scikit-learn modules, such as GridSearchCV, with cv=PredefinedSplit(...), to optimize the hyperparameters of the baseline models\n\nthis means we do not need to write manual for loops to iterate over all the hyperparameter combinations to be checked for the baseline models, train (using the training split) and evaluate (using the validation split) each model inside each iteration and then gather the results at the end; instead, we can use GridSearchCV with the desired hyperparameter grid and gather the grid search scores at the end\n\n\n\n\n\n\n\nHyperParameter Tuning\nDefine the hyperparameter grids to be scored for both baseline models\n\nparam_grid_beta = {\n    \"clf__a\": np.linspace(0.2, 0.4, num=8),\n    \"clf__b\": np.linspace(2.2, 2.6, num=8),\n}\nparam_grid_dummy = {\"clf__strategy\": [\"stratified\"]}\n\n\n\nMetrics for Model Scoring\nDefine the following custom metrics to be used during hyperparameter tuning\n\nF0.5-score\nprecision\nbrier score\nuplift, using ventiles\narea under precision-recall curve\n\nThe two metrics that are relevant to this use-case are the F-0.5 score and the uplift score. The primary metric will be the F-0.5 score. The hyperparameters that give the best (highest) score for the primary metric will be chosen as the best hyperparameters.\nA dictionary of sklearn custom scorers is defined below so we can use these metrics with GridSearchCV\n\nscoring = {\n    \"f05\": skm.make_scorer(\n        skm.fbeta_score,\n        beta=0.5,\n        average=\"binary\",\n        sample_weight=None,\n        zero_division=\"warn\",\n    ),\n    \"precision\": skm.make_scorer(\n        skm.precision_score,\n        average=\"binary\",\n        sample_weight=None,\n        zero_division=\"warn\",\n    ),\n    \"precision\": skm.make_scorer(\n        skm.precision_score,\n        average=\"binary\",\n        sample_weight=None,\n        zero_division=\"warn\",\n    ),\n    \"brier\": skm.make_scorer(\n        skm.brier_score_loss,\n        sample_weight=None,\n        pos_label=1,\n    ),\n    \"prauc\": skm.make_scorer(get_pr_auc, sample_weights=None),\n    \"uplift\": skm.make_scorer(get_uplift_score, num_ventiles=20, needs_proba=True),\n}\n\nDefine the columns produced by GridSearchCV under the .cv_results_ attribute that are to be kept\n\nclf\n\nname of model (for this step, the modle name will be baseline)\n\nmean_fit_time\n\ntraining duration\n\nmean_score_time\n\nscoring duration\n\nparams\n\nhyperparameters used\n\nmean_test_f05\n\nF0.5-score on the test split\n\nmean_test_precision\n\nprecision on the test split\n\nmean_test_prauc\n\narea under the precision-recall curve on the test split\n\nmean_test_brier\n\nBrier score on the test split\n\nmean_test_uplift\n\nuplift on the test split\n\n\n\n\nCode\ngs_cols = [\n    \"clf\",\n    \"mean_fit_time\",\n    \"mean_score_time\",\n    \"params\",\n    \"mean_test_f05\",\n    \"mean_test_precision\",\n    \"mean_test_prauc\",\n    \"mean_test_brier\",\n    \"mean_test_uplift\",\n]\n\n\nPerform hyperparameter tuning using GridSearchCV for the custom beta baseline models\n\ngs = GridSearchCV(\n    estimator=pipe_beta,\n    cv=pds,\n    param_grid=param_grid_beta,\n    refit=\"f05\",\n    return_train_score=True,\n    scoring=scoring,\n    n_jobs=None,\n)\n_ = gs.fit(X_train_val, y_train_val)\ndf_validation_scores_beta = pd.DataFrame(gs.cv_results_).assign(\n    clf=type(pipe_beta.named_steps[\"clf\"]).__name__\n)[gs_cols]\nwith pd.option_context(\"display.max_columns\", None):\n    display(df_validation_scores_beta.head(10))\n\n\n\n\n\n\n\n\nclf\nmean_fit_time\nmean_score_time\nparams\nmean_test_f05\nmean_test_precision\nmean_test_prauc\nmean_test_brier\nmean_test_uplift\n\n\n\n\n0\nBetaDistClassifier\n0.010880\n0.034319\n{'clf__a': 0.2, 'clf__b': 2.2}\n0.026617\n0.027894\n0.045713\n0.073948\n6.838147\n\n\n1\nBetaDistClassifier\n0.008813\n0.030943\n{'clf__a': 0.2, 'clf__b': 2.257142857142857}\n0.027956\n0.029762\n0.046647\n0.071823\n7.024176\n\n\n2\nBetaDistClassifier\n0.008540\n0.030084\n{'clf__a': 0.2, 'clf__b': 2.3142857142857145}\n0.034833\n0.037559\n0.052701\n0.069887\n7.733085\n\n\n3\nBetaDistClassifier\n0.008369\n0.029315\n{'clf__a': 0.2, 'clf__b': 2.3714285714285714}\n0.028710\n0.031405\n0.046930\n0.068754\n6.817600\n\n\n4\nBetaDistClassifier\n0.008818\n0.032537\n{'clf__a': 0.2, 'clf__b': 2.428571428571429}\n0.030054\n0.033451\n0.047953\n0.067007\n7.884862\n\n\n5\nBetaDistClassifier\n0.009425\n0.032975\n{'clf__a': 0.2, 'clf__b': 2.4857142857142858}\n0.039409\n0.044527\n0.056185\n0.065165\n7.229893\n\n\n6\nBetaDistClassifier\n0.008515\n0.033642\n{'clf__a': 0.2, 'clf__b': 2.5428571428571427}\n0.030896\n0.035573\n0.048475\n0.064173\n7.274676\n\n\n7\nBetaDistClassifier\n0.008695\n0.033321\n{'clf__a': 0.2, 'clf__b': 2.6}\n0.032040\n0.037500\n0.049438\n0.062946\n6.529846\n\n\n8\nBetaDistClassifier\n0.009925\n0.033463\n{'clf__a': 0.2285714285714286, 'clf__b': 2.2}\n0.044928\n0.045509\n0.064219\n0.077820\n6.849453\n\n\n9\nBetaDistClassifier\n0.010001\n0.033034\n{'clf__a': 0.2285714285714286, 'clf__b': 2.257...\n0.036169\n0.037179\n0.055205\n0.076073\n6.416212\n\n\n\n\n\n\n\nPerform hyperparameter tuning using GridSearchCV for the built-in DummyClassifier baseline model\n\ngs = GridSearchCV(\n    estimator=pipe_dummy,\n    cv=pds,\n    param_grid=param_grid_dummy,\n    refit=\"f05\",\n    return_train_score=True,\n    scoring={k: v for k, v in scoring.items() if k not in [\"uplift\"]},\n    n_jobs=None,\n)\n_ = gs.fit(X_train_val, y_train_val)\ndf_validation_scores_dummy = (\n    pd.DataFrame(gs.cv_results_)\n    .assign(mean_test_uplift=None)\n    .assign(clf=type(pipe_dummy.named_steps[\"clf\"]).__name__)[gs_cols]\n    .sort_values(by=[\"mean_test_f05\"], ascending=False, ignore_index=True)\n)\nwith pd.option_context(\"display.max_columns\", None):\n    display(df_validation_scores_dummy)\n\n\n\n\n\n\n\n\nclf\nmean_fit_time\nmean_score_time\nparams\nmean_test_f05\nmean_test_precision\nmean_test_prauc\nmean_test_brier\nmean_test_uplift\n\n\n\n\n0\nDummyClassifier\n0.007559\n0.011686\n{'clf__strategy': 'stratified'}\n0.0309\n0.031034\n0.051055\n0.080512\nNone\n\n\n\n\n\n\n\nCombine validation scores for both baseline models\n\ndf_validation_scores = (\n    pd.concat([df_validation_scores_dummy, df_validation_scores_beta])\n    .assign(\n        rank=lambda df: df[\"mean_test_f05\"]\n        .rank(ascending=False)\n        .astype(int)\n        .astype(pd.Int8Dtype())\n    )\n    .sort_values(by=[\"rank\"], ascending=True)\n)\ndf_validation_scores.head(10)\n\n\n\n\n\n\n\n\nclf\nmean_fit_time\nmean_score_time\nparams\nmean_test_f05\nmean_test_precision\nmean_test_prauc\nmean_test_brier\nmean_test_uplift\nrank\n\n\n\n\n34\nBetaDistClassifier\n0.008669\n0.030887\n{'clf__a': 0.3142857142857143, 'clf__b': 2.314...\n0.062536\n0.060353\n0.086189\n0.086698\n5.298005\n1\n\n\n57\nBetaDistClassifier\n0.008666\n0.030218\n{'clf__a': 0.4, 'clf__b': 2.257142857142857}\n0.058876\n0.054296\n0.090704\n0.103225\n4.793266\n2\n\n\n58\nBetaDistClassifier\n0.008338\n0.030452\n{'clf__a': 0.4, 'clf__b': 2.3142857142857145}\n0.057165\n0.053130\n0.086888\n0.099967\n4.434738\n3\n\n\n19\nBetaDistClassifier\n0.008352\n0.029666\n{'clf__a': 0.2571428571428572, 'clf__b': 2.371...\n0.056139\n0.057357\n0.074454\n0.075506\n5.901735\n4\n\n\n14\nBetaDistClassifier\n0.008467\n0.032691\n{'clf__a': 0.2285714285714286, 'clf__b': 2.542...\n0.054529\n0.059701\n0.070238\n0.067054\n7.923363\n5\n\n\n18\nBetaDistClassifier\n0.008350\n0.029694\n{'clf__a': 0.2571428571428572, 'clf__b': 2.314...\n0.054135\n0.054461\n0.073545\n0.078292\n6.089055\n6\n\n\n56\nBetaDistClassifier\n0.009725\n0.034417\n{'clf__a': 0.4, 'clf__b': 2.2}\n0.052780\n0.048263\n0.085532\n0.108278\n4.414328\n7\n\n\n23\nBetaDistClassifier\n0.008387\n0.029809\n{'clf__a': 0.2571428571428572, 'clf__b': 2.6}\n0.052423\n0.056061\n0.068956\n0.069651\n6.181383\n8\n\n\n30\nBetaDistClassifier\n0.008796\n0.030543\n{'clf__a': 0.2857142857142857, 'clf__b': 2.542...\n0.052122\n0.053503\n0.070372\n0.075081\n5.883619\n9\n\n\n48\nBetaDistClassifier\n0.008621\n0.031746\n{'clf__a': 0.37142857142857144, 'clf__b': 2.2}\n0.051341\n0.047551\n0.080866\n0.102186\n4.436388\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe scores on the training split are returned by GridSearchCV and they would appear as column names with the prefix mean_train_. These training scores are not used to choose the best hyperparameter. Only the scores on the validation split are used for making this choice.\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nHere, the uplift for the best hyperparameters indicates the chosen baseline model is approximately 5X better than a random model that draws samples randomly from a normal distribution.\nThe hyperparameters with the best f05 in the validation split does not have the highest uplift score. As mentioned earlier, the uplift is not calculated using the true labels, so the model’s predictions of observations in the validation split are not being scored. For this reason, the ML metrics are of relatively higher importance than uplift when scoring predictions for the current use-case.\n\n\n\n\n\nExtracting Best Baseline Model\nGet the best hyperparameters and evaluation scores for both baseline models\n\ndf_validation_scores_best = (\n    df_validation_scores.groupby(\"clf\", as_index=False)\n    .first()\n    .sort_values(by=[\"mean_test_f05\"], ascending=False, ignore_index=True)\n)\ndf_validation_scores_best\n\n\n\n\n\n\n\n\nclf\nmean_fit_time\nmean_score_time\nparams\nmean_test_f05\nmean_test_precision\nmean_test_prauc\nmean_test_brier\nmean_test_uplift\nrank\n\n\n\n\n0\nBetaDistClassifier\n0.008669\n0.030887\n{'clf__a': 0.3142857142857143, 'clf__b': 2.314...\n0.062536\n0.060353\n0.086189\n0.086698\n5.298005\n1\n\n\n1\nDummyClassifier\n0.007559\n0.011686\n{'clf__strategy': 'stratified'}\n0.030900\n0.031034\n0.051055\n0.080512\nNone\n57\n\n\n\n\n\n\n\nFinally, we’ll extract the hyperparameters of the best baseline model\n\nbest_hyperparameters_baseline = df_validation_scores_best.query(\"rank == 1\")[\n    \"params\"\n].squeeze()\nbest_hyperparameters_baseline = {\n    k.replace(\"clf__\", \"\"): v for k, v in best_hyperparameters_baseline.items()\n}\nbest_hyperparameters_baseline\n\n{'a': 0.3142857142857143, 'b': 2.3142857142857145}"
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html#ml-evaluation",
    "href": "notebooks/05-development/notebooks/05_development.html#ml-evaluation",
    "title": "Baseline Model Development",
    "section": "ML Evaluation",
    "text": "ML Evaluation\nThe baseline model with the best hyperparameters is now used to make predictions on the test split and on the combined train and validation split.\n\nMake Predictions\nWe’ll define a new pipeline with the best beta baseline model found from above and use the best hyperparameters\n\npipe = Pipeline(\n    steps=[\n        (\"clf\", BetaDistClassifier(random_state=88, **best_hyperparameters_baseline))\n    ]\n)\n\nWe’ll train the best baseline model using the combined training and validation data split\n\n\nCode\n_ = pipe.fit(X_train_val, y_train_val)\n\n\nWe’ll now make predictions on the\n\ncombined training and validation split\ntest split\n\n\n# combined training and validation\ny_train_val_pred_proba = pd.Series(\n    pipe.predict_proba(X_train_val)[:, 1:].flatten(), index=X_train_val.index\n)\ny_train_val_pred = convert_soft_to_hard_labels(y_train_val_pred_proba, 0.5)\n\n# test\ny_test_pred_proba = pd.Series(\n    pipe.predict_proba(X_test)[:, 1:].flatten(), index=X_test.index\n)\ny_test_pred = convert_soft_to_hard_labels(y_test_pred_proba, 0.5)\n\n\n\nGet True and Predicted Class Imbalance\nNext, calculate the true and predicted class imbalance for all observations (visitors) in the\n\ncombined train and validation data split\ntest data split\n\nA helper function is defined to calculate the percent change in class imbalance\n\ndef get_class_imbalance(y_true: pd.Series, y_pred: pd.Series) -&gt; float:\n    \"\"\"Get percent difference between true and predicted labels.\"\"\"\n    true_class_imbalance = y_true.value_counts(normalize=True)\n    pred_class_imbalance = y_pred.value_counts(normalize=True)\n\n    true_class_ratio = true_class_imbalance[0] / true_class_imbalance[1]\n    pred_class_ratio = pred_class_imbalance[0] / pred_class_imbalance[1]\n\n    pct_chng_class_imbalance = (\n        100 * (pred_class_ratio - true_class_ratio) / true_class_ratio\n    )\n    return [pct_chng_class_imbalance, true_class_ratio, pred_class_ratio]\n\nCalculate true and predicted class imbalance for both splits\n\n(\n    pct_chng_class_imbalance_train_val,\n    true_class_ratio_train_val,\n    pred_class_ratio_train_val,\n) = get_class_imbalance(y_train_val, y_train_val_pred)\n(\n    pct_chng_class_imbalance_test,\n    true_class_ratio_test,\n    pred_class_ratio_test,\n) = get_class_imbalance(y_test, y_test_pred)\n\n\n\nScore Predictions\nFinally, evaluate the predictions on both splits by scoring them using all the metrics described earlier\n\ndef get_eval_scores(\n    y_true: pd.Series,\n    y_pred: pd.Series,\n    y_pred_proba: pd.Series,\n    true_class_ratio: float,\n    pred_class_ratio: float,\n    pct_chng_class_imbalance: float,\n    model_type: str = \"baseline\",\n    split: str = \"test\",\n) -&gt; pd.DataFrame:\n    \"\"\"Get evaluation scores and metadata for predictions.\"\"\"\n    evaluation_scores = get_metrics(y_true, y_pred, y_pred_proba)\n    evaluation_scores.update({\"uplift\": get_uplift_score(y_true, y_pred_proba, 20)})\n    df_evaluation_scores = (\n        # metrics\n        pd.DataFrame.from_dict(evaluation_scores, orient=\"index\")\n        .transpose()\n        # model type\n        .assign(model_type=model_type)\n        # best hyperparameters\n        .assign(params=json.dumps(best_hyperparameters_baseline))\n        # true class imbalance in test data split\n        .assign(true_class_imbalance=true_class_ratio)\n        # predicted class imbalance in test data split\n        .assign(pred_class_imbalance=pred_class_ratio)\n        # percent change between true and predicted class imbalance in test data split\n        .assign(pct_chg_class_imbalance=pct_chng_class_imbalance)\n        .transpose()\n        .reset_index()\n        .rename(columns={\"index\": \"name\", 0: f\"value_{split}\"})\n        .assign(\n            col_type=lambda df: pd.Series(\n                [\"metric\"] * len(evaluation_scores) + [\"metadata\"] * 5\n            )\n        )\n    )\n    return df_evaluation_scores\n\nScoring is performed for the test split and then for the combined train and validation split\n\n# test split\ndf_evaluation_scores_test = get_eval_scores(\n    y_test,\n    y_test_pred,\n    y_test_pred_proba,\n    true_class_ratio_test,\n    pred_class_ratio_test,\n    pct_chng_class_imbalance_test,\n    \"baseline\",\n    \"test\",\n)\n\n# combined train and validation split\ndf_evaluation_scores_train_val = get_eval_scores(\n    y_train_val,\n    y_train_val_pred,\n    y_train_val_pred_proba,\n    true_class_ratio_train_val,\n    pred_class_ratio_train_val,\n    pct_chng_class_imbalance_train_val,\n    \"baseline\",\n    \"train+val\",\n)\n\nThe evaluation scores from both splits are now combined and shown below\n\ndf_evaluation_scores = df_evaluation_scores_train_val.merge(\n    df_evaluation_scores_test, on=[\"name\", \"col_type\"], how=\"left\"\n)\ncol = df_evaluation_scores.pop(\"col_type\")\ndf_evaluation_scores.insert(0, col.name, col)\ndf_evaluation_scores\n\n\n\n\n\n\n\n\ncol_type\nname\nvalue_train+val\nvalue_test\n\n\n\n\n0\nmetric\naccuracy\n0.909574\n0.930123\n\n\n1\nmetric\nbalanced_accuracy\n0.50005\n0.499137\n\n\n2\nmetric\nprecision\n0.043799\n0.022267\n\n\n3\nmetric\nrecall\n0.051287\n0.047312\n\n\n4\nmetric\nf1\n0.047248\n0.030282\n\n\n5\nmetric\nf05\n0.045117\n0.024904\n\n\n6\nmetric\nf2\n0.049592\n0.038624\n\n\n7\nmetric\nbrier\n0.090426\n0.069877\n\n\n8\nmetric\npr_auc\n0.043641\n0.022853\n\n\n9\nmetric\nuplift\n5.37144\n5.665812\n\n\n10\nmetadata\nmodel_type\nbaseline\nbaseline\n\n\n11\nmetadata\nparams\n{\"a\": 0.3142857142857143, \"b\": 2.3142857142857...\n{\"a\": 0.3142857142857143, \"b\": 2.3142857142857...\n\n\n12\nmetadata\ntrue_class_imbalance\n21.873693\n42.363441\n\n\n13\nmetadata\npred_class_imbalance\n18.534181\n19.408907\n\n\n14\nmetadata\npct_chg_class_imbalance\n-15.267253\n-54.184772\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nAccording to the ML metrics (f05, pr_auc, relevant, brier), the baseline model is overfitting since the scores on the combined train and validation data are better than the scores on the test split. By comparison, the uplift is relatively unchanged.\n\n\n\n\n\nPost-Processing of Model Evaluation\nThe true class imbalance of the training, validation, test and combined train and validation data splits is shown below\n\ndf_true_class_imbalance = pd.concat(\n    [\n        (\n            (100 * y.value_counts(normalize=True))\n            .to_frame()\n            .merge(\n                y.value_counts(normalize=False).rename(\"number\").to_frame(),\n                left_index=True,\n                right_index=True,\n            )\n            .reset_index()\n            .astype({\"made_purchase_on_future_visit\": pd.BooleanDtype()})\n            .assign(split=split_name)\n        )\n        for y, split_name in zip(\n            [y_train, y_val, y_train_val, y_test], [\"train\", \"val\", \"train+val\", \"test\"]\n        )\n    ]\n)\ndf_true_class_imbalance\n\n\n\n\n\n\n\n\nmade_purchase_on_future_visit\nproportion\nnumber\nsplit\n\n\n\n\n0\nFalse\n95.407937\n88301\ntrain\n\n\n1\nTrue\n4.592063\n4250\ntrain\n\n\n0\nFalse\n96.590641\n20455\nval\n\n\n1\nTrue\n3.409359\n722\nval\n\n\n0\nFalse\n95.628165\n108756\ntrain+val\n\n\n1\nTrue\n4.371835\n4972\ntrain+val\n\n\n0\nFalse\n97.693910\n19699\ntest\n\n\n1\nTrue\n2.306090\n465\ntest\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nWith such a strong class imbalance, even small changes to the imbalance requires changes in the baseline model hyperparameters. The best baseline model effectively replicated the imbalance of the validation data but it does not generalize to changes in the class imbalance (as is seen in the test data). The poor generalizability is seen in the worse (lower) ML metrics during model evaluation compared to model validation with the best hyperparameters. The baseline model has no input features to learn changes in the underlying features (attributes about visits) between the validation and test data splits. Although it has adequately predicted the validation split labels during hyperparameter tuning, it did not gained any knowledge that generalizes into the test split. This is the limitation of a baseline model.\n\n\n\n\n\nAssessing Evaluation Metrics by Sub-Category for All Categorical Features\nNext, the test split with the predicted - labels - probabilities - ventiles\nis extracted from the predictions made with the best pipeline above\n\nfeatures_to_use = (\n    metadata_features_unused\n    + datetime_features_unused\n    + categorical_features\n    + categorical_features_unused\n)\ndf_test_preds = df_test[features_to_use].merge(\n    (\n        X_test.assign(true=y_test)\n        .assign(pred=y_test_pred)\n        .assign(pred_proba=lambda df: y_test_pred_proba)\n        .astype({\"pred\": pd.BooleanDtype(), \"pred_proba\": pd.Float32Dtype()})\n    ),\n    left_index=True,\n    right_index=True,\n    how=\"left\",\n)\ndf_test_preds.head()\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\n...\nhits\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntime_on_site\ntrue\npred\npred_proba\n\n\n\n\n11418\n603429985440546214\n1486177788\n1\n2017-02-03 19:09:48\n1\n2\n3\n6\n19\n9\n...\n1\n9\n0\n0\n0\n1\n0\n0\nFalse\n0.000842\n\n\n9000\n076259998157345549\n1486014909\n1\n2017-02-01 21:55:09\n1\n2\n1\n4\n21\n55\n...\n1\n9\n0\n0\n0\n1\n0\n0\nFalse\n0.051213\n\n\n15579\n5388501648007496068\n1487815734\n1\n2017-02-22 18:08:54\n1\n2\n22\n4\n18\n8\n...\n6\n9\n0\n60\n0\n6\n142\n0\nFalse\n0.042726\n\n\n14130\n9626632850454813772\n1486040405\n1\n2017-02-02 05:00:05\n1\n2\n2\n5\n5\n0\n...\n1\n9\n0\n0\n0\n1\n0\n0\nFalse\n0.000014\n\n\n20054\n4450994244177849748\n1486402724\n1\n2017-02-06 09:38:44\n1\n2\n6\n2\n9\n38\n...\n3\n0\n0\n12\n1\n2\n23\n0\nFalse\n0.0\n\n\n\n\n5 rows × 27 columns\n\n\n\nFor all categorical features in the transformed data of the test split, get the sub-categories occurring with a frequency of at least 10% or higher. A list is also created that stores these popular sub-categories for each categorical feature so that the test data split can be filtered using this list before calculating metrics.\nThis is done below\n\nmost_frequenct_sub_categories = []\nfor c in categorical_features:\n    # get most popular sub-categories\n    df_top_sub_cats_test = (\n        (100 * df_test_preds[c].value_counts(normalize=True).rename(\"fraction\"))\n        .to_frame()\n        .reset_index()\n        .rename(columns={\"index\": c})\n        .astype({c: pd.BooleanDtype() if c in [\"bounces\"] else pd.StringDtype()})\n        .query(\"fraction &gt;= 10\")\n    )\n    display(df_top_sub_cats_test)\n\n    # create list of popular sub-categories for each category\n    most_frequenct_sub_categories.append(\n        {\"feature\": c, \"top_sub_cats\": df_top_sub_cats_test[c].tolist()}\n    )\n\n\n\n\n\n\n\n\ndeviceCategory\nfraction\n\n\n\n\n0\ndesktop\n69.252133\n\n\n1\nmobile\n26.765523\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbounces\nfraction\n\n\n\n\n0\nFalse\n64.773854\n\n\n1\nTrue\n35.226146\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchannelGrouping\nfraction\n\n\n\n\n0\nOrganic Search\n45.586193\n\n\n1\nDirect\n24.697481\n\n\n2\nother\n16.251736\n\n\n3\nReferral\n13.464590\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmedium\nfraction\n\n\n\n\n0\norganic\n45.586193\n\n\n1\n(none)\n24.697481\n\n\n2\nreferral\n21.860742\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\nfraction\n\n\n\n\n0\ngoogle\n50.976989\n\n\n1\n(direct)\n24.697481\n\n\n\n\n\n\n\nThe list of popular sub-categories is shown below for all categorical features\n\n\nCode\nmost_frequenct_sub_categories\n\n\n[{'feature': 'deviceCategory', 'top_sub_cats': ['desktop', 'mobile']},\n {'feature': 'bounces', 'top_sub_cats': [False, True]},\n {'feature': 'channelGrouping',\n  'top_sub_cats': ['Organic Search', 'Direct', 'other', 'Referral']},\n {'feature': 'medium', 'top_sub_cats': ['organic', '(none)', 'referral']},\n {'feature': 'source', 'top_sub_cats': ['google', '(direct)']}]\n\n\nFinally, we will iterate over all sub-categories and - calculate metrics for each selected sub-category - get metadata (number of observations) for each category and selected sub-category\n\nscores_records = []\nfor sub_cat_record in most_frequenct_sub_categories:\n    top_sub_cats = sub_cat_record[\"top_sub_cats\"]\n    df_top_sub_cats = df_test_preds.query(\n        f\"{sub_cat_record['feature']}.isin(@top_sub_cats)\"\n    )\n    for sub_category in sub_cat_record[\"top_sub_cats\"]:\n        sub_category_filter = (\n            f\"'{sub_category}'\"\n            if sub_cat_record[\"feature\"] != \"bounces\"\n            else sub_category\n        )\n        df_sub_cat = df_test_preds.query(\n            f\"{sub_cat_record['feature']} == {sub_category_filter}\"\n        )\n\n        (\n            pct_chng_class_imbalance_test,\n            true_class_ratio_test,\n            pred_class_ratio_test,\n        ) = get_class_imbalance(df_sub_cat[\"true\"], df_sub_cat[\"pred\"])\n        scores = get_metrics(\n            df_sub_cat[\"true\"],\n            df_sub_cat[\"pred\"],\n            df_sub_cat[\"pred_proba\"],\n        )\n        scores.update(\n            {\n                \"uplift\": get_uplift_score(\n                    df_sub_cat[\"true\"],\n                    df_sub_cat[\"pred_proba\"],\n                    20,\n                )\n            }\n        )\n        # add metadata\n        scores.update(\n            {\n                \"model_type\": \"baseline\",\n                \"feature\": sub_cat_record[\"feature\"],\n                \"sub_category\": sub_category,\n                \"observations_by_subcat\": len(df_sub_cat),\n                \"observations_by_cat\": len(df_top_sub_cats),\n                \"true_class_imbalance\": true_class_ratio_test,\n                \"pred_class_imbalance\": pred_class_ratio_test,\n                \"pct_chg_class_imbalance\": pct_chng_class_imbalance_test,\n            }\n        )\n        print(\n            f\"Got scoring metrics for sub-category: {sub_category} \"\n            f\"in feature: {sub_cat_record['feature']}\"\n        )\n        scores_records.append(scores)\ndf_scores_eval_sub_cats = pd.DataFrame.from_records(scores_records)\n# add ranks\ndf_scores_eval_sub_cats = df_scores_eval_sub_cats.assign(\n    rank_uplift=lambda df: df[\"uplift\"]\n    .rank(method=\"dense\", ascending=False)\n    .astype(pd.Int8Dtype()),\n    rank_uplift_by_feature=lambda df: df.groupby(\"feature\")[\"uplift\"]\n    .transform(\"rank\")\n    .astype(pd.Int8Dtype()),\n)\nwith pd.option_context(\"display.max_columns\", None):\n    display(\n        df_scores_eval_sub_cats.drop(\n            columns=[\"accuracy\", \"balanced_accuracy\", \"f1\", \"f1\", \"uplift\"]\n        )\n    )\n\nGot scoring metrics for sub-category: desktop in feature: deviceCategory\nGot scoring metrics for sub-category: mobile in feature: deviceCategory\nGot scoring metrics for sub-category: False in feature: bounces\nGot scoring metrics for sub-category: True in feature: bounces\nGot scoring metrics for sub-category: Organic Search in feature: channelGrouping\nGot scoring metrics for sub-category: Direct in feature: channelGrouping\nGot scoring metrics for sub-category: other in feature: channelGrouping\nGot scoring metrics for sub-category: Referral in feature: channelGrouping\nGot scoring metrics for sub-category: organic in feature: medium\nGot scoring metrics for sub-category: (none) in feature: medium\nGot scoring metrics for sub-category: referral in feature: medium\nGot scoring metrics for sub-category: google in feature: source\nGot scoring metrics for sub-category: (direct) in feature: source\n\n\n\n\n\n\n\n\n\nprecision\nrecall\nf05\nf2\nbrier\npr_auc\nmodel_type\nfeature\nsub_category\nobservations_by_subcat\nobservations_by_cat\ntrue_class_imbalance\npred_class_imbalance\npct_chg_class_imbalance\nrank_uplift\nrank_uplift_by_feature\n\n\n\n\n0\n0.032258\n0.047191\n0.034438\n0.043192\n0.075480\n0.031606\nbaseline\ndeviceCategory\ndesktop\n13964\n19361\n30.379775\n20.450077\n-32.685227\n5\n1\n\n\n1\n0.003448\n0.055556\n0.004244\n0.013812\n0.056698\n0.007547\nbaseline\ndeviceCategory\nmobile\n5397\n19361\n298.833333\n17.610345\n-94.106968\n1\n2\n\n\n2\n0.032457\n0.049180\n0.034826\n0.044586\n0.079014\n0.032066\nbaseline\nbounces\nFalse\n13061\n20164\n29.587822\n19.187017\n-35.152317\n4\n1\n\n\n3\n0.002933\n0.026316\n0.003566\n0.010142\n0.053076\n0.006567\nbaseline\nbounces\nTrue\n7103\n20164\n185.921053\n19.829912\n-89.334230\n3\n2\n\n\n4\n0.009980\n0.065789\n0.012019\n0.031056\n0.061684\n0.010267\nbaseline\nchannelGrouping\nOrganic Search\n9192\n20164\n119.947368\n17.347305\n-85.537569\n7\n3\n\n\n5\n0.048673\n0.057292\n0.050182\n0.055332\n0.079518\n0.041645\nbaseline\nchannelGrouping\nDirect\n4980\n20164\n24.937500\n21.035398\n-15.647526\n8\n2\n\n\n6\n0.000000\n0.000000\n0.000000\n0.000000\n0.045468\n0.003316\nbaseline\nchannelGrouping\nother\n3277\n20164\n296.909091\n22.746377\n-92.338942\n9\n1\n\n\n7\n0.048780\n0.032258\n0.044248\n0.034602\n0.109392\n0.060657\nbaseline\nchannelGrouping\nReferral\n2715\n20164\n13.596774\n21.073171\n54.986546\n2\n4\n\n\n8\n0.009980\n0.065789\n0.012019\n0.031056\n0.061684\n0.010267\nbaseline\nmedium\norganic\n9192\n18580\n119.947368\n17.347305\n-85.537569\n7\n2\n\n\n9\n0.048673\n0.057292\n0.050182\n0.055332\n0.079518\n0.041645\nbaseline\nmedium\n(none)\n4980\n18580\n24.937500\n21.035398\n-15.647526\n8\n1\n\n\n10\n0.029557\n0.032258\n0.030060\n0.031679\n0.085526\n0.037183\nbaseline\nmedium\nreferral\n4408\n18580\n22.698925\n20.714286\n-8.743317\n2\n3\n\n\n11\n0.009259\n0.062500\n0.011161\n0.029070\n0.059344\n0.009514\nbaseline\nsource\ngoogle\n10279\n15259\n127.487500\n18.035185\n-85.853370\n6\n2\n\n\n12\n0.048673\n0.057292\n0.050182\n0.055332\n0.079518\n0.041645\nbaseline\nsource\n(direct)\n4980\n15259\n24.937500\n21.035398\n-15.647526\n8\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThe true average class imbalance ratio is in the range of 20 to 25 (or, 20:1 to 25:1). There are six sub-categories in the true_class_imbalance column, out of a total of 13 that were selected, that have a true class-imbalance ratio of more than 100 and that is outside this range. By comparison, the predicted class imbalance stays in the range of approximately 19 to 22. As we saw earlier, this baseline model has not learnt from inputs (features) so it is largely insensitive to changes in features and is the primary reason for this discrepancy in ranges.\nBy reading through the rows of the precision, pr_auc, f05, precision and feature columns, we can say that based on these two metrics, the model’s performance is qualitatively worse than the average across all selected sub-categories for the following\n\ndeviceCategory == 'mobile'\nbounces == 'True'\nchannelGrouping == 'other'\nmedium == 'organic'\nsource == 'google'\n\nThese are also five of the six sub-categories mentioned in 1. above that show a significant change in true class imbalance relative to the average true class imbalance. This further re-inforces the need for a model that can learn from features to better handle differences in the class imbalance between training and unseen data.\n\n\n\n\n\nDistribution of Predicted Probabilities and Resulting Class Imbalance\nAs mentioned at the start of this step, we have chosen the beta distribution to represent predicted probabilities (visitor propensities) of a baseline model for the current use-case since it 1. provides floating point values in the range of 0 to 1 (this would match the range of values covered by probabilities, namely 0.0 to 1.0 or 0% to 100%) 2. can be reshaped to achieve the class imbalance seen in return visitor purchases on the Google marketplace\nDuring hyperparameter optimization, we partly used the class imbalance ratio to determine if the chosen beta distribution hyperparameters were an appopriate match to the validation data. Now, we will show the histogram of the baseline model’s probabilities (sampled from the beta distribution with the best hyperparameters) for visitors in the test data split.\nThis is shown below\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 4))\n_ = ax.hist(\n    df_test_preds[\"pred_proba\"],\n    density=True,\n    bins=\"auto\",\n    color=\"red\",\n    histtype=\"stepfilled\",\n    alpha=0.25,\n    label=\"baseline\",\n    zorder=10,\n)\nax.set_xlim([0, 1])\nax.set_title(\n    f\"Distribution of Baseline Model's Sampled Probabilities\",\n    loc=\"left\",\n    fontweight=\"bold\",\n)\nax.set_xlabel(\"Sampled Probabilities\")\nax.set_ylabel(\"Frequency\")\ncustomize_axis(ax)\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThere could be other distributions that can be used as a baseline model for the current use-case, but they were not explored in this version of the analysis.\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nAs expected, the distribution is strongly right skewed with most probabilities being less than 10%. Also the range of sampled values produced by the beta distribution covers a range of 0.0 to 1.0. Both of these observations meet the requirements of a suitable baseline model that were discussed earlier."
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html#export-to-disk",
    "href": "notebooks/05-development/notebooks/05_development.html#export-to-disk",
    "title": "Baseline Model Development",
    "section": "Export to Disk",
    "text": "Export to Disk\nThe scores from the baseline model evaluation section above will now be exported to disk so that ML model performance (in the next step) can be compared to the baseline performance found here.\nDefine datatypes for the DataFrame with the overall model evaluation scores\n\ndtypes_dict_scores = {\n    \"accuracy\": pd.Float32Dtype(),\n    \"balanced_accuracy\": pd.Float32Dtype(),\n    \"precision\": pd.Float32Dtype(),\n    \"recall\": pd.Float32Dtype(),\n    \"f1\": pd.Float32Dtype(),\n    \"f05\": pd.Float32Dtype(),\n    \"f2\": pd.Float32Dtype(),\n    \"brier\": pd.Float32Dtype(),\n    \"pr_auc\": pd.Float32Dtype(),\n    \"uplift\": pd.Float32Dtype(),\n    \"model_type\": pd.StringDtype(),\n    \"params\": pd.StringDtype(),\n    \"true_class_imbalance\": pd.Float32Dtype(),\n    \"pred_class_imbalance\": pd.Float32Dtype(),\n    \"pct_chg_class_imbalance\": pd.Float32Dtype(),\n}\n\nDefine datatypes for the DataFrame with the model evaluation scores by sub-category\n\ndtypes_dict_scores_by_sub_category = {\n    \"accuracy\": pd.Float32Dtype(),\n    \"balanced_accuracy\": pd.Float32Dtype(),\n    \"precision\": pd.Float32Dtype(),\n    \"recall\": pd.Float32Dtype(),\n    \"f1\": pd.Float32Dtype(),\n    \"f05\": pd.Float32Dtype(),\n    \"f2\": pd.Float32Dtype(),\n    \"brier\": pd.Float32Dtype(),\n    \"pr_auc\": pd.Float32Dtype(),\n    \"uplift\": pd.Float32Dtype(),\n    \"model_type\": pd.StringDtype(),\n    \"feature\": pd.StringDtype(),\n    \"sub_category\": pd.StringDtype(),\n    \"observations_by_subcat\": pd.Int16Dtype(),\n    \"observations_by_cat\": pd.StringDtype(),\n    \"true_class_imbalance\": pd.Float32Dtype(),\n    \"pred_class_imbalance\": pd.Float32Dtype(),\n    \"pct_chg_class_imbalance\": pd.Float32Dtype(),\n    \"rank_uplift\": pd.Int8Dtype(),\n    \"rank_uplift_by_feature\": pd.Int8Dtype(),\n}\n\nDefine datatypes for the DataFrame with the model predictions for all observations in the test split\n\ndtypes_dict_predictions = {\n    \"true\": pd.Int8Dtype(),\n    \"bounces\": pd.Int8Dtype(),\n    \"last_action\": pd.Int8Dtype(),\n}\n\nExport overall evaluation scores\n\n(\n    df_evaluation_scores[[\"name\", \"value_test\"]]\n    .set_index(\"name\")\n    .transpose()\n    .astype(dtypes_dict_scores)\n    .to_parquet(\n        os.path.join(models_dir, \"baseline_scores_overall_test.parquet.gzip\"),\n        index=False,\n        engine=\"pyarrow\",\n        compression=\"gzip\",\n    )\n)\n\nExport evaluation scores by sub-category for the popular sub-categories in the categorical features\n\n(\n    df_scores_eval_sub_cats.astype(dtypes_dict_scores_by_sub_category).to_parquet(\n        os.path.join(models_dir, \"baseline_scores_by_sub_category_test.parquet.gzip\"),\n        index=False,\n        engine=\"pyarrow\",\n        compression=\"gzip\",\n    )\n)\n\nExport predicted probabilities for the test split\n\n(\n    df_test_preds.astype(dtypes_dict_predictions).to_parquet(\n        os.path.join(models_dir, \"predictions_test.parquet.gzip\"),\n        index=False,\n        engine=\"pyarrow\",\n        compression=\"gzip\",\n    )\n)"
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html#summary-of-tasks-performed",
    "href": "notebooks/05-development/notebooks/05_development.html#summary-of-tasks-performed",
    "title": "Baseline Model Development",
    "section": "Summary of Tasks Performed",
    "text": "Summary of Tasks Performed\nThis step trained a baseline model to predict visitor propensity to make a purchase during their next visit during January 2017.\nThe steps performed were 1. choose an appropriate baseline model to model prediction probabilities (predicted visitor propensities) - the beta distribution was chosen since it randomly samples values between 0 and 1, which is required for the current use-case 2. model validation - optimize hyperparameters of the baseline model - uses validation data split - determines best hyperparameters 3. model evaluation - uses model with best hyperparameters to make predictions on data that has not been seen during ML model development - uses test data split - post-processing - assesses model’s performance across the most popular sub-categories in all categorical features in the transformed data from the test data split"
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html#key-findings",
    "href": "notebooks/05-development/notebooks/05_development.html#key-findings",
    "title": "Baseline Model Development",
    "section": "Key Findings",
    "text": "Key Findings\n\nThe baseline model cannot generalize since it does not use features. It cannot handle changes in class imbalance seen between model validation and model evaluation.\n\ninput features will likely be key to improving on this performance using a ML model\n\nSeveral evaluation metrics are more sentitive to changes in sub-category than to changes in baseline model hyperparameters\n\nmodel performance should be compared to ML model performance for each of the popular sub-categories\nthis further re-inforces the finding from 1. above"
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html#summary-of-assumptions",
    "href": "notebooks/05-development/notebooks/05_development.html#summary-of-assumptions",
    "title": "Baseline Model Development",
    "section": "Summary of Assumptions",
    "text": "Summary of Assumptions\n\nIt is assumed that the class imbalance can be adequately represented by the beta distribution. The parameters of this distribution are modified during model validation in order to capture the class-imbalance seen in the validation data split.\nThe top three sub-categories are considered for evaluating baseline model performance since these occurred in at least 10% of all observations (visits) per category."
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html#limitations-1",
    "href": "notebooks/05-development/notebooks/05_development.html#limitations-1",
    "title": "Baseline Model Development",
    "section": "Limitations",
    "text": "Limitations\n\nA single choice of skewed distribution (beta) was used to represent the baseline model. This was chosen since it can sample values in the range of 0 to 1, which is the range of prediction probabilities that we want for a binary classification model.\nFor the uplift evaluation metric, ventiles (20 bins) were used. However, other binning approaches (deciles, which would give 10 bins) were not explored.\nThe definition of uplift does not make a comparison of the predicted and true class labels. Instead, it only defines ventiles based on the predicted probability. This is acceptable since uplift is meant to compare a model’s predictive power to a random model. However, this also means it is possible to have a non-zero uplift with an inaccurate model. For this reason, the other metric (area under the precision-recall curve) must also be used to evaluate model performance, in addition to uplift.\nThe baseline model does not learn from features so its hyperparameters were only chosen based on its ability to match the class imbalance seen in the validation data split. This does not generalize to unseen data with a different class imbalance, but this is adequate for a baseline model."
  },
  {
    "objectID": "notebooks/05-development/notebooks/05_development.html#next-step",
    "href": "notebooks/05-development/notebooks/05_development.html#next-step",
    "title": "Baseline Model Development",
    "section": "Next Step",
    "text": "Next Step\nThe next step in the analysis will repeat the analysis in this step, but with a machine learning model used in place of the baseline visitor propensity (binary classifier) model used here. A ML model that is able to deliver value to the business user for this project should outperform the evaluation scores reported here for the baseline model.\nSince the end-to-end ML development workflow is very similar to the workflow shown here, comments will only be included in that step for new steps, such as feature engineering, feature transformation, etc., that were not necessary for a baseline model."
  },
  {
    "objectID": "notebooks/07-create-audience/notebooks/07_create_audience.html",
    "href": "notebooks/07-create-audience/notebooks/07_create_audience.html",
    "title": "Create Marketing Audience Cohorts",
    "section": "",
    "text": "Import Python libraries\nCode\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/07-create-audience/notebooks/07_create_audience.html#about",
    "href": "notebooks/07-create-audience/notebooks/07_create_audience.html#about",
    "title": "Create Marketing Audience Cohorts",
    "section": "About",
    "text": "About\n\nObjective\nThe final step in the workflow for this project is to generate the marketing audience test (or treatment) and control cohorts. Generating the marketing audience cohorts is the deliverable that is required at the end of this project. This step will create this deliverable.\n\n\nAudience Propensity Groups\nThree marketing audience propensity groups were chosen in the previous step (post-processing)\n\nlow\n\nvisitors in this audience group are predicted to have a low propensity (or likelihood) to make a purchase on a return visit to the store\n\nmedium\nhigh\n\nPredictions of propensity (probabilities) come from the ML model’s predicted probabilities for visitors in the unseen data split. The unseen data covers March 1 - 31, 2017 and represents the production period for this project.\n\n\nTest (or Treatment) and Control Cohorts\nThe test and control cohorts are randomly selected from each audience propensity group (low, medium and high). The size of each cohort is chosen based on the output of the previous analysis step (the media experiment design, or post-processing).\nIn the current step\n\nthe control cohort is randomly chosen from each audience propensity group (low, medium and high)\na random sampling from all visitors that do not belong to the control group is drawn and these visitors are placed in the test (or treatment) group\n\nall other visitors do not belong to a cohort\n\n\n\n\nSummary of Analysis in This Step\nIn summary, the final output of this step is to append two new columns to the propensity predictions for the unseen data\n\naudience_group\ncohort\n\n\n\nAssumptions\nAs was the case with the preceding (post-processing, or media design) step, the current step can only be be run on March 31, 2017 after\n\nall the unseen data has been collected\npropensity (probabilities) have been predicted for this data\n\nSo, we will assume that\n\nthe current date is April 1, 2017\nthe unseen data has been\n\ncollected\nprocessed (drop duplicates, bin categorical features, etc. as was done during ML model development)\n\nwe have used the trained ML model to make predictions of the probability (likelihood or propenisty) of making a purchase on a future visit for all visitors in the unseen data"
  },
  {
    "objectID": "notebooks/07-create-audience/notebooks/07_create_audience.html#user-inputs",
    "href": "notebooks/07-create-audience/notebooks/07_create_audience.html#user-inputs",
    "title": "Create Marketing Audience Cohorts",
    "section": "User Inputs",
    "text": "User Inputs\nGet relative path to project root directory\n\nPROJ_ROOT_DIR = os.path.join(os.pardir)\n\nDefine the following\n\naudience_groups\n\ndesired audience groups into which the first-time visitors propensities will be placed\n\nnum_propens_groups specifies the number of groups\npropens_group_labels specifies names of the groups\n\nthis is a Python dictionary with the following key-value pairs\n\nnum_propens_groupsnum_propens_groups represents the number of desired audience propensity groups (low, medium and high)\n\npropens_group_labels gives the desired audience propensity group labels\n\nmin_control_group_sample_sizes\n\nthe least number of samples (first-time visitors) to be included in both the control and test cohorts of each audience group\nthese should come from the output of the previous step (designing a media or marketing experiment)\nthis is a Python list with integers that represent the desired minimum size of the control and test cohort\n\n\n\naudience_groups = {\n    \"num_propens_groups\": 3,\n    \"propens_group_labels\": [\"High\", \"Medium\", \"Low\"],\n}\n\nmin_control_group_sizes = [1000, 2000, 3000]\n\nDefine filepath to data directory where the unseen data, with predictions, is stored\n\nmodel_dir = os.path.join(PROJ_ROOT_DIR, \"models\")\nml_preds_fpath = os.path.join(model_dir, \"unseen_data_predictions.parquet.gzip\")\n\nDefine filepath to data directory where the created audience cohorts will be stored\n\ndata_dir = os.path.join(PROJ_ROOT_DIR, \"data\")\nprocessed_data_dir = os.path.join(data_dir, \"processed\")\n\nCreate a mapping between audience group number (0, 1, 2) and name (high, medium, low), where\n\n0 is mapped to high\n1 is mapped to medium\n2 is mapped to low\n\nsince it is standard to assign a label the top percentile (highest propensity) with the smallest number (0)\n\nmapper_dict_audience = dict(\n    zip(\n        range(audience_groups[\"num_propens_groups\"]),\n        audience_groups[\"propens_group_labels\"],\n    )\n)\n\nDefine a helper function to show datatypes and number of missing values in a DataFrame\n\n\nCode\ndef summarize_df(df: pd.DataFrame) -&gt; None:\n    \"\"\"Show datatypes and count missing values in columns of DataFrame.\"\"\"\n    display(\n        df.dtypes.rename(\"dtype\")\n        .to_frame()\n        .merge(\n            df.isna().sum().rename(\"missing\").to_frame(),\n            left_index=True,\n            right_index=True,\n            how=\"left\",\n        )\n    )"
  },
  {
    "objectID": "notebooks/07-create-audience/notebooks/07_create_audience.html#load-data",
    "href": "notebooks/07-create-audience/notebooks/07_create_audience.html#load-data",
    "title": "Create Marketing Audience Cohorts",
    "section": "Load Data",
    "text": "Load Data\nLoad the predictions of propensity to make a purchase on a return visit for all first-time visitors in the unseen data split\n\ndf = pd.read_parquet(ml_preds_fpath, engine=\"pyarrow\")\ndisplay(df.head())\ndisplay(df.tail())\nsummarize_df(df)\ndf.info()\n\n\n\n\n\n\n\n\nfullvisitorid\nlabel\nscore\npredicted_score_label\n\n\n\n\n0\n9568\nFalse\n0.008079\nFalse\n\n\n1\n18180\nFalse\n0.030055\nFalse\n\n\n2\n154\nFalse\n0.0\nFalse\n\n\n3\n1718\nFalse\n0.000253\nFalse\n\n\n4\n413\nFalse\n0.065967\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfullvisitorid\nlabel\nscore\npredicted_score_label\n\n\n\n\n23995\n19834\nFalse\n0.000077\nFalse\n\n\n23996\n3875\nFalse\n0.000001\nFalse\n\n\n23997\n20780\nFalse\n0.00001\nFalse\n\n\n23998\n11178\nTrue\n0.278035\nFalse\n\n\n23999\n12921\nFalse\n0.003235\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndtype\nmissing\n\n\n\n\nfullvisitorid\nstring[python]\n0\n\n\nlabel\nbool\n0\n\n\nscore\nFloat32\n0\n\n\npredicted_score_label\nboolean\n0\n\n\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 24000 entries, 0 to 23999\nData columns (total 4 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   fullvisitorid          24000 non-null  string \n 1   label                  24000 non-null  bool   \n 2   score                  24000 non-null  Float32\n 3   predicted_score_label  24000 non-null  boolean\ndtypes: Float32(1), bool(1), boolean(1), string(1)\nmemory usage: 375.1 KB\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe fullvisitorid column is the ID of each visitor who\n\nmade a purchase on a return visit to the store\nmade their first visit to the store during the dates covered by the test data split\n\nThe true ML label (y_true) will not be known until a later date, after March 31, 2017 (see the project scope for details)\nThe predicted_score_label column is the predicted label using ML (y_pred).\nThe score column is the predicted probability (using .pred_proba()), which is the propensity of a visitor to make a purchase on a return visit.\nAny other columns are either\n\nused in ML as features (X), or\nnot used in ML"
  },
  {
    "objectID": "notebooks/07-create-audience/notebooks/07_create_audience.html#process-data",
    "href": "notebooks/07-create-audience/notebooks/07_create_audience.html#process-data",
    "title": "Create Marketing Audience Cohorts",
    "section": "Process Data",
    "text": "Process Data\nLabels for the audience and cohort groups will now be assigned to all visitors in the test data.\nFor the audience, all visitors will be placed in one of three groups based on their predicted propensity to make a purchase on a return visit to the store. Groups are created using pandas.qcut(). There will be three such groups - low, medium and high propensity.\nTo do this, a separate column audience_group will be assigned and will contain integers (2, 1, 0). These integers are in descending order since\n\nthe scores (predicted probabilities) have been sorted in descending order\npandas.qcut() assigns bin numbers starting at 0, which is a standard practice (see the explanation for mapper_dict_audience earlier in this step)\n\nLater, labels (Low, Medium or High) will be mapped to the integers using the mapping dictionary created above (mapper_dict_audience).\nThe the cohort, a subset of visitors will be placed in one of two cohorts - test (or treatment) and control. All other visitors will be assigned a choort of None. To do this, a separate column audience_group will be assigned and will contain Control, Test or None.\n\nGet Audience Groups Based on Propensity to Make Purchase on Return Visit\nTo create the audience for the marketing campaign, we’ll first\n\n(optionally) sort the predicted probabilities in the unseen (or inference) data, in descending order\nassign a row number to each observation (i.e. to each row or visitor) in this unseen data\n\n\ndf = (\n    df\n    .sort_values(by=\"score\", ascending=False)\n    .assign(row_number=lambda df: range(len(df)))\n)\ndf.head()\n\nCPU times: user 3.09 ms, sys: 2.87 ms, total: 5.96 ms\nWall time: 5.35 ms\n\n\n\n\n\n\n\n\n\nfullvisitorid\nlabel\nscore\npredicted_score_label\nrow_number\n\n\n\n\n5277\n6539\nFalse\n0.957472\nTrue\n0\n\n\n997\n15317\nFalse\n0.929886\nTrue\n1\n\n\n12343\n4313\nFalse\n0.918627\nTrue\n2\n\n\n1644\n9806\nFalse\n0.908974\nTrue\n3\n\n\n22156\n4375\nFalse\n0.906313\nTrue\n4\n\n\n\n\n\n\n\nNext, we’ll bin the predicted probabilities using their quantiles with pandas.qcut, where the visitors are binned based on visit row number in order to avoid duplication of bin boundaries (three bins are created since earlier we specified we wanted. Three audience groups, namely low, medium and high propensity to purchase.\nThis is done below\n\ndf[\"audience_group\"] = pd.qcut(\n    x=df[\"row_number\"], q=audience_groups[\"num_propens_groups\"], labels=False\n)\ndisplay(df.head())\ndisplay(df.tail())\n\n\n\n\n\n\n\n\nfullvisitorid\nlabel\nscore\npredicted_score_label\nrow_number\naudience_group\n\n\n\n\n5277\n6539\nFalse\n0.957472\nTrue\n0\n0\n\n\n997\n15317\nFalse\n0.929886\nTrue\n1\n0\n\n\n12343\n4313\nFalse\n0.918627\nTrue\n2\n0\n\n\n1644\n9806\nFalse\n0.908974\nTrue\n3\n0\n\n\n22156\n4375\nFalse\n0.906313\nTrue\n4\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfullvisitorid\nlabel\nscore\npredicted_score_label\nrow_number\naudience_group\n\n\n\n\n21391\n11855\nFalse\n0.0\nFalse\n23995\n2\n\n\n12926\n1147\nFalse\n0.0\nFalse\n23996\n2\n\n\n1387\n10329\nFalse\n0.0\nFalse\n23997\n2\n\n\n20790\n14354\nFalse\n0.0\nFalse\n23998\n2\n\n\n575\n610\nFalse\n0.0\nFalse\n23999\n2\n\n\n\n\n\n\n\n\n\nCreate Test and Control Cohorts\nWith the three audience propensity groups now assigned, we can create the test and control cohorts within each audience group.\nTest and control group cohorts should be randomly selected without replacement. This means the same visitor should not be randomly selected multiple times from the audience group and placed in each cohort. The two cohorts need to be independent and so the same visitor cannot be assigned to both cohorts.\nIn Python, there are two options to make such random selections without duplication\n\nrandom.sample() in the Python standard library\nnumpy.random.choice() with replacement=False in the numpy library\n\nHere, the numpy approach is used.\nTo create these groups, we’ll iterate over each of the desired minimum test and control visitor cohort sizes that were recommended by the previous step about experiment design (post-processing) and then draw a random sample from each propensity audience group to be chosen as control and test.\nThe approach will consist of the following steps for each audience group (low, medium or high propensity)\n\ndraw random sample of visitors, without replacement, as the control group\nget all non-selected visitors\nfrom non-selected visitors, draw random sample of visitors, without replacement, as the treatment (test) group\n(optional) get all visitors that are not part of either control or test group\nextract the following\n\nall features (X)\nmetadata columns not used in ML\npredicted ML label (y_pred)\npredicted propensities (probabilities, in the score column)\n\nand store the results in\n\ndf_test (test or treatment cohort of visitors)\ndf_control (control cohort of visitors)\n(optional) df_excluded (cohort of visitors that are not selected in either test or control group)\n\n\nThis approach is shown below\n\ngroups = []\nfor k, group_size in enumerate(min_control_group_sizes):\n    df_audience = df.query(f\"audience_group == {k}\")\n    audience_array = df_audience['fullvisitorid'].to_numpy()\n\n    # 1. get control \n    rng = np.random.default_rng(88)\n    control_group_visitors = rng.choice(audience_array, group_size, replace=False).tolist()\n\n    # 2. get all remaining visitors\n    other_array = list(set(audience_array) - set(control_group_visitors))\n\n    # 3. get test group visitors by randomly sampling from remaining visitors\n    test_group_visitors = rng.choice(other_array, group_size, replace=False).tolist()\n\n    # 4. get combined control and test cohorts of visitors\n    combined_cohort_visitors = control_group_visitors + test_group_visitors\n    # get all excluded visitors (not part of test or control cohort)\n    excluded_visitors = list(set(audience_array) - set(combined_cohort_visitors))\n\n    # # get audience scores\n    # audience_scores = df_audience.query(\n    #     \"fullvisitorid.isin(@combined_cohort_visitors)\"\n    # )['score'].to_numpy()\n\n    print(\n        f\"audience={k}: {mapper_dict_audience[k]}, \"\n        f\"size={len(audience_array):,}, \",\n        f\"excluded={len(other_array):,}, \"\n        f\"wanted={group_size:,}, \"\n        f\"control={len(control_group_visitors):,}, \"\n        f\"test={len(test_group_visitors):,}\"\n    )\n\n    # 5. get extract attributes for each cohort per audience group\n    df_test = (\n        df_audience.drop(columns=['row_number'])\n        .query(\"fullvisitorid.isin(@test_group_visitors)\")\n        .assign(audience_group=k)\n        .assign(cohort='Test')\n    )\n    df_control = (\n        df_audience.drop(columns=['row_number'])\n        .query(\"fullvisitorid.isin(@control_group_visitors)\")\n        .assign(audience_group=k)\n        .assign(cohort='Control')\n    )\n    df_excluded = (\n        df_audience.drop(columns=['row_number'])\n        .query(\"fullvisitorid.isin(@excluded_visitors)\")\n        .assign(audience_group=k)\n        .assign(cohort=None)\n    )\n    df_coh = pd.concat([df_control, df_test, df_excluded], ignore_index=True)\n    groups.append(df_coh)\ndf_test_audience_groups = (\n    pd.concat(groups, ignore_index=True)\n    .assign(audience_group=lambda df: df['audience_group'].map(mapper_dict_audience))\n    .astype({'audience_group': pd.StringDtype(), 'cohort': pd.StringDtype()})\n)\ndisplay(df_test_audience_groups.head())\ndisplay(df_test_audience_groups.head())\n\naudience=0: High, size=8,000,  excluded=7,000, wanted=1,000, control=1,000, test=1,000\naudience=1: Medium, size=8,000,  excluded=6,000, wanted=2,000, control=2,000, test=2,000\naudience=2: Low, size=8,000,  excluded=5,000, wanted=3,000, control=3,000, test=3,000\nCPU times: user 61.1 ms, sys: 5.16 ms, total: 66.3 ms\nWall time: 65.6 ms\n\n\n\n\n\n\n\n\n\nfullvisitorid\nlabel\nscore\npredicted_score_label\naudience_group\ncohort\n\n\n\n\n0\n11635\nFalse\n0.805879\nTrue\nHigh\nControl\n\n\n1\n9652\nFalse\n0.783223\nTrue\nHigh\nControl\n\n\n2\n15041\nFalse\n0.769196\nTrue\nHigh\nControl\n\n\n3\n6620\nFalse\n0.765363\nTrue\nHigh\nControl\n\n\n4\n3295\nFalse\n0.736128\nTrue\nHigh\nControl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfullvisitorid\nlabel\nscore\npredicted_score_label\naudience_group\ncohort\n\n\n\n\n0\n11635\nFalse\n0.805879\nTrue\nHigh\nControl\n\n\n1\n9652\nFalse\n0.783223\nTrue\nHigh\nControl\n\n\n2\n15041\nFalse\n0.769196\nTrue\nHigh\nControl\n\n\n3\n6620\nFalse\n0.765363\nTrue\nHigh\nControl\n\n\n4\n3295\nFalse\n0.736128\nTrue\nHigh\nControl\n\n\n\n\n\n\n\nEarlier, we mentioned that the treatment (or test) and control groups should be similar to each other for the property to be tested. This is a fundamental requirement of test and control groups. In this case, the probability (score column) is the property of interest.\nWith this in mind, we now show selected descriptive statistics for both the test (treatment) and control cohorts within each desired audience group (low, medium and high propensity)\n\ndf_aud_stats = df_test_audience_groups.groupby(\n    [\"audience_group\", \"cohort\"], dropna=False, as_index=False\n).agg({\"score\": [\"count\", \"min\", \"mean\", \"median\", \"max\"]})\ndf_aud_stats.columns = [\n    \"_\".join(c).rstrip(\"_\") for c in df_aud_stats.columns.to_flat_index()\n]\ndf_aud_stats = df_aud_stats.astype(\n    {f\"score_{stat}\": pd.Float32Dtype() for stat in [\"min\", \"mean\", \"median\", \"max\"]}\n).sort_values(by=[\"audience_group\"])\ndf_aud_stats\n\n\n\n\n\n\n\n\naudience_group\ncohort\nscore_count\nscore_min\nscore_mean\nscore_median\nscore_max\n\n\n\n\n0\nHigh\nControl\n1000\n0.011979\n0.137127\n0.073412\n0.805879\n\n\n1\nHigh\nTest\n1000\n0.011967\n0.145986\n0.083493\n0.918627\n\n\n2\nHigh\n&lt;NA&gt;\n6000\n0.011962\n0.143285\n0.077881\n0.957472\n\n\n3\nLow\nControl\n3000\n0.0\n0.000006\n0.0\n0.000052\n\n\n4\nLow\nTest\n3000\n0.0\n0.000006\n0.0\n0.000052\n\n\n5\nLow\n&lt;NA&gt;\n2000\n0.0\n0.000006\n0.0\n0.000052\n\n\n6\nMedium\nControl\n2000\n0.000053\n0.002735\n0.001312\n0.011958\n\n\n7\nMedium\nTest\n2000\n0.000053\n0.002649\n0.00122\n0.011951\n\n\n8\nMedium\n&lt;NA&gt;\n4000\n0.000053\n0.002617\n0.001202\n0.011958\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nWith the exception of outliers in the High propensity audience group, we see good agreement in the statistics for the probabilities per Test-Control cohort within the same audience group."
  },
  {
    "objectID": "notebooks/07-create-audience/notebooks/07_create_audience.html#export-to-disk",
    "href": "notebooks/07-create-audience/notebooks/07_create_audience.html#export-to-disk",
    "title": "Create Marketing Audience Cohorts",
    "section": "Export to Disk",
    "text": "Export to Disk\nFinally, the unseen data with the\n\naudience (audience_group)\ncohorts (cohort)\n\ncolumns assigned will now be exported to disk for use by the marketing team.\nShow the datatypes\n\n\nCode\nsummarize_df(df_test_audience_groups)\n\n\n\n\n\n\n\n\n\ndtype\nmissing\n\n\n\n\nfullvisitorid\nstring[python]\n0\n\n\nlabel\nbool\n0\n\n\nscore\nFloat32\n0\n\n\npredicted_score_label\nboolean\n0\n\n\naudience_group\nstring[python]\n0\n\n\ncohort\nstring[python]\n12000\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe cohort has missing values for visitors who were not assigned to either the test or control groups. This is expected.\n\n\n\nSave to disk\n\naudience_cohorts_fpath = os.path.join(\n    processed_data_dir, \"marketing_audience_cohort_groups__unseen_data.parquet.gzip\"\n)\ndf_test_audience_groups.to_parquet(\n    audience_cohorts_fpath, index=False, engine=\"pyarrow\", compression=\"gzip\"\n)"
  },
  {
    "objectID": "notebooks/07-create-audience/notebooks/07_create_audience.html#summary-of-tasks-performed",
    "href": "notebooks/07-create-audience/notebooks/07_create_audience.html#summary-of-tasks-performed",
    "title": "Create Marketing Audience Cohorts",
    "section": "Summary of Tasks Performed",
    "text": "Summary of Tasks Performed\nThis step in the project’s overall workflow has performed the following\n\ncreated marketing audience propensity groups\ncreated test (or treatment) and control cohorts\ndemonstrated similarity between test and control cohorts, in terms of propensity to make a purchase, as required"
  },
  {
    "objectID": "notebooks/07-create-audience/notebooks/07_create_audience.html#summary-of-assumptions",
    "href": "notebooks/07-create-audience/notebooks/07_create_audience.html#summary-of-assumptions",
    "title": "Create Marketing Audience Cohorts",
    "section": "Summary of Assumptions",
    "text": "Summary of Assumptions\n\nAs was the case with the preceding (post-processing, or media design) step, the current step can only be be run on March 31, 2017 after\n\nall the unseen data has been collected\npropensity (probabilities) have been predicted for this data\n\nSo, we will assume that\n\nthe current date is April 1, 2017\nthe unseen data has been\n\ncollected\nprocessed (drop duplicates, bin categorical features, etc. as was done during ML model development)\n\nwe have used the trained ML model to make predictions of the probability (likelihood or propenisty) of making a purchase on a future visit for all visitors in the unseen data1. As was the case with the preceding (post-processing, or media design) step, the current step can only be be run on March 31, 2017 after\nall the unseen data has been collected\npropensity (probabilities) have been predicted for this data\n\nSo, we have assumed that\n\nthe current date is April 1, 2017\nthe unseen data has been\n\ncollected\nprocessed (drop duplicates, bin categorical features, etc. as was done during ML model development)\n\nwe have used the trained ML model to make predictions of the probability (likelihood or propenisty) of making a purchase on a future visit for all visitors in the unseen data"
  },
  {
    "objectID": "notebooks/07-create-audience/notebooks/07_create_audience.html#limitations",
    "href": "notebooks/07-create-audience/notebooks/07_create_audience.html#limitations",
    "title": "Create Marketing Audience Cohorts",
    "section": "Limitations",
    "text": "Limitations\n\nIn practice, we may want to perform a brief profiling of the cohorts (get the main attributes using some of the columns with ML features such as browser, number of pageviews, etc.) within each audience propensity group and append this profile as a new column as this might help the marketing team devise an appropriate strategy/promotion.\nProfiling of the test and control cohorts within each of the audience propensity groups could provide helpful context to the marketing team when they are devising a marketing promotion/strategy. This cohort profiling step was not performed here."
  }
]