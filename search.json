[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning for Propensity Modeling",
    "section": "",
    "text": "E-Commerce is important since it allows a business to reach customers across a larger footprint than a group of physical (brick-and-mortar) stores. Visitors to an e-commerce store can make a purchase at an time, in any location and in their choice of currency. The ability to attract such a diverse customer base is the main value of e-commerce to a business. While website traffic is a highly-tracked metric by e-commerce businesses, their hard work and efforts to attract visitors to their site should not go to waste. It is customers that every e-commerce site owner needs to sustain their business."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Machine Learning for Propensity Modeling",
    "section": "",
    "text": "E-Commerce is important since it allows a business to reach customers across a larger footprint than a group of physical (brick-and-mortar) stores. Visitors to an e-commerce store can make a purchase at an time, in any location and in their choice of currency. The ability to attract such a diverse customer base is the main value of e-commerce to a business. While website traffic is a highly-tracked metric by e-commerce businesses, their hard work and efforts to attract visitors to their site should not go to waste. It is customers that every e-commerce site owner needs to sustain their business."
  },
  {
    "objectID": "index.html#problem-with-visitors-to-an-e-commerce-store",
    "href": "index.html#problem-with-visitors-to-an-e-commerce-store",
    "title": "Machine Learning for Propensity Modeling",
    "section": "Problem with Visitors to an E-Commerce Store",
    "text": "Problem with Visitors to an E-Commerce Store\nThe majority of visitors to an e-commerce site leave without performing a transaction (making a purchase) on their first visit. In the case of the the Google merchandise store’s site, the fraction of such visitors leaving is greater than 95%. Industry research shows that the majority of purchases by a visitor to such a site don’t occur on the visitor’s first visit. If they do purchase, then more often than not they will return later and make a purchase."
  },
  {
    "objectID": "index.html#why-market-to-return-users",
    "href": "index.html#why-market-to-return-users",
    "title": "Machine Learning for Propensity Modeling",
    "section": "Why Market to Return Users?",
    "text": "Why Market to Return Users?\nBeing able to identify such high-value visitors ahead of time can be of tremendous help to a marketing team to develop campaigns to grow the number of first-time visitors who make a purchase (converters) or a return purchase (repeat customers).\nThe marketing team can design and deploy a campaign after visitors’ first visit to improve their chances of making a purchase on a future visit."
  },
  {
    "objectID": "index.html#why-machine-learning",
    "href": "index.html#why-machine-learning",
    "title": "Machine Learning for Propensity Modeling",
    "section": "Why Machine Learning?",
    "text": "Why Machine Learning?\nThrough the use of machine learning (ML), we can scale this approach to capture all first-time visitors to the store and also improve their likelihood (or propensity) to make a future purchase while they search on a competitor’s site for the same or a similar product. Doing so is one way to help grow the base of converters and repeat customers."
  },
  {
    "objectID": "index.html#who-is-the-business-client",
    "href": "index.html#who-is-the-business-client",
    "title": "Machine Learning for Propensity Modeling",
    "section": "Who is the Business Client?",
    "text": "Who is the Business Client?\nThis project would be directly useful to Robertson Marketing, who is responsible for management of the Google merchandise store."
  },
  {
    "objectID": "index.html#what-is-this-project-about",
    "href": "index.html#what-is-this-project-about",
    "title": "Machine Learning for Propensity Modeling",
    "section": "What is This Project About?",
    "text": "What is This Project About?\nIn this project, ML predictions are used to select a marketing audience with a low, medium and high propensity to make a purchase on a return visit. Within each group, we also develop and briefly profile test (or treatment) and control cohorts in order to help facilitate deployment of a marketing campaign."
  },
  {
    "objectID": "references/Scope.html",
    "href": "references/Scope.html",
    "title": "Project Scope",
    "section": "",
    "text": "The Google Merchandise store is an e-commerce store that sells Google-branded products. Less than 5% of visitors make a purchase from the Google merchandise store on the Google Marketplace. So, a large number of visitors are not making a purchase. They are either just visiting the store once (their first and only visit) and leaving, or visiting multiple times and but not making a purchase on any of those visits. It goes without saying that customers, and not visitors alone, ensure the sustainability of an e-commerce business.\nConsiderable effort has been made by the web design team to build the store’s website to attract site traffic and make a good impression on first-time visitors to the store. This effort should not go to waste. However, approximately 90% of purchases do not happen during an initial visit to an e-commerce website. Furthermore, repeat customers spend 33% more with a brand than new customers do. Only approximately 20% of existing customers account for approximately 80% of future profits. Getting visitors to return to a site is important, but is possibly of equal or greater importance to an e-commerce business to have these visitors make a purchase on a return visit. This underscores the importance of getting visitors to make a purchase on a subsequent visit to the store.\nBut, it is not just sufficient to ensure return visits occur. This is of no use to a business since they can’t grow customer revenue by relying visitors to return and make purchaes on their own volition. The visitor should be motivated between visits to return and make a purchase on one or more future visits, rather than just returning and browsing through the store as they did during a previous visit. Between August 1, 2016 and August 1, 2017, a little less than 2% of visitors made a purchase on a return visit to the Google merchandise store. One of the main reasons for visitors browsing an e-commerce store, rather than making a purchase, is because they are comparson-shopping across multiple such websites looking for the lowest price for the same or a very similar product.\nIn summary, it will help the Google Merchandise store grow their customer base and increase revenue if some of these first-time visitors who have\n\nnot made a purchase during their first visit\nmade a purchase during their first visit\n\nto the store will make a purchase during a subsequent visit. In other words, it is desirable that first-time visitors to the store become customers or repeat customers.\n\n\n\nIf the business can find ways to reach out to (interact with) visitors to the store after their first visit, and provide promotions, shipping offers, etc., then this could be one way to motivate these visitors to make a purchase on a return visit to the store. If the Google merchandise store management company can reach out to these first-time visitors based on the characteristics of their first visit to the store and get them to make a purchase on a return visit then this can not only grow the business’ customer base but also reduce the loss of customer revenue to a competitor.\nAlphabet is the parent company of Google, but Robertson Marketing is the company that manages the Google Merchandise store. The management company (Robertson Marketing) is impacted by the problem of low conversions or repeat customers among the pool of first-time visitors to the merchandise store and they would be interested in ways to turn such first-time visitors into one of the following\n\nfuture customers (converting visitors into customers, or conversions)\nrepeat-customers (customer retention)\n\n\n\n\nThe business (store management company, Robertson Marketing) has tasked its marketing team with growing\n\nnew customers (conversion)\nrepeat customers (getting customers to become repeat customers)\n\nfrom the pool of first-time visitors to the store.\nFor obvious reasons, not all first-time visitors to the merchandise store are alike and reaching out to visitors is a costly process. With this mind, the marketing team would like to design an appropriate marketing campaign to help achive the business’ objectives. With a strong preference to spend marketing funds (budget) wisely, the marketing team wants to interact with such first-time visitors through focused and relevant recommendations, reminders and other types of marketing promotions after their first visit to the store.\nA logical approach to developing targeted promotions to grow customers or repeat customers is to offer a promotion based on a visitor’s likelihood of making a purchase during a future visit to the merchandise store. Knowing this likelihood is useful in knowing which visitors the marketing team should be focused on and, subsequently, how much funding can be allocated to communicating with those visitors. Accordingly, the business can determine the type of promotion that should be offered. For example, a minimal discount could be offered to visitors with a high likelihood of making a future purchase. Similarly, more loyalty points, coupon giveaways or free shipping could be offered to visitors who are deemed less likely to make a future purchase from the store. These promotions can be offered after a visitor’s first visit to the store with the aim of persuading them to make a purchase during a future visit.\n\n\n\nWithout knowing visitors’ likelihood of making a purchase on a return visit, it is not possible to segment visitors into audience groups (eg. visitors with a high, medium or low likelihood of making a purchase on a return visit) after their first visit. If these groups and cohorts were known, the marketing team could test how they respond to marketing strategies. Naive random guesses at visitor likelihood groupings are costly and unlikely to get buy-in for funding requests from business management. Furthermore, a test and control cohort is needed within each group in order to quantitatively determine how each group responds to the chosen marketing campaign (i.e. these cohorts are needed to evaluate the performance of the campaign).\nWe will assume that the marketing team has not yet designed any approaches to address this problem. With this in mind, both the size of the audience groups and the size of these cohorts are currently completely unknown."
  },
  {
    "objectID": "references/Scope.html#understanding-the-problem",
    "href": "references/Scope.html#understanding-the-problem",
    "title": "Project Scope",
    "section": "",
    "text": "The Google Merchandise store is an e-commerce store that sells Google-branded products. Less than 5% of visitors make a purchase from the Google merchandise store on the Google Marketplace. So, a large number of visitors are not making a purchase. They are either just visiting the store once (their first and only visit) and leaving, or visiting multiple times and but not making a purchase on any of those visits. It goes without saying that customers, and not visitors alone, ensure the sustainability of an e-commerce business.\nConsiderable effort has been made by the web design team to build the store’s website to attract site traffic and make a good impression on first-time visitors to the store. This effort should not go to waste. However, approximately 90% of purchases do not happen during an initial visit to an e-commerce website. Furthermore, repeat customers spend 33% more with a brand than new customers do. Only approximately 20% of existing customers account for approximately 80% of future profits. Getting visitors to return to a site is important, but is possibly of equal or greater importance to an e-commerce business to have these visitors make a purchase on a return visit. This underscores the importance of getting visitors to make a purchase on a subsequent visit to the store.\nBut, it is not just sufficient to ensure return visits occur. This is of no use to a business since they can’t grow customer revenue by relying visitors to return and make purchaes on their own volition. The visitor should be motivated between visits to return and make a purchase on one or more future visits, rather than just returning and browsing through the store as they did during a previous visit. Between August 1, 2016 and August 1, 2017, a little less than 2% of visitors made a purchase on a return visit to the Google merchandise store. One of the main reasons for visitors browsing an e-commerce store, rather than making a purchase, is because they are comparson-shopping across multiple such websites looking for the lowest price for the same or a very similar product.\nIn summary, it will help the Google Merchandise store grow their customer base and increase revenue if some of these first-time visitors who have\n\nnot made a purchase during their first visit\nmade a purchase during their first visit\n\nto the store will make a purchase during a subsequent visit. In other words, it is desirable that first-time visitors to the store become customers or repeat customers.\n\n\n\nIf the business can find ways to reach out to (interact with) visitors to the store after their first visit, and provide promotions, shipping offers, etc., then this could be one way to motivate these visitors to make a purchase on a return visit to the store. If the Google merchandise store management company can reach out to these first-time visitors based on the characteristics of their first visit to the store and get them to make a purchase on a return visit then this can not only grow the business’ customer base but also reduce the loss of customer revenue to a competitor.\nAlphabet is the parent company of Google, but Robertson Marketing is the company that manages the Google Merchandise store. The management company (Robertson Marketing) is impacted by the problem of low conversions or repeat customers among the pool of first-time visitors to the merchandise store and they would be interested in ways to turn such first-time visitors into one of the following\n\nfuture customers (converting visitors into customers, or conversions)\nrepeat-customers (customer retention)\n\n\n\n\nThe business (store management company, Robertson Marketing) has tasked its marketing team with growing\n\nnew customers (conversion)\nrepeat customers (getting customers to become repeat customers)\n\nfrom the pool of first-time visitors to the store.\nFor obvious reasons, not all first-time visitors to the merchandise store are alike and reaching out to visitors is a costly process. With this mind, the marketing team would like to design an appropriate marketing campaign to help achive the business’ objectives. With a strong preference to spend marketing funds (budget) wisely, the marketing team wants to interact with such first-time visitors through focused and relevant recommendations, reminders and other types of marketing promotions after their first visit to the store.\nA logical approach to developing targeted promotions to grow customers or repeat customers is to offer a promotion based on a visitor’s likelihood of making a purchase during a future visit to the merchandise store. Knowing this likelihood is useful in knowing which visitors the marketing team should be focused on and, subsequently, how much funding can be allocated to communicating with those visitors. Accordingly, the business can determine the type of promotion that should be offered. For example, a minimal discount could be offered to visitors with a high likelihood of making a future purchase. Similarly, more loyalty points, coupon giveaways or free shipping could be offered to visitors who are deemed less likely to make a future purchase from the store. These promotions can be offered after a visitor’s first visit to the store with the aim of persuading them to make a purchase during a future visit.\n\n\n\nWithout knowing visitors’ likelihood of making a purchase on a return visit, it is not possible to segment visitors into audience groups (eg. visitors with a high, medium or low likelihood of making a purchase on a return visit) after their first visit. If these groups and cohorts were known, the marketing team could test how they respond to marketing strategies. Naive random guesses at visitor likelihood groupings are costly and unlikely to get buy-in for funding requests from business management. Furthermore, a test and control cohort is needed within each group in order to quantitatively determine how each group responds to the chosen marketing campaign (i.e. these cohorts are needed to evaluate the performance of the campaign).\nWe will assume that the marketing team has not yet designed any approaches to address this problem. With this in mind, both the size of the audience groups and the size of these cohorts are currently completely unknown."
  },
  {
    "objectID": "references/Scope.html#project-client-and-definition-of-objective",
    "href": "references/Scope.html#project-client-and-definition-of-objective",
    "title": "Project Scope",
    "section": "Project Client and Definition of Objective",
    "text": "Project Client and Definition of Objective\n\nBusiness Client\nThe client for this project is a marketing team responsible for managing marketing campaigns related to the Google merchandise store.\n\n\nProject Goal\nThis project exists to help the marketing team (client) interact with first-time visitors to the merchandise store, with the hopes of increasing the likelihood that these visitors will make a purchase (convert) or repeat-purchase during a future visit to the store. If this can be done, then it will help the team address the business’ objectives of growing new and repeat customers as mentioned above.\nThe objective of this project is to increase the number of first-time visitors to the merchandise store who are converted into new or repeat customers."
  },
  {
    "objectID": "references/Scope.html#actions-that-need-to-be-taken",
    "href": "references/Scope.html#actions-that-need-to-be-taken",
    "title": "Project Scope",
    "section": "Actions that Need to be Taken",
    "text": "Actions that Need to be Taken\nThis project will facilitate develpoment of a proactive and targeted marketing strategy (eg. promotions) to grow new and repeat customers."
  },
  {
    "objectID": "references/Scope.html#analysis",
    "href": "references/Scope.html#analysis",
    "title": "Project Scope",
    "section": "Analysis",
    "text": "Analysis\n\nType of Analysis\nWe need to answer the important question: Which visitors should we prioritize through proactive marketing promotions. In orther words, we want to identify the visitors with a low, medium and high likelihood of making a purchase during a future (or return) visit to the store.\nSince we want to intervene before a visitor’s next visit to the store, we would predict the likelihood of every first-time visitor making a purchase during a subsequent visit. These predictions will used to create audience groups based on the likelihood of making a future purchase and prioritize and focus the marketing strategy per group.\nThe analysis to be performed here is a prediction task. We need to predict the likelihood (or propensity) of a purchase during a future visit.\n\n\nFormat of Data\nThe analysis will be performed using machine learning (ML). A ML model will be trained using attributes (features) of the visitors’ first visit and it will predict visitors’ propensity to make a purchase on a return visit to the store. The best-performing trained ML model will be the one that can make this prediction with the highest accuracy or some other evaluation metric (this will be discussed later in the Evaluation Metric sub-section below). This application of ML is called propensity modeling. The outcome to be predicted is binary (there are only two possible outcomes)\n\nthe visitor will make a purchase on a return visit\nthe visitor will not make a purchase on a return visit\n\nIn a ML context, this is a supervised learning problem. Attributes about the first visit made by visitors to the store site are retrieved from Google Analytics (GA) tracking data accumulated for visitors to the merchandise store. GA tracking code has been embeded in the store’s website in order to anonymously track visitor interactions on the site. These attributes, or characteristics, of visitors’ first visit are the independent variables or features in ML.\nFor the same visitors, the outcome (or label in ML) of return visits (whether a purchase on a return visit was made or not) is retrieved to determine if a purchase on a future visit was made by this visitor. This label is a forward-looking label since it references events from the future. By comparison, the features are from the past (historical data) since they reference attributes of the visitor’s first visit to the store. Both features and label refer to the same visitor.\n\n\nAnalysis Workflow Overview\nWith such a dataset of Google Analytics tracking data available for all visitors to the store between August 1, 2016 and August 1, 2017, a ML model will be trained to predict whether first-time visitors will make a purchase during a future (return) visit. The trained model then predicts probabilities (which are interpreted as likelihoods or propensities) for new visitors to make a purchase on a return (or future) visit to the store. These new visitors were not part of the model’s training data. The predicted probabilities are then used to generate marketing audience groups (low, medium and high propensity) and test (or treatment) and control cohorts within each group as described above.\nIn summary the steps of such a workflow are\n\ntrain ML model using historical data for first visit of visitors\n\nthis is the training data\n\nuse trained model to predict probabilities for first visit of visitors that are not part of the training data\n\nthis is the unseen data\n\nuse predicted probabilities to assign audience cohorts (test or control) to all visitors in the unseen data\nbuild a brief profile of the visitors in the test cohort in unseen data\n\nwhen building a marketing strategy, we are not allowed to look at the control cohort and so the profile will be required for the test cohort only\n\nprovide audience test cohorts and their associated profile summaries to the marketing team\n\n\n\nTimeframes for Study\n\nIn order to avoid data leakage (or lookahead bias), the data splits are created in chronological order\n\ntraining data\n\nSeptember 1, 2016 to December 31, 2016\n\nvalidation data\n\nJanuary 1 - 31, 2017\n\ntest data\n\nFebruary 1 - 28, 2017\n\nunseen data\n\nMarch 1 - 31, 2017\n\n\nWe will assume that\n\nthe current date is March 1, 2017\nML model development can be performed between March 1 - 31, 2017\n\nThe marketing team is interested in growing new and repeat customers from visitors who made their first visit to the store during March 1 - 31, 2017 (unseen data). With this in mind, they want to\n\nbuild their campaign around these visitors\nlaunch their campaign on April 10, 2017\n\nThere are two constraints facing the client (marketing team)\n\nthe first visit data covering this period (unseen data) is only available on March 31, 2017 and this is close to the proposed campaign start date of April 10, 2017\ndesigning a typical marketing campaign takes 1 - 12 weeks\ncampaign launch windows occur every month\n\ncampaigns can be launched on April 10, 2017, May 10, 2017, June 10, 2017, etc.\n\n\nWith this in mind, the marketing team wants to start designing their campaign today (March 1, 2017). They do not want to wait until March 31, 2017 to receive recommended audience cohorts from the data science team and begin their campaign design. So, instead of waiting until March 31, 2017, the marketing team will start their campaign design using the audience cohorts recommended by the data science team using the test data split, which covers February 1 - 28, 2017.\nOn March 31, 2017, the marketing team will receive the audience cohorts from the data science team for the visits who made their first visit to the store during March 1 - 31, 2017. Between April 1, 2017 and April 9, 2017, the marketing team will start making adjustments to the campaign strategy by using the audience cohorts recommended by the data science team using the unseen data period (covering March 1 - 31, 2017). This will allow the marketing team to meet the proposed campaign start date of April 10, 2017.\nIf the datascience team is unable to generate a sufficiently accurate ML model to meet the April 10, 2017 campaign launch date then they will need to improve their analysis in order to try to meet the next available launch date (May 10, 2017).\n\n\n\nNotes\n\nRegarding ML labels (y)\n\nas mentioned earlier, these are forward-looking labels\n\nthe ML features (X) are attributes of a visitor’s first visit to the store\nthe ML labels (y) are the outcome (whether a purchase occurred or not) of that same visitor’s future visits to the store\n\na purchase is allowed to occur during any future visit to the store, not just the next visit\n\n\nin the period covering train, validation and test data splits, if a visitor has\n\nmade at least one purchase of a product during their return visit, then the label is set to True (or 1)\nnot made at least one purchase of a product during their return visit, then the label is set to False (or 0)\n\n\nThe data science team’s recommended audience cohorts (test and control) of visitors will be accepted by the marketing team if the ML model’s performance during evaluation (using the test data split) is better than that of a random model."
  },
  {
    "objectID": "references/Scope.html#how-do-actions-follow-from-the-analysis",
    "href": "references/Scope.html#how-do-actions-follow-from-the-analysis",
    "title": "Project Scope",
    "section": "How do Actions Follow From the Analysis",
    "text": "How do Actions Follow From the Analysis\nBased on visitors’ predicted likelihood of making a purchase on a future visit, marketing audience test and control groups (cohorts) will be created. Each group will contain a visitor ID as well as all the attributes of the visitor’s first visit that were used to predict the likelihood of a purchase during a return visit to the store.\nThese groups can be used by the marketing team to\n\ndesign appropriate strategies that can be implemented during activation\nestimate campaign costs"
  },
  {
    "objectID": "references/Scope.html#validation",
    "href": "references/Scope.html#validation",
    "title": "Project Scope",
    "section": "Validation",
    "text": "Validation\n\nDuring Development\nDevelopment covers September 1, 2016 - February 28, 2017.\nSince the current date is assumed to be March 1, 2017 and the training, validation and test data splits end no later than February 28, 2017, ML model predictions during validation (using validation split) and evaluation (using test split) can be scored before March 31, 2017.\nScoring is performed using evalaution metrics discussed in the ML Evaluation Metric sub-section below.\n\n\nDuring Production\nProduction covers March 1 - 31, 2017.\nThe model’s predictions will be scored against the outcome (whether the visitor makes a purchase on their return/future visit to the store) at the end of the marketing campaign.\n\n\nDifferences between Development and Production\n\nDuring production, the predictions are used to inform a marketing audience cohorts (test and control). By definition, the marketing strategy will be applied to the test cohort. It will not be applied to the control group. With this in mind, during the production period (March 1 - 31, 2017), we can only evaluate the predictions of the trained ML model that are associated with first-time visitors to the store during this period if those visitors are placed in the control cohort.\nAs mentioned earlier, the marketing team will only accept the data science team’s recommended audiente cohorts if the ML model outperforms a random model. At the same time, the data science team should also be checking for drift between attributes (features) of the first visit using the test split (development) and using the unseen data (production). If drift in features is observed outside a pre-defined threshold, then the data science team will need to repeat\n\nML model training using more training data (earlier start date than September 1, 2016)\nevaluation using the test split\nevaluation using the unseen data\n\nuntil drift is no longer observed. Feature drift checking will need to be done on April 1, 2017, before marketing audience visitor cohorts are given to the marketing team.\n\n\n\nWorkflow in Production\n\nThe trained model will make predictions of probability (propensity or likelihood) for all first-time visitors to the store during March 1 - 31, 2017. Predictions are used to identify marketing audience test (or treatment) and control cohorts.\nA marketing strategy is applied to all first-time visitors in the test group\nLength of marketing promotion campaign is to be determined by marketing team\nAt the end of the marketing campaign\n\nwe will know which visitors who were predicted to make a purchase on a return visit did actually make a purchase\nwe can evaluate the predictions made by the trained ML model on first visits that occurred during March 1 - 31, 2017\nwe can calculate a suitable KPI for this project\n\nKPI = number of purchases made by visitors in the test cohort - number of purchases made by visitors in the control cohort\n\nif this KPI is larger than zero, then we have successfully grown our customer base, which was the objective of the task that the business has given to the marketing team\n\nadditional KPIs can also be considered\n\n\n\nWe mentioned that scoring predictions of first visits that occurred during the unseen data period (production) of March 1 - 31, 2017 cannot be performed until the end of the marketing campaign. This was also mentioned in the during production sub-section above. It is worth emphasizing that until the end of the marketing campaign, we are unable to evalute the ML model’s predictions of data during the unseen data period (production). For this reason, it is improtant to check for drift in ML features between the unseen data (March 1 - 31, 2017) and test data (February 1 - 28, 2017) before the predictions are made and the audience cohorts are generated.\nGenerally, marketing campaigns run for approximately three months but this depends on numerous factors including\n\nmessage\ncall to action (CTA)\nfunds available (marketing budget)\nexpectations (desired uplift, etc.)\n\nThe duration, design and structure of the campaign will be determined by the marketing team starting on March 1, 2017 (today) and it will be finalized between April 1 - 9, 2019. On April 9, 2017, if it is determined that it is not feasible to design a campaign based on the audience cohorts recommended by the data science team (eg. cohorts are too large, etc.) then\n\nthe next available campaign launch window (May 10, 2017) will have to be targeted\nthe new unseen data (production) period will cover April 1 - 30, 2017\nthe datascience team will have to improve ML model performance between April 10 - 30, 2017\n\n\n\nML Evaluation Metric\nFalse negatives (tweets that should have been responded to but were predicted to not need a response) and false positives (tweets that did not need review by a team member but were predicted as requiring a review) are the most important types of errors. So the candidate metrics to be used to assess ML model performance are\n\nF1-score (if false negatives and false positives are equally important)\nF2-score (if false negatives are more important)\nF0.5-score (if false positives are more important)\n\nFor the current predicton task, there are two possible outcomes indicate whether a visitor did or did not make a purchase on a return visit to the store and these are\n\nactual\n\nis the true outcome\nthis is known after a visitor’s first visit to the store\nthis indicates that action that the marketing team should have taken\n\npredicted\n\nis the predicted outcome\nthis is predicted after the visitor’s first visit to the store\nthis indicates that action that the marketing team was predicted to have taken\n\n\nThe four possible ML prediction scenarios are listed below for the prediction of the outcome [whether a first-time visitor will, or will not, make a purchase on a return (future) visit to the merchandise store]\n\nTP: actual = makes purchase on return visit, predicted = makes purchase on return visit\n\npredicted marketing strategy matches what should be the actual marketing strategy\n\nTN: actual = does not make purchase on return visit, predicted = does not make purchase on return visit\n\npredicted marketing strategy matches what should be the actual marketing strategy\n\nFN: actual = makes purchase on return visit, predicted = does not make purchase on return visit\n\npredicted marketing strategy\n\npredicted to offer minimal promotion\n\nactual marketing strategy\n\nactually should have offered a stronger promotion\n\neg. more loyalty points, more frequent free/shipping, etc.\n\n\nthese errors in prediction lead to missed opportunities to correctly target first-time visitors since the predicted promotion offered is an underestimate of the true promotion that the team should have offered to these visitors\nthese errors lead to underspending on promotions to first-time visitors who are likely to benefit from them\n\nFP: actual = does not make purchase on return visit, predicted = makes purchase on return visit\n\npredicted marketing strategy\n\npredicted to offer a stronger promotion\n\nactual marketing strategy\n\nactually should have offered minimal promotion\n\nthese errors lead to overspending on promotions to first-time visitors who are not likely to benefit from them\nthis is the most expensive type of prediction error\nthis scenario must be avoided\n\n\nSince FP (false positives) are more costly than FN (false negatives), the scoring metric chosen to evaluate predictions made using the ML model is F0.5-score."
  },
  {
    "objectID": "references/Scope.html#data",
    "href": "references/Scope.html#data",
    "title": "Project Scope",
    "section": "Data",
    "text": "Data\nAn important factor that is driving propensity modeling in marketing is the need to do more with first-party customer data. This is data that comes directly from the customer and not from third-party sources. For marketing use-cases, effective propensity models use customer attributes from online and offline first-party data sources, including site analytics (online) and CRM (offline) data.\nHere, we have access to online data only Google Analytics tracking data (see the dataset and its documentation). This will be used to build a ML model to predict visitors’ propensity to make a purchase during a future visit to the store.\nVisit data for the merchandise store is available for the period of August 1, 2016 to August 1, 2017. This data provides information such as\n\nvisitor ID\nvisit date\nvist datetime\nactions performed during visit\n\nadd to cart\nremove from cart\nmake purchase\nview product details\netc.\n\ntotal time spent viewing pages during each visit\netc."
  },
  {
    "objectID": "references/Scope.html#analysis-notebooks",
    "href": "references/Scope.html#analysis-notebooks",
    "title": "Project Scope",
    "section": "Analysis Notebooks",
    "text": "Analysis Notebooks\n\nGet data and EDA Part 1\n\nconnect to raw visit data generated by Google Analytics tracking embedded in the merchandise store’s site\n\ndata is stored as Google BigQuery public dataset\nuse Python client to connect to dataset\nget overview of the columns in the raw visit data\n\nunderstand underlying patterns and stats about the visit-level data\nEDA part 1/2\n01_get_data.ipynb\n\nEDA Part 2\n\nEDA part 2/2\n02_eda.ipynb\n\nTransform data\n\n03_transform.ipynb\nextract the first visit per visitor (features, or X) and align with whether they made a purchase on a return (future) visit to the store (labels, or y)\n\nBaseline model development\n\ndevelop a baseline model to predict probability of purchase during future visit\n\nthis will be fast to train and will demonstrate the end-to-end project workflow, but will likely be over-simplified and so will underperform relative to a ML-based approach\n\n04_development.ipynb\n\nML model development\n\nrepeat baseline model development, but use a ML model instead\n05_dev_v2.ipynb"
  },
  {
    "objectID": "references/Scope.html#limitations",
    "href": "references/Scope.html#limitations",
    "title": "Project Scope",
    "section": "Limitations",
    "text": "Limitations\n\nBusiness Use Case\nThe analysis implemented here is only possible if Google Analytics tracking is embedded into an e-commerce website. Guides for embedding GA tracking code are documented below\n\nchartio blog post\nGoogle Support documentation\n\nFor the current use-case, this was already done for the Google Merchandise store’s website and so valuable tracking data could be collected and used. However, if such a solution is to be adopted for other digital marketplaces, then the Google Analytics tracking code must be embedded into those websites.\n\n\nData\n\nThe analytics dataset used in this project is based on a version of Google Analytics (GA360) that is deprecated as of July 1 2023 or 2024.\n\n\n\nOthers\nFor other limitations, please see the Limitations section in each notebook."
  },
  {
    "objectID": "references/Scope.html#assumptions",
    "href": "references/Scope.html#assumptions",
    "title": "Project Scope",
    "section": "Assumptions",
    "text": "Assumptions\n\nBusiness Use Case\n\nFor visitors who made a purchase on a return visit, we will include those who could have bought on their first as well. These are repeat customers, who we have assumed are one of the two types of visitors that we want to grow. For this reason, we will include their visits in the data.\nThe marketing team has does not have a preliminary idea as to the size of the audience groups (low, medium, high likelihood or propensity to make a purchase on a return visit) or cohorts and the strategy they will deploy as part of a campaign. As such, they have not yet designed any approaches to address this problem.\nDeployment-related assumptions (see point 4. in Timeframes for Study)\n\nwe have assumed that the current date is March 1, 2017\nwe have assumed that a trained ML model will make predictions for all first-time visitors to the store between March 1 - 31, 2017\n\n\nFor other assumptions, please see the Assumptions section in each notebook."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/02_get_best_model.html",
    "href": "notebooks/04-transform/notebooks/02_get_best_model.html",
    "title": "Register Best Model in MLFlow Model Registry",
    "section": "",
    "text": "This step retrieves the best ML model across all the MLFlow experiment runs that were tracked during ML development. This best model is then registered in the MLFlow Model Registry."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/02_get_best_model.html#about",
    "href": "notebooks/04-transform/notebooks/02_get_best_model.html#about",
    "title": "Register Best Model in MLFlow Model Registry",
    "section": "",
    "text": "This step retrieves the best ML model across all the MLFlow experiment runs that were tracked during ML development. This best model is then registered in the MLFlow Model Registry."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/02_get_best_model.html#user-inputs",
    "href": "notebooks/04-transform/notebooks/02_get_best_model.html#user-inputs",
    "title": "Register Best Model in MLFlow Model Registry",
    "section": "User Inputs",
    "text": "User Inputs\nDefine the primary ML scoring metric\n\nprimary_metric = \"fbeta2\""
  },
  {
    "objectID": "notebooks/04-transform/notebooks/02_get_best_model.html#manage-ml-experiments",
    "href": "notebooks/04-transform/notebooks/02_get_best_model.html#manage-ml-experiments",
    "title": "Register Best Model in MLFlow Model Registry",
    "section": "Manage ML Experiments",
    "text": "Manage ML Experiments\n\nInspect Experiment Run Outputs\nGet all runs of all experiments\n\ndf_expt_runs = modh.get_all_experiment_runs()\nwith pd.option_context(\"display.max_columns\", None):\n    display(df_expt_runs.drop(columns=['params', 'column_names']))\n\n\n\n\n\n\n\n\nresampling_approach\nclf\nparam_clf__a\nparam_clf__b\nparam_preprocessor__cat__rarecats__fe__ignore_format\nparam_preprocessor__cat__rarecats__fe__n_categories\nparam_preprocessor__cat__rarecats__fe__replace_with\nparam_preprocessor__cat__rarecats__fe__tol\nparam_resampler__sampling_strategy\nparam_select__threshold\ntest_accuracy\ntest_balanced_accuracy\ntest_precision\ntest_recall\ntest_roc_auc\ntest_f1\ntest_fbeta05\ntest_fbeta2\ntest_pr_auc\ntest_avg_precision\nfit_time\nscore_time\nexperiment_run_type\ntrain_val_accuracy\ntrain_val_balanced_accuracy\ntrain_val_precision\ntrain_val_recall\ntrain_val_roc_auc\ntrain_val_f1\ntrain_val_fbeta05\ntrain_val_fbeta2\ntrain_val_pr_auc\ntrain_val_avg_precision\ntrain_start_date\ntest_end_date\nnum_observations\nnum_columns\nexperiment_id\nrun_id\n\n\n\n\n0\nos\nBetaDistClassifier\n0.2\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.937527\n0.498671\n0.498509\n0.498671\n0.498671\n0.498536\n0.498505\n0.498605\n0.036529\n0.034013\n0.818807\n0.125538\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n1\nos\nBetaDistClassifier\n0.2\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.93611\n0.50061\n0.500647\n0.50061\n0.50061\n0.500612\n0.500629\n0.500607\n0.033432\n0.034135\n0.830849\n0.12732\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n2\nos\nBetaDistClassifier\n0.2\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.939274\n0.503584\n0.504179\n0.503584\n0.503584\n0.503755\n0.503975\n0.503626\n0.035172\n0.034386\n0.867677\n0.133546\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n3\nos\nBetaDistClassifier\n0.2\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.939699\n0.497791\n0.49731\n0.497791\n0.497791\n0.49741\n0.497311\n0.49761\n0.034562\n0.03397\n0.866694\n0.124131\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n4\nos\nBetaDistClassifier\n0.2\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.94291\n0.502793\n0.503763\n0.502793\n0.502793\n0.50285\n0.503268\n0.502736\n0.036729\n0.034317\n0.846498\n0.123729\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n5\nos\nBetaDistClassifier\n0.3\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.91996\n0.498931\n0.499247\n0.498931\n0.498931\n0.498406\n0.498788\n0.498506\n0.035258\n0.034026\n0.822364\n0.125427\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n6\nos\nBetaDistClassifier\n0.3\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.917599\n0.500381\n0.500255\n0.500381\n0.500381\n0.499342\n0.499721\n0.499646\n0.032524\n0.034119\n0.83201\n0.126385\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n7\nos\nBetaDistClassifier\n0.3\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.921471\n0.499713\n0.499792\n0.499713\n0.499713\n0.499173\n0.499437\n0.499319\n0.035789\n0.034075\n0.843319\n0.129222\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n8\nos\nBetaDistClassifier\n0.3\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.923124\n0.492552\n0.494275\n0.492552\n0.492552\n0.493149\n0.493773\n0.492713\n0.035887\n0.03376\n0.831795\n0.127664\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n9\nos\nBetaDistClassifier\n0.3\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.927468\n0.498809\n0.499004\n0.498809\n0.498809\n0.498747\n0.498868\n0.49874\n0.034053\n0.03402\n0.836382\n0.127909\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n10\nos\nBetaDistClassifier\n0.4\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.903527\n0.487084\n0.493013\n0.487084\n0.487084\n0.488426\n0.490939\n0.487011\n0.032204\n0.033569\n0.914054\n0.147198\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n11\nos\nBetaDistClassifier\n0.4\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.901214\n0.499916\n0.499957\n0.499916\n0.499916\n0.496766\n0.498245\n0.497401\n0.036118\n0.034088\n0.990844\n0.164699\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n12\nos\nBetaDistClassifier\n0.4\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.907069\n0.496266\n0.497905\n0.496266\n0.496266\n0.495118\n0.496496\n0.495079\n0.036368\n0.033876\n1.030078\n0.153129\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n13\nos\nBetaDistClassifier\n0.4\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.910186\n0.500552\n0.500323\n0.500552\n0.500552\n0.49855\n0.499323\n0.499071\n0.034426\n0.034131\n1.002369\n0.170874\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n14\nos\nBetaDistClassifier\n0.4\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.913113\n0.491378\n0.494578\n0.491378\n0.491378\n0.492028\n0.493401\n0.491315\n0.034109\n0.033696\n1.070083\n0.199056\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n15\nos\nBetaDistClassifier\n0.35\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.914105\n0.503249\n0.502022\n0.503249\n0.503249\n0.501085\n0.501396\n0.501849\n0.034786\n0.034332\n1.228456\n0.14818\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n16\nos\nBetaDistClassifier\n0.35\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.910091\n0.495826\n0.497536\n0.495826\n0.495826\n0.495127\n0.496331\n0.494992\n0.033179\n0.033856\n1.084647\n0.189495\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n17\nos\nBetaDistClassifier\n0.35\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.915474\n0.501285\n0.500824\n0.501285\n0.501285\n0.499793\n0.500202\n0.500263\n0.034925\n0.034182\n0.995956\n0.135104\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n18\nos\nBetaDistClassifier\n0.35\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.917363\n0.500259\n0.500172\n0.500259\n0.500259\n0.49922\n0.499621\n0.499517\n0.033667\n0.034111\n0.906044\n0.143231\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n19\nos\nBetaDistClassifier\n0.35\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.92081\n0.496698\n0.497617\n0.496698\n0.496698\n0.496627\n0.497124\n0.496506\n0.036853\n0.033905\n0.913986\n0.142901\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n20\nos\nBetaDistClassifier\n0.15\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.944751\n0.501743\n0.502559\n0.501743\n0.501743\n0.501511\n0.501933\n0.501538\n0.036488\n0.034225\n0.913431\n0.135588\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n21\nos\nBetaDistClassifier\n0.15\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.943099\n0.497547\n0.496569\n0.497547\n0.497547\n0.496703\n0.496512\n0.497144\n0.033841\n0.033964\n0.902203\n0.132549\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n22\nos\nBetaDistClassifier\n0.15\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.946215\n0.503837\n0.505958\n0.503837\n0.503837\n0.50395\n0.504836\n0.503724\n0.034105\n0.034433\n0.913117\n0.132311\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n23\nos\nBetaDistClassifier\n0.15\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.947112\n0.502297\n0.503768\n0.502297\n0.502297\n0.501978\n0.502683\n0.502002\n0.032433\n0.034278\n0.87933\n0.131583\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n24\nos\nBetaDistClassifier\n0.15\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.948671\n0.500431\n0.500779\n0.500431\n0.500431\n0.499377\n0.499759\n0.49983\n0.031457\n0.034123\n0.87516\n0.12981\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n25\nos\nBetaDistClassifier\n0.45\n2.31\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.893469\n0.492567\n0.496543\n0.492567\n0.492567\n0.490987\n0.493861\n0.490442\n0.034396\n0.033696\n0.871497\n0.139538\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n26\nos\nBetaDistClassifier\n0.45\n2.25\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.889928\n0.493406\n0.49707\n0.493406\n0.493406\n0.490981\n0.494107\n0.4906\n0.035242\n0.033728\n0.892268\n0.130184\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n27\nos\nBetaDistClassifier\n0.45\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.896916\n0.496355\n0.498238\n0.496355\n0.496355\n0.493825\n0.496015\n0.493923\n0.033175\n0.033877\n0.877123\n0.141782\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n28\nos\nBetaDistClassifier\n0.45\n2.4\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.900836\n0.500388\n0.500197\n0.500388\n0.500388\n0.49701\n0.498474\n0.497732\n0.0338\n0.034119\n0.88059\n0.129084\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n29\nos\nBetaDistClassifier\n0.45\n2.5\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.907163\n0.499655\n0.499808\n0.499655\n0.499655\n0.497494\n0.498551\n0.497958\n0.033712\n0.034071\n0.853995\n0.127923\nnested\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n30\nos\nBetaDistClassifier\n0.15\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n0.95611\n0.497739\n0.018349\n0.497739\n0.497739\n0.497656\n0.497615\n0.497703\n0.021443\n0.02298\n&lt;NA&gt;\n&lt;NA&gt;\nparent\n0.936559\n0.501876\n0.050861\n0.501876\n0.501876\n0.500605\n0.501582\n0.501072\n0.044134\n0.043901\n20160901\n20170228\n113728\n28\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n\n\n\n\n\nCPU times: user 320 ms, sys: 19.8 ms, total: 340 ms\nWall time: 335 ms\n\n\n\n\nGet Outputs of Best Experiment Run\n\ndf_best_expt_run = modh.get_best_experiment_run(\n    df_expt_runs, \"experiment_run_type == 'parent'\", f\"test_{primary_metric}\"\n)\nwith pd.option_context(\"display.max_columns\", None):\n    display(df_best_expt_run.to_frame().transpose())\n\n\n\n\n\n\n\n\nresampling_approach\nclf\nparam_clf__a\nparam_clf__b\nparam_preprocessor__cat__rarecats__fe__ignore_format\nparam_preprocessor__cat__rarecats__fe__n_categories\nparam_preprocessor__cat__rarecats__fe__replace_with\nparam_preprocessor__cat__rarecats__fe__tol\nparam_resampler__sampling_strategy\nparam_select__threshold\nparams\ntest_accuracy\ntest_balanced_accuracy\ntest_precision\ntest_recall\ntest_roc_auc\ntest_f1\ntest_fbeta05\ntest_fbeta2\ntest_pr_auc\ntest_avg_precision\nfit_time\nscore_time\nexperiment_run_type\ntrain_val_accuracy\ntrain_val_balanced_accuracy\ntrain_val_precision\ntrain_val_recall\ntrain_val_roc_auc\ntrain_val_f1\ntrain_val_fbeta05\ntrain_val_fbeta2\ntrain_val_pr_auc\ntrain_val_avg_precision\ntrain_start_date\ntest_end_date\nnum_observations\nnum_columns\ncolumn_names\nexperiment_id\nrun_id\n\n\n\n\n30\nos\nBetaDistClassifier\n0.15\n2.35\nTrue\n1\nother\n0.1\n0.1\n0.7\n{\"clf__a\": 0.15, \"clf__b\": 2.35, \"preprocessor...\n0.95611\n0.497739\n0.018349\n0.497739\n0.497739\n0.497656\n0.497615\n0.497703\n0.021443\n0.02298\n&lt;NA&gt;\n&lt;NA&gt;\nparent\n0.936559\n0.501876\n0.050861\n0.501876\n0.501876\n0.500605\n0.501582\n0.501072\n0.044134\n0.043901\n20160901\n20170228\n113728\n28\n[\"fullvisitorid\", \"visitId\", \"visitNumber\", \"v...\n1\neee3d281e0cb48cd8dba9f93a6de4347\n\n\n\n\n\n\n\nCPU times: user 19.3 ms, sys: 0 ns, total: 19.3 ms\nWall time: 18.9 ms\n\n\n\n\nGet Parameters Associated With Best Experiment Run\nGet the metadata and metrics for all available data and experiment run ID for best performing run\n\nfeatures\n\nlist of column names\n\nmetrics\n\nprimary metric score on the test split, during ML evaluation\n\nrun ID\n\nMLFlow experiment run ID\n\n\n\ncols_best_expt_run = json.loads(df_best_expt_run[\"column_names\"])\nbest_model_eval_score = df_best_expt_run[f\"test_{primary_metric}\"]\nbest_run_id = df_best_expt_run[\"run_id\"]\n\nprint(best_model_eval_score)\ncols_best_expt_run\n\n0.49770334\n\n\n['fullvisitorid',\n 'visitId',\n 'visitNumber',\n 'visitStartTime',\n 'quarter',\n 'month',\n 'day_of_month',\n 'day_of_week',\n 'hour',\n 'minute',\n 'second',\n 'source',\n 'medium',\n 'channelGrouping',\n 'hits',\n 'bounces',\n 'last_action',\n 'promos_displayed',\n 'promos_clicked',\n 'product_views',\n 'product_clicks',\n 'pageviews',\n 'time_on_site',\n 'browser',\n 'os',\n 'deviceCategory',\n 'added_to_cart',\n 'made_purchase_on_future_visit']\n\n\n\n\nGet Name of Logged Model Associated with Best Experiment Run\nGet name of model associated with best run\n\ndf_best_run_model = modh.get_single_registered_model(f\"run_id == '{best_run_id}'\")\nbest_run_model_name = df_best_run_model.squeeze()[\"name\"]\nwith pd.option_context(\"display.max_colwidth\", None):\n    display(df_best_run_model)\nprint(best_run_model_name)\n\n\n\n\n\n\n\n\nname\nrun_id\ndescription\nsource\nversion\nstatus\n\n\n\n\n0\nBetaDistClassifier_20160901_20170228_133892_feats__20230519_234249\neee3d281e0cb48cd8dba9f93a6de4347\nBest BetaDistClassifier model with fbeta2 score of 0.4977033311\n/home/jovyan/notebooks/mlruns/eee3d281e0cb48cd8dba9f93a6de4347/artifacts/model\n1\nREADY\n\n\n\n\n\n\n\nBetaDistClassifier_20160901_20170228_133892_feats__20230519_234249\nCPU times: user 13.5 ms, sys: 5.84 ms, total: 19.3 ms\nWall time: 19.1 ms\n\n\n\n\nAdd MLFlow Model Associated with Best Experiment Run to Model Registry\nCreate a new registered model, with a version\n\nclient = MlflowClient(tracking_uri=mlflow.get_tracking_uri())\nresult = client.create_model_version(\n    name=best_run_model_name,\n    await_creation_for=None,\n    tags={'deployment-candidate': \"yes\"},\n    description=(\n        f'Best Model based on {primary_metric} score of \"\n        f\"{best_model_eval_score:.10f}'\n    ),\n    source=f\"mlruns/{best_run_id}/artifacts/model\",\n    run_id=best_run_id,\n)\n\nCPU times: user 4.86 ms, sys: 5.03 ms, total: 9.89 ms\nWall time: 13.3 ms"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/02_get_best_model.html#next-step",
    "href": "notebooks/04-transform/notebooks/02_get_best_model.html#next-step",
    "title": "Register Best Model in MLFlow Model Registry",
    "section": "Next Step",
    "text": "Next Step\nThe registered model will be used during inference to make predictions for first-time visitors to the store during the production period (following the end of the test data split)."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/03_design_experiment.html",
    "href": "notebooks/04-transform/notebooks/03_design_experiment.html",
    "title": "Post-Processing",
    "section": "",
    "text": "As discussed for the overall use-case, a marketing strategy is to be developed based on first-time visitors’ propenisty to make a purchase during a future (return) visit to the merchandise store website. The aim of the strategy is to convert these first time visitors into customers (if they did not make a purchase during their first visit), or repeat customers (if they did make a purchase during their first visit). This covers steps 2. and 3. from a typical A/B Testing workflow.\n\n\n\nTwo marketing strategies to consider for first-time visitors to the store during the inference period are\n\nwhen conducting the marketing campaign, use all first-time visitors, grouped by their predicted propensity to make a purchase during a future visit, where the strategy is modified based on the group\nwhen conducting the marketing campaign, use the group of first-time visitors with the highest predicted propensity to make a purchase during a future visit, where a single strategy can be used for all visitors in the group\n\nEach group will be referred to as a marketing audience group. For the multi-group strategy (strategy 1), the groups of visitors are created based on predicted propensities. If three such groups are preferred, then the group with the visitors who are predicted to have the highest propensity to make a purchase on a return visit would be named as the High propensity group. Similarly, the other two groups would be named the Medium and Low propensity groups. The size of each group would be as equal as possible. For 15,000 first-time visitors, each group would consist of 5,000 visitors. Similarly, if 10 such groups are preferred then each group would consist of 1,500 visitors. The groups are created by binning the predicted propensities using their quantiles.\nFor the single-group strategy (strategy 2), a single group is required. Again, the quantiles are used to create the groups (or bins or segments or buckets) of visitors based on their predicted propensity. However, only the visitors in the top group will be chosen for the marketing campaign. For the case where 10 groups (quantiles) are preferred, then only the top group is chosen.\n\n\n\nRegardless of the number of groups used in this strategy, a rank is assigned to each group where the lowest rank corresponds to the group with the highest predicted propensities to make a purchase on a future (return) visit. For the case where 10 groups (quantiles) are preferred, then the top group would be the one consisting of visitors with a propensity higher than 90% of all first-time visitors to the store during the inference data period. Similarly, the bottom group would be the one consisting of visitors with a propensity higher than 10% of all such visitors. If three groups are preferred, then the top group captures visitors with a predicted propensity higher than 66.667% of all first-time visitors, while the visitors in the bottom group are predicted to have a propensity to make a purchase on a return visit that is higher than 33.333% of all visitors.\n\n\n\nVisitors placed in each group will then be randomly placed into test and control cohorts to run an A/B test for quantifying the impact of the marketing campaign. So, the sample size required for the test and control cohort must also be determined for both these strategies. The KPI to be maximized is the future conversion rate since we want visitors to become customers during a future (return) visit to the store. The required sample sizes will be estimated for a combinations of conversion rate, uplift, power and confidence level (1, 2)."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/03_design_experiment.html#about",
    "href": "notebooks/04-transform/notebooks/03_design_experiment.html#about",
    "title": "Post-Processing",
    "section": "",
    "text": "As discussed for the overall use-case, a marketing strategy is to be developed based on first-time visitors’ propenisty to make a purchase during a future (return) visit to the merchandise store website. The aim of the strategy is to convert these first time visitors into customers (if they did not make a purchase during their first visit), or repeat customers (if they did make a purchase during their first visit). This covers steps 2. and 3. from a typical A/B Testing workflow.\n\n\n\nTwo marketing strategies to consider for first-time visitors to the store during the inference period are\n\nwhen conducting the marketing campaign, use all first-time visitors, grouped by their predicted propensity to make a purchase during a future visit, where the strategy is modified based on the group\nwhen conducting the marketing campaign, use the group of first-time visitors with the highest predicted propensity to make a purchase during a future visit, where a single strategy can be used for all visitors in the group\n\nEach group will be referred to as a marketing audience group. For the multi-group strategy (strategy 1), the groups of visitors are created based on predicted propensities. If three such groups are preferred, then the group with the visitors who are predicted to have the highest propensity to make a purchase on a return visit would be named as the High propensity group. Similarly, the other two groups would be named the Medium and Low propensity groups. The size of each group would be as equal as possible. For 15,000 first-time visitors, each group would consist of 5,000 visitors. Similarly, if 10 such groups are preferred then each group would consist of 1,500 visitors. The groups are created by binning the predicted propensities using their quantiles.\nFor the single-group strategy (strategy 2), a single group is required. Again, the quantiles are used to create the groups (or bins or segments or buckets) of visitors based on their predicted propensity. However, only the visitors in the top group will be chosen for the marketing campaign. For the case where 10 groups (quantiles) are preferred, then only the top group is chosen.\n\n\n\nRegardless of the number of groups used in this strategy, a rank is assigned to each group where the lowest rank corresponds to the group with the highest predicted propensities to make a purchase on a future (return) visit. For the case where 10 groups (quantiles) are preferred, then the top group would be the one consisting of visitors with a propensity higher than 90% of all first-time visitors to the store during the inference data period. Similarly, the bottom group would be the one consisting of visitors with a propensity higher than 10% of all such visitors. If three groups are preferred, then the top group captures visitors with a predicted propensity higher than 66.667% of all first-time visitors, while the visitors in the bottom group are predicted to have a propensity to make a purchase on a return visit that is higher than 33.333% of all visitors.\n\n\n\nVisitors placed in each group will then be randomly placed into test and control cohorts to run an A/B test for quantifying the impact of the marketing campaign. So, the sample size required for the test and control cohort must also be determined for both these strategies. The KPI to be maximized is the future conversion rate since we want visitors to become customers during a future (return) visit to the store. The required sample sizes will be estimated for a combinations of conversion rate, uplift, power and confidence level (1, 2)."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/03_design_experiment.html#implementation",
    "href": "notebooks/04-transform/notebooks/03_design_experiment.html#implementation",
    "title": "Post-Processing",
    "section": "Implementation",
    "text": "Implementation\nThis step will used the best ML model to estimate the size of the group, for both such strategies. The model will be used with the test data split (last month of data that was used during ML model development) to estimate the required size of marketing audience (test and control) cohorts for both strategies. In the next step, the same ML model will be used to predict the propensity for first-time visitors to the store during the inference period, assign these visitors to one or more audience groups based on the type of strategy (single group or multiple groups) and finally select random test and control cohorts from each group. The cohort (sample) sizes estimated in this step will be used to create the test and control cohorts in the next step."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/03_design_experiment.html#user-inputs",
    "href": "notebooks/04-transform/notebooks/03_design_experiment.html#user-inputs",
    "title": "Post-Processing",
    "section": "User Inputs",
    "text": "User Inputs\nDefine the following\n\nname of column containing label (outcome)\nprimary ML scoring metric\naudience_groups_strategy_n\n\ndesired audience groups into which the first-time visitors propensities will be placed\n\nnum_propens_groups specifies the number of desired audience propensity groups\npropens_group_labels specifies names of the desired audience propensity groups (low, medium and high, etc.)\n\n\ngrid of inputs for which sample sizes are required (uplift, power, confidence level)\n\n\n# 1. label column\nlabel = \"made_purchase_on_future_visit\"\n\n# 2. scoring metric\nprimary_metric = \"fbeta2\"\n\n# 3. mapping dictionaries\naudience_groups_strategy_1 = {\n    \"num_propens_groups\": 3,\n    \"propens_group_labels\": [\"High\", \"Medium\", \"Low\"],\n}\naudience_groups_strategy_2 = {\n    \"num_propens_groups\": 3,\n    \"propens_group_labels\": [\"High\", \"High-Medium\", \"High-Medium-Low\"],\n}\n\n# 4. grid of inputs for estimating sample sizes\nuplift_ranges = (10, 12, 15)\npower_ranges = (55, 57, 60, 63, 65, 66, 67, 68, 69, 70, 80, 90)\nci_level_ranges = (55, 57, 60, 63, 65, 66, 67, 68, 69, 70, 75, 85, 90, 95)\n\nCreate a mapping between audience group number (0, 1, 2) and name (high, medium, low), where\n\n0 is mapped to high\n1 is mapped to medium\n2 is mapped to low\n\nsince it is standard to assign a label the top percentile (highest propensity) with the smallest number (0)\n\nmapper_dict_audience_strategy_1 = dict(\n    zip(\n        range(audience_groups_strategy_1[\"num_propens_groups\"]),\n        audience_groups_strategy_1[\"propens_group_labels\"],\n    )\n)\nmapper_dict_audience_strategy_2 = dict(\n    zip(\n        range(audience_groups_strategy_1[\"num_propens_groups\"]),\n        audience_groups_strategy_2[\"propens_group_labels\"],\n    )\n)\nprint(mapper_dict_audience_strategy_1)\nprint(mapper_dict_audience_strategy_2)\n\n{0: 'High', 1: 'Medium', 2: 'Low'}\n{0: 'High', 1: 'High-Medium', 2: 'High-Medium-Low'}\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe group integer-to-name mapping dictionary contains keys that start at the lowest group number (0) for the High propensity audience group. For this audience strategy, a single group is used, so 0 is the only key in this dictionary."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/03_design_experiment.html#get-best-mlflow-model-from-model-registry",
    "href": "notebooks/04-transform/notebooks/03_design_experiment.html#get-best-mlflow-model-from-model-registry",
    "title": "Post-Processing",
    "section": "Get Best MLFlow Model from Model Registry",
    "text": "Get Best MLFlow Model from Model Registry\n\nFetch Latest Version of Best Deployment Candidate Model from Model Registry\nGet all MLFlow deployment candidate models from model registry\n\ndf_deployment_candidate_mlflow_models = modh.get_all_deployment_candidate_models()\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThese are registered models that have been assigned a tag (tags={deployment-candidate: 'yes'}), as opposed to models that are not deployment candidates and do not have any tag (tags={}).\n\n\n\nGet name of best deployment candidate model\n\nbest_run_model_name = modh.get_best_deployment_candidate_model(\n    df_deployment_candidate_mlflow_models\n)\n\n\n\nGet Data Used in Development of Best Deployment Candidate Model\nGet all available data used during model development of the best deployment candidate model\n\ndf_all = modh.get_data_for_run_id(df_deployment_candidate_mlflow_models, 'processed_data')\n\nCPU times: user 93.9 ms, sys: 46.3 ms, total: 140 ms\nWall time: 68.8 ms\n\n\nSeparate features and label\n\nX, y = [df_all.drop(columns=[label, 'split_type']), df_all[label]]\n\nCPU times: user 7.16 ms, sys: 7.13 ms, total: 14.3 ms\nWall time: 13.9 ms\n\n\nGet test split from the data used during development of the best deployment candidate model\n\ndf_prediction_best_run = (\n    df_all\n    .query(\"split_type == 'test'\")\n    .rename(columns={label: 'label'})\n    .drop(columns=['split_type'])\n)\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe fullvisitorid column is the ID of each visitor who\n\nmade a purchase on a return visit to the store\nmade their first visit to the store during the dates covered by the test data split\n\nThe score column is the predicted probability (using .pred_proba()), which is the propensity of a visitor to make a purchase on a return visit.\nThe predicted_score_label column is the predicted label using ML. A discrmination threshold of 0.5 is used to convert the predicted score (probability) into a label. The true ML label will not be known until a later date. As of the current date, only the predicted ML label can be known. See the project scope for details.\n\n\n\n\n\nLoad Best Deployment Candidate Model from Model Registry\n\nbest_model_uri = f\"models:/{best_run_model_name}/latest\"\nmodel = mlflow.sklearn.load_model(model_uri=best_model_uri)\nmodel"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/03_design_experiment.html#explore-best-model-performance",
    "href": "notebooks/04-transform/notebooks/03_design_experiment.html#explore-best-model-performance",
    "title": "Post-Processing",
    "section": "Explore Best Model Performance",
    "text": "Explore Best Model Performance\nMake predictions on same data that best model was trained on\n\ny_pred, y_pred_proba = modh.make_inference(model, X, y.name)\n\nGet evaluation metrics on same data that best model was trained on\n\ndf_metrics_all = mh.calculate_metrics(y, y_pred, y_pred_proba, None, None, None, [\"all\"])\n\n\nOptimal Discrimination Threshold Tuning\nShow a visualization of evaluation metrics relative to the discrimination threshold\n\nvh.plot_multi_line_threshold_chart(\n    df_thresholds_best_run.set_index(\"t\"),\n    ptitle_str=\"Threshold Plot\",\n    xlabel=\"Threshold (t)\",\n    ylabel=\"Score\",\n    title_fontsize=12,\n    axis_label_fontsize=12,\n    figsize=(8, 4),\n)\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nTo be done."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/03_design_experiment.html#get-size-of-marketing-audience-for-campaign",
    "href": "notebooks/04-transform/notebooks/03_design_experiment.html#get-size-of-marketing-audience-for-campaign",
    "title": "Post-Processing",
    "section": "Get Size of Marketing Audience For Campaign",
    "text": "Get Size of Marketing Audience For Campaign\n\nGet Marketing Audience Using All Propensity Groups (Strategy 1)\nPerform the following to get the sample size estimates for each audience propensity group, using this approach\n\nSort predictions in ascending order of the predicted probability (propensity) (score)\n\nwhen predictions are then assigned to groups (bins), this order of sorting means the lowest group number will be assigned to samples (predictions) with the lowest probability, so this ordering is non-standard\nthe group integer-to-name mapping dictionary contains keys that start at the lowest group number (0) for the High propensity audience group\n\nSeparate the predictions into group, using the predicted probability, based on the required number of groups\nGet conversion rates (KPI) per group\nget sample sizes per group, based on\n\nKPI per group\ndesired magnitude of the input (uplift, power, confidence level)\n\nreverse order of groups and sort by group number\nassign meaningful names to groups\nset datatypes for columns\n(optional) move column with meaningful names to second from front (for display purposes only)\n\n\ndf_sample_sizes_strategy_1 = (\n    df_prediction_best_run.copy()\n    .pipe(ash.sort_scores, True)\n    .pipe(ash.get_audience_groups_by_propensity, audience_groups_strategy_1[\"num_propens_groups\"])\n    .pipe(ash.get_kpi_per_audience_group)\n    .pipe(\n        ash.calculate_multi_group_sample_sizes,\n        uplift_ranges,\n        power_ranges,\n        ci_level_ranges,\n    )\n    .pipe(ash.invert_group_numbers, audience_groups_strategy_1[\"num_propens_groups\"])\n    .pipe(ash.map_audience_group_number_to_name, mapper_dict_audience_strategy_1)\n    .pipe(\n        ash.set_datatypes,\n        {\n            \"group_number\": pd.Int8Dtype(),\n            \"maudience\": pd.StringDtype(),\n            \"group_size\": pd.Int16Dtype(),\n            \"group_min_propensity\": pd.Float32Dtype(),\n            \"group_conv_rate\": pd.Float32Dtype(),\n            \"uplift\": pd.Int8Dtype(),\n            \"power\": pd.Int8Dtype(),\n            \"ci_level\": pd.Int8Dtype(),\n            \"required_sample_size\": pd.Int32Dtype(),\n        },\n    ).pipe(ash.move_cols_to_front, [\"group_number\", \"maudience\"])\n)\ndf_sample_sizes_strategy_1\n\nSet all specified datatypes.\nCPU times: user 5.7 s, sys: 0 ns, total: 5.7 s\nWall time: 5.69 s\n\n\n\n\n\n\n\n\n\ngroup_number\nmaudience\ngroup_size\ngroup_min_propensity\ngroup_conv_rate\nuplift\npower\nci_level\nrequired_sample_size\n\n\n\n\n0\n0\nHigh\n6721\n0.02255\n2.038387\n15\n90\n95\n22444\n\n\n1\n0\nHigh\n6721\n0.02255\n2.038387\n10\n90\n66\n23934\n\n\n2\n0\nHigh\n6721\n0.02255\n2.038387\n10\n90\n67\n24375\n\n\n3\n0\nHigh\n6721\n0.02255\n2.038387\n10\n90\n68\n24828\n\n\n4\n0\nHigh\n6721\n0.02255\n2.038387\n10\n90\n69\n25293\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1507\n2\nLow\n6722\n0.0\n2.558762\n15\n55\n67\n1849\n\n\n1508\n2\nLow\n6722\n0.0\n2.558762\n15\n55\n68\n1942\n\n\n1509\n2\nLow\n6722\n0.0\n2.558762\n15\n55\n69\n2038\n\n\n1510\n2\nLow\n6722\n0.0\n2.558762\n12\n90\n70\n14182\n\n\n1511\n2\nLow\n6722\n0.0\n2.558762\n10\n55\n55\n1853\n\n\n\n\n1512 rows × 9 columns\n\n\n\n\n\nGet Marketing Audience Using Top Propensity Group (Strategy 2)\nPerform the following to get the sample size estimates for the top audience propensity group, using this approach\n\nSort predictions in descending order of the predicted probability (propensity) (score)\nget group (bin) size that separates the predictions into groups (bins) of equal size, using the predicted probability, based on the required number of groups\nseparate the predictions into groups, using the predicted probability, based on the\n\nrequired number of groups\ncalculated group size from above step\n\nget conversion rates (KPI) per group\nget sample sizes per group, based on\n\nKPI per group\ndesired magnitude of the input (uplift, power, confidence level)\n\nsubtract one from group number\nassign meaningful names to groups\nset datatypes for columns\n(optional) move column with meaningful names to second from front (for display purposes only)\n\n\ndf_sample_sizes_strategy_2 = (\n    df_prediction_best_run.copy()\n    .pipe(ash.sort_scores, False)\n    .pipe(\n        ash.calculate_single_group_sample_sizes,\n        audience_groups_strategy_2[\"num_propens_groups\"],\n        ash.get_group_size(df_prediction_best_run, audience_groups_strategy_2[\"num_propens_groups\"]),\n        uplift_ranges,\n        power_ranges,\n        ci_level_ranges,\n    )\n    .pipe(ash.subtract_one_from_group_numbers, \"group_number\")\n    .pipe(\n        ash.map_audience_group_number_to_name,\n        mapper_dict_audience_strategy_2,\n        \"group_number\",\n    )\n    .pipe(\n        ash.set_datatypes,\n        {\n            \"group_number\": pd.Int8Dtype(),\n            \"maudience\": pd.StringDtype(),\n            \"group_size\": pd.Int16Dtype(),\n            \"group_size_proportion\": pd.Float32Dtype(),\n            \"group_min_propensity\": pd.Float32Dtype(),\n            \"group_conv_rate\": pd.Float32Dtype(),\n            \"uplift\": pd.Int8Dtype(),\n            \"power\": pd.Int8Dtype(),\n            \"ci_level\": pd.Int8Dtype(),\n            \"required_sample_size\": pd.Int32Dtype(),\n        },\n    )\n    .pipe(ash.move_cols_to_front, [\"group_number\", \"maudience\"])\n)\ndisplay(df_sample_sizes_strategy_2.dtypes.rename(\"dtype\").to_frame().transpose())\ndf_sample_sizes_strategy_2\n\nSet all specified datatypes.\nCPU times: user 5.97 s, sys: 0 ns, total: 5.97 s\nWall time: 5.97 s\n\n\n\n\n\n\n\n\n\ngroup_number\nmaudience\ngroup_size\ngroup_size_proportion\ngroup_min_propensity\ngroup_conv_rate\nuplift\npower\nci_level\nrequired_sample_size\n\n\n\n\ndtype\nInt8\nstring[python]\nInt16\nFloat32\nFloat32\nFloat32\nInt8\nInt8\nInt8\nInt32\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup_number\nmaudience\ngroup_size\ngroup_size_proportion\ngroup_min_propensity\ngroup_conv_rate\nuplift\npower\nci_level\nrequired_sample_size\n\n\n\n\n0\n0\nHigh\n6721\n33.33168\n0.02195\n2.038387\n10\n55\n55\n2338\n\n\n1\n0\nHigh\n6721\n33.33168\n0.02195\n2.038387\n10\n55\n57\n2802\n\n\n2\n0\nHigh\n6721\n33.33168\n0.02195\n2.038387\n10\n55\n60\n3508\n\n\n3\n0\nHigh\n6721\n33.33168\n0.02195\n2.038387\n10\n55\n63\n4233\n\n\n4\n0\nHigh\n6721\n33.33168\n0.02195\n2.038387\n10\n55\n65\n4732\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1507\n2\nHigh-Medium-Low\n20163\n99.995041\n0.0\n2.306204\n15\n90\n70\n10097\n\n\n1508\n2\nHigh-Medium-Low\n20163\n99.995041\n0.0\n2.306204\n15\n90\n75\n11126\n\n\n1509\n2\nHigh-Medium-Low\n20163\n99.995041\n0.0\n2.306204\n15\n90\n85\n13940\n\n\n1510\n2\nHigh-Medium-Low\n20163\n99.995041\n0.0\n2.306204\n15\n90\n90\n16124\n\n\n1511\n2\nHigh-Medium-Low\n20163\n99.995041\n0.0\n2.306204\n15\n90\n95\n19783\n\n\n\n\n1512 rows × 10 columns"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/03_design_experiment.html#export-audience-sample-sizes-to-disk",
    "href": "notebooks/04-transform/notebooks/03_design_experiment.html#export-audience-sample-sizes-to-disk",
    "title": "Post-Processing",
    "section": "Export Audience Sample Sizes to Disk",
    "text": "Export Audience Sample Sizes to Disk\nConcatenate the two DataFrames with the required sample sizes to acheive a KPI using different combinations of inputs (uplift, confidence level and power) and then export to disk\n\n(\n    df_combined_sample_sizes,\n    aud_sizes_fpath,\n) = ash.combine_and_export_sample_size_estimates(\n    df_sample_sizes_strategy_1,\n    df_sample_sizes_strategy_2,\n    df_deployment_candidate_mlflow_models,\n    processed_data_dir,\n)"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/03_design_experiment.html#ml-experiment-tracking",
    "href": "notebooks/04-transform/notebooks/03_design_experiment.html#ml-experiment-tracking",
    "title": "Post-Processing",
    "section": "ML Experiment Tracking",
    "text": "ML Experiment Tracking\nGet best run ID\n\nbest_run_id = df_deployment_candidate_mlflow_models.squeeze()[\"run_id\"]\n\nLog audience sample sizes file as MLFlow artifact\n\nwith mlflow.start_run(run_id=best_run_id) as run:\n    mlflow.log_artifact(aud_sizes_fpath)\nprint(\n    \"Logged required audience sizes as artifact in file \"\n    f\"{os.path.basename(aud_sizes_fpath)}\"\n)\n\nLogged required audience sizes as artifact in file audience_sample_sizes__run_eee3d281e0cb48cd8dba9f93a6de4347.parquet.gzip"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/03_design_experiment.html#next-step",
    "href": "notebooks/04-transform/notebooks/03_design_experiment.html#next-step",
    "title": "Post-Processing",
    "section": "Next Step",
    "text": "Next Step\nNext, test and control cohorts will be created using the inference data at the end of the production period (one month). The size of each cohort will be chosen based on the required sizes that were found in this step."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html",
    "href": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "",
    "text": "This step generates the randomized test and control cohorts needed to run a marketing campaign. The impact of the campaign on conversions (KPI) will be assessed using an A/B test at the end of the campaign. The randomized cohorts are needed in order to conduct this test. This is step 4. from a typical A/B Testing workflow.\nThis step can be run prospectively at the end of the inference period, just before the start of the campaign, when all the inference data (first-time visitors to the store) becomes available.\nFor the current use-case, the sizes of one or more audience groups were determined in the previous step. The audience strategy determines if one of more groups are used. For a single audience group, only the visitors predicted to have a high propensity to make a purchase on a return visit are selected to participate in the campaign and so randomized cohorts are to be drawn from this single group. For multiple audience groups, namely visitors predicted to have a low, medium or high propensity, randomized cohorts are to be drawn from each such group.\nThis step first assigns all first-time visitors to the store during the inference period to an audience group based on the requied audience strategy (single or multiple audience groups). Next, within each audience group, visitors are randomly assigned to test or control cohorts. A brief profile of each audience group is then performed using attributes of the visitors’ first visit. The profile and the inference data with the audience group and cohort for each first-time visitor during the inference period can be used by the marketing team to design and implement the marketing campaign aimed at growing the customer base.\n\n\n\nA custom python module in src/cohorts.py has been developed to create the test and control audience cohorts based on required sample sizes that were estimated in the previous step and logged as a MLFlow artifact (file) for the best MLFlow deployment candidate model.\nThe procedure followed in this step is briefle outlined below\n\nthe MLFlow artifact (file) is loaded in order to access the required sample sizes for each\n\naudience strategy\ndesired combination of effect sizes (power, confidence level and uplift)\n\nthe file is filtered based on the\n\naudience strategy\ndesired combination of effect sizes (see wanted_effect_sizes from User Inputs)\n\ninference data (first-time visitors) is retrieved and the best ML model is used to make inference predictions of the propensity of these visitors to make a purchase on a return (future) visit to the store.\npropensities are used to assign first-time visitors to audience groups (or bins)\n\nif the desired audience strategy is for a single audience group, then visitors are placed into a single bin\nif the desired audience strategy is for multiple audience groups, then multiple bins are creaed\n\nfor each bin (audience group), visitors are randomly placed into test and control cohorts\nthe inference data with\n\npredicted propensity (probability)\naudience group\ncohort (test or control)\n\nis then logged as a MLFlow artifact for use by the marketing team to build and design the campaign."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#about",
    "href": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#about",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "",
    "text": "This step generates the randomized test and control cohorts needed to run a marketing campaign. The impact of the campaign on conversions (KPI) will be assessed using an A/B test at the end of the campaign. The randomized cohorts are needed in order to conduct this test. This is step 4. from a typical A/B Testing workflow.\nThis step can be run prospectively at the end of the inference period, just before the start of the campaign, when all the inference data (first-time visitors to the store) becomes available.\nFor the current use-case, the sizes of one or more audience groups were determined in the previous step. The audience strategy determines if one of more groups are used. For a single audience group, only the visitors predicted to have a high propensity to make a purchase on a return visit are selected to participate in the campaign and so randomized cohorts are to be drawn from this single group. For multiple audience groups, namely visitors predicted to have a low, medium or high propensity, randomized cohorts are to be drawn from each such group.\nThis step first assigns all first-time visitors to the store during the inference period to an audience group based on the requied audience strategy (single or multiple audience groups). Next, within each audience group, visitors are randomly assigned to test or control cohorts. A brief profile of each audience group is then performed using attributes of the visitors’ first visit. The profile and the inference data with the audience group and cohort for each first-time visitor during the inference period can be used by the marketing team to design and implement the marketing campaign aimed at growing the customer base.\n\n\n\nA custom python module in src/cohorts.py has been developed to create the test and control audience cohorts based on required sample sizes that were estimated in the previous step and logged as a MLFlow artifact (file) for the best MLFlow deployment candidate model.\nThe procedure followed in this step is briefle outlined below\n\nthe MLFlow artifact (file) is loaded in order to access the required sample sizes for each\n\naudience strategy\ndesired combination of effect sizes (power, confidence level and uplift)\n\nthe file is filtered based on the\n\naudience strategy\ndesired combination of effect sizes (see wanted_effect_sizes from User Inputs)\n\ninference data (first-time visitors) is retrieved and the best ML model is used to make inference predictions of the propensity of these visitors to make a purchase on a return (future) visit to the store.\npropensities are used to assign first-time visitors to audience groups (or bins)\n\nif the desired audience strategy is for a single audience group, then visitors are placed into a single bin\nif the desired audience strategy is for multiple audience groups, then multiple bins are creaed\n\nfor each bin (audience group), visitors are randomly placed into test and control cohorts\nthe inference data with\n\npredicted propensity (probability)\naudience group\ncohort (test or control)\n\nis then logged as a MLFlow artifact for use by the marketing team to build and design the campaign."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#user-inputs",
    "href": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#user-inputs",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "User Inputs",
    "text": "User Inputs\nDefine the following\n\nstart and end dates for inference data\nname of column containing label (outcome)\naudience_groups\n\ndesired audience groups into which the first-time visitors propensities will be placed\n\nnum_propens_groups specifies the number of desired audience propensity groups (low, medium and high)\npropens_group_labels specifies names of the desired audience propensity groups\n\n\ninputs (uplift, powerm confidence level) for which random cohort sizes are to be created\ntype of audience strategy (single- or multi- group) from which to create cohorts\n\n\n# 1. start and end dates\ninfer_start_date = \"20170301\"\ninfer_end_date = \"20170331\"\n\n# 2. label column\nlabel = \"made_purchase_on_future_visit\"\n\n# 3. mapping dictionaries\naudience_groups_strategy_1 = {\n    \"num_propens_groups\": 3,\n    \"propens_group_labels\": [\"High\", \"Medium\", \"Low\"],\n}\naudience_groups_strategy_2 = {\n    \"num_propens_groups\": 3,\n    \"propens_group_labels\": [\"High\", \"High-Medium\", \"High-Medium-Low\"],\n}\n\n# 4. wanted inputs for estimating sample sizes\nwanted_inputs = {\n    \"uplift_percentage\": 10,\n    \"power_percentage\": 55,\n    \"confidence_level_percentage\": 55,\n}\n\n# 5. type of audience strategy to use when creating groups\naudience_strategy = 1\n\nCreate a mapping between audience group number (0, 1, 2) and group name\n\nmapper_dict_audience_strategy_1 = dict(\n    zip(\n        range(audience_groups_strategy_1[\"num_propens_groups\"]),\n        audience_groups_strategy_1[\"propens_group_labels\"],\n    )\n)\nmapper_dict_audience_strategy_2 = dict(\n    zip(\n        range(audience_groups_strategy_2[\"num_propens_groups\"]),\n        audience_groups_strategy_2[\"propens_group_labels\"],\n    )\n)\nprint(mapper_dict_audience_strategy_1)\nprint(mapper_dict_audience_strategy_2)\n\n{0: 'High', 1: 'Medium', 2: 'Low'}\n{0: 'High', 1: 'High-Medium', 2: 'High-Medium-Low'}\n\n\nGet desired effect size queries and number of audience groups\n\nif audience_strategy == 1:\n    query_inputs = (\n        f\"(uplift == {wanted_inputs['uplift_percentage']}) & \"\n        f\"(power == {wanted_inputs['power_percentage']}) & \"\n        f\"(ci_level == {wanted_inputs['confidence_level_percentage']})\"\n    )\nelse:\n    query_inputs = (\n        f\"(group_size_proportion &lt; 34) & \"\n        f\"(uplift == {wanted_inputs['uplift_percentage']}) & \"\n        f\"(power == {wanted_inputs['power_percentage']}) & \"\n        f\"(ci_level == {wanted_inputs['confidence_level_percentage']})\"\n    )\n\nnum_bins = (\n    audience_groups_strategy_1[\"num_propens_groups\"]\n    if audience_strategy == 1\n    else audience_groups_strategy_2[\"num_propens_groups\"]\n)"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#get-inference-data",
    "href": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#get-inference-data",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "Get Inference Data",
    "text": "Get Inference Data\n\nquery_infer = sqlh.get_sql_query_infer(infer_start_date, infer_end_date)\nX_infer, _ = th.extract_data(query_infer, gcp_auth_dict).pipe(\n    th.transform_data,\n    datatypes_dict={k:v for k,v in dtypes_dict.items() if k != label},\n    duplicate_cols=[\"fullvisitorid\"],\n    column_mapper_dict={'last_action': action_mapper},\n)\nX_infer = X_infer.pipe(th.shuffle_data)\n\nQuery execution start time = 2023-05-19 19:44:35.392...done at 2023-05-19 19:44:40.548 (5.156 seconds).\nQuery returned 21,768 rows\nGot 21,752 rows and 27 columns after dropping duplicates\nTransformed data has 21,752 rows & 27 columns\nCPU times: user 1 s, sys: 109 ms, total: 1.11 s\nWall time: 5.19 s\n\n\nGet the size of each audience cohort in the inference data\n\nbin_size_infer = len(X_infer) / num_bins\nbin_size_infer_control = int(bin_size_infer / 2)"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#get-model",
    "href": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#get-model",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "Get Model",
    "text": "Get Model\n\nFetch Latest Version of Best Deployment Candidate Model from Model Registry\nGet name of best deployment candidate model from model registry\n\ndf_deployment_candidate_mlflow_models = modh.get_all_deployment_candidate_models()\nbest_run_model_name = modh.get_best_deployment_candidate_model(\n    df_deployment_candidate_mlflow_models\n)\nwith pd.option_context(\"display.max_colwidth\", None):\n    display(df_deployment_candidate_mlflow_models)\n\n\n\n\n\n\n\n\nname\ndescription\nrun_id\ntags\nversion\nscore\n\n\n\n\n0\nBetaDistClassifier_20160901_20170228_133892_feats__20230519_234249\nBest Model based on fbeta2 score of 0.4977033436\neee3d281e0cb48cd8dba9f93a6de4347\n{'deployment-candidate': 'yes'}\n2\n0.497703\n\n\n\n\n\n\n\nCPU times: user 277 ms, sys: 28.2 ms, total: 305 ms\nWall time: 305 ms\n\n\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures(threshold=0.7)),\n                ('resampler',\n                 RandomOverSampler(random_state=88, sampling_strategy=0.1)),\n                ['clf', BetaDistClassifier(a=0.15, b=2.35, random_state=88)]])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('aboveavg',\n                                                                   AboveAveragePagePromoEngager(cols=['hits',\n                                                                                                      'promos_displayed',\n                                                                                                      'promos_clicked',\n                                                                                                      'product_views',\n                                                                                                      'product_clicks',\n                                                                                                      'pageviews',\n                                                                                                      'time_on_site'])),\n                                                                  ('scaler',\n                                                                   MinMaxScaler())]),\n                                                  ['hits', 'promos_displayed',\n                                                   'promos_clicked',\n                                                   'product_views',\n                                                   'product_clicks',\n                                                   'page...\n                                                                   OneHotEncoder(drop='first',\n                                                                                 dtype=&lt;class 'int'&gt;,\n                                                                                 handle_unknown='ignore'))]),\n                                                  ['bounces', 'last_action',\n                                                   'source', 'medium',\n                                                   'channelGrouping', 'browser',\n                                                   'os', 'deviceCategory'])])),\n                ('select', DropCorrelatedFeatures(threshold=0.7)),\n                ('resampler',\n                 RandomOverSampler(random_state=88, sampling_strategy=0.1)),\n                ['clf', BetaDistClassifier(a=0.15, b=2.35, random_state=88)]])preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('aboveavg',\n                                                  AboveAveragePagePromoEngager(cols=['hits',\n                                                                                     'promos_displayed',\n                                                                                     'promos_clicked',\n                                                                                     'product_views',\n                                                                                     'product_clicks',\n                                                                                     'pageviews',\n                                                                                     'time_on_site'])),\n                                                 ('scaler', MinMaxScaler())]),\n                                 ['hits', 'promos_displayed', 'promos_clicked',\n                                  'product_views', 'product_clicks',\n                                  'pageviews', 'time_on_site']),\n                                ('cat',\n                                 P...\n                                                                                                    n_categories=1,\n                                                                                                    replace_with='other',\n                                                                                                    tol=0.1,\n                                                                                                    variables=['source',\n                                                                                                               'browser']),\n                                                                                   ['bounces',\n                                                                                    'last_action',\n                                                                                    'source',\n                                                                                    'medium',\n                                                                                    'channelGrouping',\n                                                                                    'browser',\n                                                                                    'os',\n                                                                                    'deviceCategory'])])),\n                                                 ('dummy',\n                                                  OneHotEncoder(drop='first',\n                                                                dtype=&lt;class 'int'&gt;,\n                                                                handle_unknown='ignore'))]),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])num['hits', 'promos_displayed', 'promos_clicked', 'product_views', 'product_clicks', 'pageviews', 'time_on_site']AboveAveragePagePromoEngagerAboveAveragePagePromoEngager(cols=['hits', 'promos_displayed', 'promos_clicked',\n                                   'product_views', 'product_clicks',\n                                   'pageviews', 'time_on_site'])MinMaxScalerMinMaxScaler()cat['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']rarecats: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('fe',\n                                 RareLabelEncoder(ignore_format=True,\n                                                  n_categories=1,\n                                                  replace_with='other', tol=0.1,\n                                                  variables=['source',\n                                                             'browser']),\n                                 ['bounces', 'last_action', 'source', 'medium',\n                                  'channelGrouping', 'browser', 'os',\n                                  'deviceCategory'])])fe['bounces', 'last_action', 'source', 'medium', 'channelGrouping', 'browser', 'os', 'deviceCategory']RareLabelEncoderRareLabelEncoder(ignore_format=True, n_categories=1, replace_with='other',\n                 tol=0.1, variables=['source', 'browser'])remainderpassthroughpassthroughOneHotEncoderOneHotEncoder(drop='first', dtype=&lt;class 'int'&gt;, handle_unknown='ignore')DropCorrelatedFeaturesDropCorrelatedFeatures(threshold=0.7)RandomOverSamplerRandomOverSampler(random_state=88, sampling_strategy=0.1)BetaDistClassifierBetaDistClassifier(a=0.15, b=2.35, random_state=88)"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#make-inference-predictions",
    "href": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#make-inference-predictions",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "Make Inference Predictions",
    "text": "Make Inference Predictions\nMake inference predictions with best deployment candidate model\n\ny_infer_pred, y_infer_pred_proba = modh.make_inference(model, X_infer, label)\ndisplay(\n    y_infer_pred.value_counts(normalize=True).rename('proportion').to_frame().merge(\n        y_infer_pred.value_counts(normalize=False).rename('number').to_frame(),\n        left_index=True,\n        right_index=True,\n        how='left',\n    ).reset_index()\n)\n\n/opt/conda/envs/transform/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [6] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n/opt/conda/envs/transform/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [6] during transform. These unknown categories will be encoded as all zeros\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nmade_purchase_on_future_visit\nproportion\nnumber\n\n\n\n\n0\n0\n0.978439\n21283\n\n\n1\n1\n0.021561\n469\n\n\n\n\n\n\n\nCPU times: user 116 ms, sys: 12.1 ms, total: 128 ms\nWall time: 128 ms\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPer the business use-case, these are predictions of whether a visitor will make a purchase during a later (return) visit to the merchandise store. Such predictions are made using attributes (features) of the first visit by visitors to the store and they can only be evaluated at a later time (after the outcome of the same visitor’s later visit is known)."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#create-cohorts",
    "href": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#create-cohorts",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "Create Cohorts",
    "text": "Create Cohorts\nPerform the following to prepare the inference observations for extracting cohorts\n\ncombine inference features and predicted hard and soft labels into single DataFrame\nsort predictions in ascending order of the predicted probability (propensity) (score)\nseparate the predictions into bins, using the predicted probability, based on the required number of bins\nrename bin_number column (for use in downstream step)\nset datatypes for columns\n\n\ndf_infer_pred = (\n    ch.combine_infer_data(X_infer, y_infer_pred, y_infer_pred_proba)\n    .pipe(ash.sort_scores, False)\n    .pipe(ash.get_audience_groups_by_propensity, num_bins)\n    .pipe(ch.rename_columns, {\"group_number\": \"maudience\"})\n    .pipe(\n        ash.set_datatypes,\n        {\n            \"row_number\": pd.Int16Dtype(),\n            \"fullvisitorid\": pd.StringDtype(),\n            \"score\": pd.Float32Dtype(),\n            \"predicted_score_label\": pd.BooleanDtype(),\n            \"maudience\": pd.Int8Dtype(),\n        },\n    )\n)\nwith pd.option_context(\"display.max_columns\", 1000):\n    display(df_infer_pred.dtypes.rename(\"dtype\").to_frame().T)\n    display(df_infer_pred)\n\nSet all specified datatypes.\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\n...\npageviews\ntime_on_site\nbrowser\nos\ndeviceCategory\nadded_to_cart\nscore\npredicted_score_label\nrow_number\nmaudience\n\n\n\n\ndtype\nstring[python]\nstring[python]\nInt8\ndatetime64[ns]\nInt8\nInt8\nInt8\nInt8\nInt8\nInt8\n...\nInt16\nInt16\ncategory\ncategory\ncategory\nInt16\nFloat32\nboolean\nInt16\nInt8\n\n\n\n\n1 rows × 31 columns\n\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\nsecond\nsource\nmedium\nchannelGrouping\nhits\nbounces\nlast_action\npromos_displayed\npromos_clicked\nproduct_views\nproduct_clicks\npageviews\ntime_on_site\nbrowser\nos\ndeviceCategory\nadded_to_cart\nscore\npredicted_score_label\nrow_number\nmaudience\n\n\n\n\n12968\n2750061365734892376\n1490726705\n1\n2017-03-28 11:45:05\n1\n3\n28\n3\n11\n45\n5\ngoogle\norganic\nOrganic Search\n1\n1\nUnknown\n0\n0\n12\n0\n1\n0\nChrome\nWindows\ndesktop\n0\n0.93977\nFalse\n0\n0\n\n\n5429\n5725820476426446335\n1490547900\n1\n2017-03-26 10:05:00\n1\n3\n26\n1\n10\n5\n0\ngoogle\norganic\nOrganic Search\n4\n0\nUnknown\n0\n0\n24\n0\n4\n1491\nSafari\niOS\nmobile\n0\n0.934336\nTrue\n1\n0\n\n\n18567\n9832530502455357095\n1488923809\n1\n2017-03-07 13:56:49\n1\n3\n7\n3\n13\n56\n49\nkeep.google.com\nreferral\nReferral\n1\n1\nUnknown\n0\n0\n10\n0\n1\n0\nChrome\nWindows\ndesktop\n0\n0.929517\nFalse\n2\n0\n\n\n4808\n0146316056086331538\n1490891406\n1\n2017-03-30 09:30:06\n1\n3\n30\n5\n9\n30\n6\nanalytics.google.com\nreferral\nReferral\n3\n0\nUnknown\n0\n0\n12\n0\n3\n46\nFirefox\nWindows\ndesktop\n0\n0.0\nFalse\n21749\n2\n\n\n5803\n9128678154595193098\n1490638312\n1\n2017-03-27 11:11:52\n1\n3\n27\n2\n11\n11\n52\nmall.googleplex.com\nreferral\nReferral\n5\n0\nUnknown\n9\n0\n47\n0\n5\n219\nChrome\nMacintosh\ndesktop\n0\n0.0\nFalse\n21750\n2\n\n\n11156\n6907576544396353102\n1489941679\n1\n2017-03-19 09:41:19\n1\n3\n19\n1\n9\n41\n19\ngoogle\norganic\nOrganic Search\n4\n0\nUnknown\n9\n0\n27\n0\n4\n82\nSafari\niOS\nmobile\n0\n0.0\nFalse\n21751\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThis is the same data preparation that was used during the (previous) sample size estimation step step in order to prepare the test data split for estimating the required sample size.\n\n\n\nPerform the following to get the test and control cohorts from the inference data\n\nload estimates for required sample sizes associated with best deployment candidate model\nget required sample sizes for the chosen audience strategy\nget required sample sizes that can be supported by size of inference data\nget required sample sizes that support all required audience groups (bins)\nget required sample sizes that capture required effect sizes\nget random test and control cohorts\n\n\ndf_infer_audience_groups = (\n    ch.load_file_from_mlflow_artifact(\n        df_deployment_candidate_mlflow_models, \"audience_sample_sizes\"\n    )\n    .pipe(ch.get_sample_sizes_by_strategy, audience_strategy)\n    .pipe(ch.get_suitable_sample_sizes, bin_size_infer_control)\n    .pipe(ch.get_sample_sizes_with_all_audience_groups, num_bins)\n    .pipe(ch.get_required_inputs, query_inputs)\n    .pipe(\n        ch.create_cohorts,\n        df_infer_pred,\n        mapper_dict_audience_strategy_1\n        if audience_strategy == 1\n        else mapper_dict_audience_strategy_2,\n        audience_strategy,\n    )\n    .pipe(\n        ash.set_datatypes,\n        {\n            \"maudience\": pd.StringDtype(),\n            \"cohort\": pd.StringDtype(),\n            \"audience_strategy\": pd.Int8Dtype(),\n        },\n    )\n)\nwith pd.option_context(\"display.max_columns\", 1000):\n    display(df_infer_audience_groups)\n\naudience=0: High, size=7,251,  excluded=2,575, wanted=2,338, control=2,338, test=2,338\naudience=1: Medium, size=7,250,  excluded=3,156, wanted=2,047, control=2,047, test=2,047\naudience=2: Low, size=7,251,  excluded=3,545, wanted=1,853, control=1,853, test=1,853\nFound suitable sample sizes and generated cohorts.\nSet all specified datatypes.\n\n\n\n\n\n\n\n\n\nfullvisitorid\nvisitId\nvisitNumber\nvisitStartTime\nquarter\nmonth\nday_of_month\nday_of_week\nhour\nminute\n...\ntime_on_site\nbrowser\nos\ndeviceCategory\nadded_to_cart\nscore\npredicted_score_label\nmaudience\ncohort\naudience_strategy\n\n\n\n\n0\n2750061365734892376\n1490726705\n1\n2017-03-28 11:45:05\n1\n3\n28\n3\n11\n45\n...\n0\nChrome\nWindows\ndesktop\n0\n0.93977\nFalse\nHigh\nControl\n1\n\n\n1\n5743383239793607743\n1490248543\n1\n2017-03-22 22:55:43\n1\n3\n22\n4\n22\n55\n...\n41\nChrome\nMacintosh\ndesktop\n0\n0.868146\nFalse\nHigh\nControl\n1\n\n\n2\n3103623410019510094\n1489274258\n1\n2017-03-11 15:17:38\n1\n3\n11\n7\n15\n17\n...\n0\nChrome\nWindows\ndesktop\n0\n0.867407\nFalse\nHigh\nControl\n1\n\n\n3\n1346722877763370807\n1489769082\n1\n2017-03-17 09:44:42\n1\n3\n17\n6\n9\n44\n...\n0\nChrome\nWindows\ndesktop\n0\n0.864194\nFalse\nHigh\nControl\n1\n\n\n4\n555475706674174642\n1488554437\n1\n2017-03-03 07:20:37\n1\n3\n3\n6\n7\n20\n...\n0\nChrome\nChrome OS\ndesktop\n0\n0.860542\nFalse\nHigh\nControl\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n21747\n3338493143074772484\n1489969663\n1\n2017-03-19 17:27:43\n1\n3\n19\n1\n17\n27\n...\n260\nFirefox\nWindows\ndesktop\n0\n0.0\nFalse\nLow\n&lt;NA&gt;\n1\n\n\n21748\n4708572783660861313\n1489607438\n1\n2017-03-15 12:50:38\n1\n3\n15\n4\n12\n50\n...\n748\nChrome\nWindows\ndesktop\n0\n0.0\nFalse\nLow\n&lt;NA&gt;\n1\n\n\n21749\n1248256241052574965\n1488761975\n1\n2017-03-05 16:59:35\n1\n3\n5\n1\n16\n59\n...\n22\nChrome\niOS\nmobile\n0\n0.0\nFalse\nLow\n&lt;NA&gt;\n1\n\n\n21750\n9128678154595193098\n1490638312\n1\n2017-03-27 11:11:52\n1\n3\n27\n2\n11\n11\n...\n219\nChrome\nMacintosh\ndesktop\n0\n0.0\nFalse\nLow\n&lt;NA&gt;\n1\n\n\n21751\n6907576544396353102\n1489941679\n1\n2017-03-19 09:41:19\n1\n3\n19\n1\n9\n41\n...\n82\nSafari\niOS\nmobile\n0\n0.0\nFalse\nLow\n&lt;NA&gt;\n1\n\n\n\n\n21752 rows × 32 columns\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe final sample size used to generate the cohorts is the least number of samples (first-time visitors) to be included in each of the control and test cohorts of each audience group. These sizes come from the output of the previous step (designing a media or marketing experiment - 1, 2).\n\n\n\nThe treatment (or test) and control groups should be similar to each other for the property to be tested. This is a fundamental requirement of test and control groups. In this case, the probability (score column) is the property of interest. With this in mind, we now show selected descriptive statistics, for the score column, for both the test (treatment) and control cohorts within each desired audience group (low, medium and high propensity)\n\ndf_aud_stats = ch.get_cohort_stats(df_infer_audience_groups)\ndf_aud_stats\n\n\n\n\n\n\n\n\nmaudience\ncohort\nscore_count\nscore_min\nscore_mean\nscore_median\nscore_max\n\n\n\n\n0\nHigh\nControl\n2338\n0.022184\n0.174242\n0.107771\n0.93977\n\n\n1\nHigh\nTest\n2338\n0.022178\n0.178281\n0.111958\n0.934336\n\n\n2\nHigh\n&lt;NA&gt;\n2575\n0.022201\n0.177941\n0.109885\n0.929517\n\n\n3\nLow\nControl\n1853\n0.0\n0.000031\n0.000003\n0.000219\n\n\n4\nLow\nTest\n1853\n0.0\n0.00003\n0.000002\n0.000224\n\n\n5\nLow\n&lt;NA&gt;\n3545\n0.0\n0.00003\n0.000002\n0.000225\n\n\n6\nMedium\nControl\n2047\n0.000227\n0.005895\n0.003517\n0.022168\n\n\n7\nMedium\nTest\n2047\n0.000225\n0.00569\n0.003184\n0.022128\n\n\n8\nMedium\n&lt;NA&gt;\n3156\n0.000225\n0.005708\n0.00327\n0.022167\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nIt is reassuring that there is agreement in the statistics for the probabilities per Test-Control cohort within the same audience group.\n\n\n\nFinally, the audience groups in the inference data are now briefly profiled, in order to identify characteristics that might help the marketing team build an appropriate strategy to be implemented during the campaign. For each audience group, the profile consists of the following\n\nproportion of visitors who displayed the following behavior\n\nwhose last action during thier first visit was\n\nlast_action == 'Add product(s) to cart'\nlast_action == 'Product detail_views'\nlast_action == 'Click through of product lists'\n\nwho\n\nused a medium == 'referral' to get to the store’s website on their first visit\nadded one item to their shopping cart (added_to_cart &gt; 0) on their first visit\n\nwhose\n\nfirst visit occurred on a weekend (day_of_week.isin(@weekend_days))\n\n\nbounce rate during all first visits\nfollowing descriptive statistics\n\nhour, day_of_week\n\nstatistic: mean\n\nsource, medium, channelGrouping, last_action, browser, os, deviceCategory\n\nstatistic: mode (most common value)\n\npromos_displayed, promos_clicked, product_views, product_clicks, pageviews, added_to_cart\n\nstatistics: mean, max\n\n\n\n\ndf_profile = pa.get_audience_profile(df_infer_audience_groups)\nwith pd.option_context(\"display.max_columns\", 1000):\n    display(df_profile)\n\nSet all specified datatypes.\nSet all specified datatypes.\n\n\n\n\n\n\n\n\nmaudience\nstat\nstat_type\nHigh\nLow\nMedium\naudience\n\n\n\n\n0\nhour__mean\nmean\n12.973382981657702\n12.981657702385878\n13.070344827586206\nhour\n\n\n1\nday_of_week__mean\nmean\n4.0103434009102195\n4.006481864570404\n3.9950344827586206\nday_of_week\n\n\n2\nsource__mode\nmode\ngoogle\ngoogle\ngoogle\nsource\n\n\n3\nmedium__mode\nmode\norganic\norganic\norganic\nmedium\n\n\n4\nchannelGrouping__mode\nmode\nOrganic Search\nOrganic Search\nOrganic Search\nchannelGrouping\n\n\n5\nlast_action__mode\nmode\nUnknown\nUnknown\nUnknown\nlast_action\n\n\n6\nbrowser__mode\nmode\nChrome\nChrome\nChrome\nbrowser\n\n\n7\nos__mode\nmode\nMacintosh\nMacintosh\nMacintosh\nos\n\n\n8\ndeviceCategory__mode\nmode\ndesktop\ndesktop\ndesktop\ndeviceCategory\n\n\n9\nhits__mean\nmean\n6.293338849813819\n6.338298165770238\n6.433931034482758\nhits\n\n\n10\nhits__max\nmax\n500\n179\n220\nhits\n\n\n11\npromos_displayed__mean\nmean\n8.410426148117502\n8.654944145635085\n8.636275862068965\npromos_displayed\n\n\n12\npromos_displayed__max\nmax\n189\n189\n171\npromos_displayed\n\n\n13\npromos_clicked__mean\nmean\n0.0\n0.00013791201213625708\n0.0\npromos_clicked\n\n\n14\npromos_clicked__max\nmax\n0\n1\n0\npromos_clicked\n\n\n15\nproduct_views__mean\nmean\n22.585988139566957\n23.573851882498964\n23.403310344827585\nproduct_views\n\n\n16\nproduct_views__max\nmax\n987\n670\n573\nproduct_views\n\n\n17\nproduct_clicks__mean\nmean\n0.6760446834919321\n0.6615639222176252\n0.6926896551724138\nproduct_clicks\n\n\n18\nproduct_clicks__max\nmax\n79\n48\n46\nproduct_clicks\n\n\n19\npageviews__mean\nmean\n5.322714108398841\n5.39167011446697\n5.468275862068966\npageviews\n\n\n20\npageviews__max\nmax\n466\n132\n176\npageviews\n\n\n21\nadded_to_cart__mean\nmean\n0.19873120948834644\n0.1933526410150324\n0.19806896551724137\nadded_to_cart\n\n\n22\nadded_to_cart__max\nmax\n25\n22\n22\nadded_to_cart\n\n\n23\nlast_action_added_to_cart\nbehavior\n5.254447662391394\n4.8131292235553715\n5.241379310344827\n&lt;NA&gt;\n\n\n24\nviewed_product_detail\nbehavior\n0.0\n0.0\n0.0\n&lt;NA&gt;\n\n\n25\nclicked_through_product_lists\nbehavior\n0.05516480485450283\n0.09653840849537995\n0.013793103448275862\n&lt;NA&gt;\n\n\n26\nused_referral\nbehavior\n24.67245897117639\n24.369052544476624\n24.02758620689655\n&lt;NA&gt;\n\n\n27\nadded_gt_1_to_cart\nbehavior\n3.751206730106192\n3.9029099434560752\n4.248275862068965\n&lt;NA&gt;\n\n\n28\nweekend_visitors\nbehavior\n18.259550406840436\n17.900979175286167\n17.889655172413793\n&lt;NA&gt;\n\n\n29\nbounce_rate\nbehavior\n33.333333333333336\n33.2092125224107\n33.51724137931034\n&lt;NA&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe audience profile consists of different types of statistics about the first visit, for each audience group.\nThe stat column shows the attribute from the first visit in the inference data for which a statistic is calculated.\nThe stat_type column indicates the type of statistic.\nThe High, Medium and Low columns (or just the High column, if the single-group audience strategy is being used) show the statistic of each attribute within each audience group."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#export-to-disk-and-ml-experiment-tracking",
    "href": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#export-to-disk-and-ml-experiment-tracking",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "Export to Disk and ML Experiment Tracking",
    "text": "Export to Disk and ML Experiment Tracking\nGet the best MLFlow run ID\n\nbest_run_id = df_deployment_candidate_mlflow_models.squeeze()[\"run_id\"]\n\n\nAudience Cohorts\nShow summary of DataFrame with audience cohorts\n\nut.summarize_df(df_infer_audience_groups)\n\n\n\n\n\n\n\n\ncolumn\ndtype\nmissing\n\n\n\n\n0\nfullvisitorid\nstring[python]\n0\n\n\n1\nvisitId\nstring[python]\n0\n\n\n2\nvisitNumber\nInt8\n0\n\n\n3\nvisitStartTime\ndatetime64[ns]\n0\n\n\n4\nquarter\nInt8\n0\n\n\n5\nmonth\nInt8\n0\n\n\n6\nday_of_month\nInt8\n0\n\n\n7\nday_of_week\nInt8\n0\n\n\n8\nhour\nInt8\n0\n\n\n9\nminute\nInt8\n0\n\n\n10\nsecond\nInt8\n0\n\n\n11\nsource\ncategory\n0\n\n\n12\nmedium\ncategory\n0\n\n\n13\nchannelGrouping\ncategory\n0\n\n\n14\nhits\nInt16\n0\n\n\n15\nbounces\ncategory\n0\n\n\n16\nlast_action\ncategory\n0\n\n\n17\npromos_displayed\nInt16\n0\n\n\n18\npromos_clicked\nInt16\n0\n\n\n19\nproduct_views\nInt16\n0\n\n\n20\nproduct_clicks\nInt16\n0\n\n\n21\npageviews\nInt16\n0\n\n\n22\ntime_on_site\nInt16\n0\n\n\n23\nbrowser\ncategory\n0\n\n\n24\nos\ncategory\n0\n\n\n25\ndeviceCategory\ncategory\n0\n\n\n26\nadded_to_cart\nInt16\n0\n\n\n27\nscore\nFloat32\n0\n\n\n28\npredicted_score_label\nboolean\n0\n\n\n29\nmaudience\nstring[python]\n0\n\n\n30\ncohort\nstring[python]\n9276\n\n\n31\naudience_strategy\nInt8\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe cohort column has missing values for visitors who were not assigned to either the test or control groups. This is expected.\n\n\n\nExport to disk and log exported file as MLFlow artifact\n\nut.export_and_track(\n    os.path.join(\n        processed_data_dir,\n        f\"audience_cohorts__run_\"\n        f\"{best_run_id}__\"\n        f\"infer_month_{month_name[1:][X_infer['month'].iloc[-1] - 1]}__\"\n        f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet.gzip\",\n    ),\n    df_infer_audience_groups,\n    (\n        \"inference audience cohorts for \"\n        f\"{month_name[1:][X_infer['month'].iloc[-1] - 1]}\"\n    ),\n    best_run_id,\n)\n\nExported inference audience cohorts for March to file audience_cohorts__run_eee3d281e0cb48cd8dba9f93a6de4347__infer_month_March__20230519_234456.parquet.gzip\nLogged inference audience cohorts for March as artifact in file audience_cohorts__run_eee3d281e0cb48cd8dba9f93a6de4347__infer_month_March__20230519_234456.parquet.gzip\n\n\n\n\nAudience Profiles\nShow summary DataFrame with audience profiles\n\nut.summarize_df(df_profile)\n\n\n\n\n\n\n\n\nmaudience\ndtype\nmissing\n\n\n\n\n0\nstat\nstring[python]\n0\n\n\n1\nstat_type\nstring[python]\n0\n\n\n2\nHigh\nstring[python]\n0\n\n\n3\nLow\nstring[python]\n0\n\n\n4\nMedium\nstring[python]\n0\n\n\n5\naudience\nstring[python]\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe behavioral attributes in the profile are not specific to an individual column. So, the column in the profile DataFrame has missing values for these attributes.\n\n\n\nExport to disk and log exported file as MLFlow artifact\n\nut.export_and_track(\n    os.path.join(\n        processed_data_dir,\n        f\"audience_profiles__run_\"\n        f\"{best_run_id}__\"\n        f\"infer_month_{month_name[1:][X_infer['month'].iloc[-1] - 1]}__\"\n        f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet.gzip\",\n    ),\n    df_profile,\n    (\n        \"inference audience profiles for \"\n        f\"{month_name[1:][X_infer['month'].iloc[-1] - 1]}\"\n    ),\n    best_run_id,\n)\n\nExported inference audience profiles for March to file audience_profiles__run_eee3d281e0cb48cd8dba9f93a6de4347__infer_month_March__20230519_234459.parquet.gzip\nLogged inference audience profiles for March as artifact in file audience_profiles__run_eee3d281e0cb48cd8dba9f93a6de4347__infer_month_March__20230519_234459.parquet.gzip"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#next-step",
    "href": "notebooks/04-transform/notebooks/04_get_audience_cohorts.html#next-step",
    "title": "Get Randomized Audience Cohorts From Inference",
    "section": "Next Step",
    "text": "Next Step\nThe next step will assess the performance of the marketing campaign, after the campaign has completed."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/05_analyze_campaign_outputs.html",
    "href": "notebooks/04-transform/notebooks/05_analyze_campaign_outputs.html",
    "title": "Run Campaign and Analyze Results",
    "section": "",
    "text": "This step assess the impact of running the marketing campaign. This step can be performed retrospectively, at the end of the campaign, when the outcome of the return visit of the first-time visitors to the merchandise store (during the inference period) is known. This is step 5. from a typical A/B Testing workflow.\nFor the current use-case, if the marketing campaign results in more conversions in the control cohort compared to the test cohort, then this could suggest that the campaign has grown the customer base and thereby met the objective of this project. However, a test of statistical significance will be needed in order to ensure that this impact seen by running the campaign (growth in conversions) was not a random occurrence.\nThis step compares the proportions (conversions) taken from two independent samples (test and control cohorts). The purpose is to determine if the number of conversions (KPI) of the test cohort is statistically different from that of the control cohort. If the\n\nconversion rate is higher in the test cohort\ndifference in conversion rate between the test and control cohort is statistically significant at some level of confidence (eg. 95%)\n\nthen it is possible to say with 95% confidence that the campaign has grown the customer base.\n\n\n\nIn python, such a comparison is implemented using the statsmodels library in the proportions_chisquare() method where the count parameter represents the number of convertions in each cohort (test or control) and nobs represents the overall size of the same cohort."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/05_analyze_campaign_outputs.html#about",
    "href": "notebooks/04-transform/notebooks/05_analyze_campaign_outputs.html#about",
    "title": "Run Campaign and Analyze Results",
    "section": "",
    "text": "This step assess the impact of running the marketing campaign. This step can be performed retrospectively, at the end of the campaign, when the outcome of the return visit of the first-time visitors to the merchandise store (during the inference period) is known. This is step 5. from a typical A/B Testing workflow.\nFor the current use-case, if the marketing campaign results in more conversions in the control cohort compared to the test cohort, then this could suggest that the campaign has grown the customer base and thereby met the objective of this project. However, a test of statistical significance will be needed in order to ensure that this impact seen by running the campaign (growth in conversions) was not a random occurrence.\nThis step compares the proportions (conversions) taken from two independent samples (test and control cohorts). The purpose is to determine if the number of conversions (KPI) of the test cohort is statistically different from that of the control cohort. If the\n\nconversion rate is higher in the test cohort\ndifference in conversion rate between the test and control cohort is statistically significant at some level of confidence (eg. 95%)\n\nthen it is possible to say with 95% confidence that the campaign has grown the customer base.\n\n\n\nIn python, such a comparison is implemented using the statsmodels library in the proportions_chisquare() method where the count parameter represents the number of convertions in each cohort (test or control) and nobs represents the overall size of the same cohort."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/05_analyze_campaign_outputs.html#user-inputs",
    "href": "notebooks/04-transform/notebooks/05_analyze_campaign_outputs.html#user-inputs",
    "title": "Run Campaign and Analyze Results",
    "section": "User Inputs",
    "text": "User Inputs\nDefine the following\n\nstart and end dates for inference data\nconfidence levels at which the difference in the cohort conversion rates is to be checked\n\n\n# 1. start and end dates\ninfer_start_date = \"20170301\"\ninfer_end_date = \"20170331\"\n\n# 2. confidence levels to check difference in conversion rates\nci_levels = np.arange(0.20, 1.00, 0.05, dtype=float)\n\nThe following helper functions are defined in the module src/statistical_checks.py and are used here\n\nget_inference_data_with_cohorts()\n\nloads inference predictions data with audience groups and cohorts assigned as separate columns\n\nget_outcome_labels()\n\nloads the outcome (ML label) of the inference predictions data\nfor demonstration purposes only, this outcome is randomly generated here\n\nget_cohorts()\n\nfilters audience data to retrieve visitors that were placed in one of the two audience cohorts (test or control)\n\nget_overall_and_converted_cohort_sizes\n\ncalculate the size of the overall cohort and the conversions, for both test and control cohorts\n\ncheck_significance_using_chisq()\n\nchecks significance of the difference in conversions between test and control cohort"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/05_analyze_campaign_outputs.html#get-data",
    "href": "notebooks/04-transform/notebooks/05_analyze_campaign_outputs.html#get-data",
    "title": "Run Campaign and Analyze Results",
    "section": "Get Data",
    "text": "Get Data\n\nFetch Latest Version of Best Deployment Candidate Model from Model Registry\nGet best deployment candidate model from model registry\n\ndf_candidate_mlflow_models = modh.get_all_deployment_candidate_models()\n\n\n\nGet Inference Data with Audience Cohorts, Associated with Best Model\nLoad inference data, with the audience groups and cohorts shown as separate columns. This data should contain the outcome for all these first-time visitors to the store during the inference period. Filter this data to only get visitors who were placed in a test or control cohorts, and exclude others. This is done below\n\ndf_infer_audience_cohorts = (\n    sc.get_inference_data_with_cohorts(df_candidate_mlflow_models, \"audience_cohorts\")\n    .pipe(sc.get_outcome_labels)\n    .pipe(sc.get_cohorts, \"cohort\")\n)"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/05_analyze_campaign_outputs.html#compare-difference-in-kpi-between-cohorts",
    "href": "notebooks/04-transform/notebooks/05_analyze_campaign_outputs.html#compare-difference-in-kpi-between-cohorts",
    "title": "Run Campaign and Analyze Results",
    "section": "Compare Difference in KPI Between Cohorts",
    "text": "Compare Difference in KPI Between Cohorts\nCheck if the difference between conversions across the two cohorts is statistically significant\n\noverall_sizes, conversion_sizes = sc.get_overall_and_converted_cohort_sizes(\n    df_infer_audience_cohorts, verbose=False\n)\ndf_sig_checks = sc.check_significance_using_chisq(\n    overall_sizes, conversion_sizes, ci_levels\n).pipe(\n    ash.set_datatypes,\n    {\n        \"check\": pd.StringDtype(),\n        \"p_value\": pd.Float32Dtype(),\n        \"ci_level\": pd.Int8Dtype(),\n        \"control_overall\": pd.Int16Dtype(),\n        \"test_overall\": pd.Int16Dtype(),\n        \"control_conversions\": pd.Int16Dtype(),\n        \"test_conversions\": pd.Int16Dtype(),\n    },\n)\ndf_sig_checks\n\nSet all specified datatypes.\n\n\n\n\n\n\n\n\n\ncheck\np_value\nci_level\ncontrol_overall\ntest_overall\ncontrol_conversions\ntest_conversions\n\n\n\n\n0\nnot statistically significant\n0.526886\n94\n6238\n6238\n330\n346\n\n\n1\nstatistically significant\n0.526886\n44\n6238\n6238\n330\n346"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/05_analyze_campaign_outputs.html#export-to-disk-and-ml-experiment-tracking",
    "href": "notebooks/04-transform/notebooks/05_analyze_campaign_outputs.html#export-to-disk-and-ml-experiment-tracking",
    "title": "Run Campaign and Analyze Results",
    "section": "Export to Disk and ML Experiment Tracking",
    "text": "Export to Disk and ML Experiment Tracking\nGet the best MLFlow run ID\n\nbest_run_id = df_candidate_mlflow_models.squeeze()[\"run_id\"]\n\nShow summary of DataFrame with checks of significance\n\nut.summarize_df(df_sig_checks)\n\n\n\n\n\n\n\n\ncolumn\ndtype\nmissing\n\n\n\n\n0\ncheck\nstring[python]\n0\n\n\n1\np_value\nFloat32\n0\n\n\n2\nci_level\nInt8\n0\n\n\n3\ncontrol_overall\nInt16\n0\n\n\n4\ntest_overall\nInt16\n0\n\n\n5\ncontrol_conversions\nInt16\n0\n\n\n6\ntest_conversions\nInt16\n0\n\n\n\n\n\n\n\nExport to disk and log exported file as MLFlow artifact\n\nut.export_and_track(\n    os.path.join(\n        processed_data_dir,\n        f\"campaign_analysis__run_\"\n        f\"{best_run_id}__\"\n        f\"infer_month_{infer_month}__\"\n        f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet.gzip\",\n    ),\n    df_sig_checks,\n    f\"campaign outcome analysis for inference during {infer_month}\",\n    best_run_id,\n)\n\nExported campaign outcome analysis for inference during March to file campaign_analysis__run_eee3d281e0cb48cd8dba9f93a6de4347__infer_month_March__20230519_234530.parquet.gzip\nLogged campaign outcome analysis for inference during March as artifact in file campaign_analysis__run_eee3d281e0cb48cd8dba9f93a6de4347__infer_month_March__20230519_234530.parquet.gzip"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/06_cleanup.html",
    "href": "notebooks/04-transform/notebooks/06_cleanup.html",
    "title": "Cleanup",
    "section": "",
    "text": "This step cleans up all project resources that were created, including\n\nintermediate (non-MLFlow) files\nMLFlow outputs (artifacts, metrics, etc.)."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/06_cleanup.html#about",
    "href": "notebooks/04-transform/notebooks/06_cleanup.html#about",
    "title": "Cleanup",
    "section": "",
    "text": "This step cleans up all project resources that were created, including\n\nintermediate (non-MLFlow) files\nMLFlow outputs (artifacts, metrics, etc.)."
  },
  {
    "objectID": "notebooks/04-transform/notebooks/06_cleanup.html#user-inputs",
    "href": "notebooks/04-transform/notebooks/06_cleanup.html#user-inputs",
    "title": "Cleanup",
    "section": "User Inputs",
    "text": "User Inputs\nDefine the file name prefix for the following MLFlow artifacts associated with the best deployment candidate MLFLow ML model\n\nprocessed data\nthreshold tuning data, where predictions per threshold were evaluated using the test data split\nestimated sample sizes, where sizes were estimated using the test data split\naudience cohorts created using the inference data split\naudience profiles using the inference data split\npost-campaign impact evaluation\n\n\nartifact_file_prefixes = [\n    'processed_data',\n    \"threshold_tuning_data\",\n    \"audience_sample_sizes\",\n    \"audience_cohorts\",\n    \"audience_profiles\",\n    \"campaign_analysis\",\n]\n\nDefine a helper function to get the filepath to an MLFlow artifact and the associated local (non-MLFlow) file\n\ndef show_artifact_and_local_filepath_for_run_id(\n    df: pd.DataFrame, file_prefix: str, local_dir: str\n) -&gt; pd.DataFrame:\n    \"\"\"Get artifact and local file associated with a run ID.\"\"\"\n    artifact_par_dir = mlflow.artifacts.download_artifacts(\n        run_id=df.squeeze()[\"run_id\"]\n    )\n\n    glob_str = f\"{file_prefix}__run_*.parquet.gzip\"\n    artifact_glob_dir_str = os.path.join(artifact_par_dir, glob_str)\n    artifact_fpath = glob(artifact_glob_dir_str)[-1]\n\n    artifact_dir = os.path.basename(artifact_fpath)\n    local_fpath = os.path.join(local_dir, artifact_dir)\n    return [artifact_fpath, local_fpath]"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/06_cleanup.html#get-data",
    "href": "notebooks/04-transform/notebooks/06_cleanup.html#get-data",
    "title": "Cleanup",
    "section": "Get Data",
    "text": "Get Data\n\nFetch Latest Version of Best Deployment Candidate Model\nGet best deployment candidate model from model registry\n\ndf_candidate_mlflow_models = modh.get_all_deployment_candidate_models()\n\n\n\nGet Filepaths to MLFlow File Artifacts\nGet the filepath to the MLFlow artifacts associated with the best deployment candidate model, as well as the filepath to the corresponding local file used when logging the artifact\n\nartifacts_dict = {\n    k: show_artifact_and_local_filepath_for_run_id(\n        df_candidate_mlflow_models, k, processed_data_dir\n    )\n    for k in artifact_file_prefixes\n}"
  },
  {
    "objectID": "notebooks/04-transform/notebooks/06_cleanup.html#cleanup",
    "href": "notebooks/04-transform/notebooks/06_cleanup.html#cleanup",
    "title": "Cleanup",
    "section": "Cleanup",
    "text": "Cleanup\n\n(Non-MLFlow) Local Outputs\n\nfor k,v in artifacts_dict.items():\n    os.remove(v[1])\n    print(f\"Deleted: {os.path.basename(v[1]).split('__run')[0]}\")\n\n\n\n(Non-MLFlow) ML Development Experiment Outputs\n\nml_runs_glob = os.path.join(raw_data_dir, \"ml__run_*__expt_*.parquet*\")\nfor local_fpath in glob(ml_runs_glob):\n    os.remove(local_fpath)\n    print(f\"Deleted: {os.path.basename(local_fpath).split('__run')[0]}\")\n\n\n\nMLFlow Outputs\nDelete the MLFlow run-logging database file\n\nos.remove(mlruns_db_fpath)\nprint(f\"Removed MLFlow database at {os.path.basename(mlruns_db_fpath)}\")\n\nDelete MLFlow artifact directory (mlruns), and all the artifact sub-directories contained in that directory\n\nshutil.rmtree(mlflow_artifact_fpath)\nprint(\n    \"Removed local MLFlow artifact logging directory at \"\n    f\"{os.path.basename(mlflow_artifact_fpath)}\"\n)"
  }
]